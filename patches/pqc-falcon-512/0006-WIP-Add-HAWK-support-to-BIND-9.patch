From 0adaad4c60868530f0cc8df0e5f651864685f435 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Ondr=CC=8Cej=20Sury=CC=81?= <ondrej@isc.org>
Date: Sun, 21 Jul 2024 15:29:18 -0700
Subject: [PATCH 06/12] WIP: Add HAWK support to BIND 9

---
 bin/dnssec/dnssec-keyfromlabel.rst |    2 +-
 bin/dnssec/dnssec-keygen.c         |    8 +
 bin/dnssec/dnssec-keygen.rst       |    2 +-
 bin/dnssec/dnssec-ksr.c            |    3 +
 bin/dnssec/dnssec-signzone.c       |    2 +-
 configure.ac                       |    2 +-
 lib/dns/Makefile.am                |   23 +-
 lib/dns/dst_api.c                  |    6 +
 lib/dns/dst_internal.h             |    8 +
 lib/dns/dst_parse.c                |  207 +-
 lib/dns/dst_parse.h                |    6 +-
 lib/dns/gssapi_link.c              |   12 +-
 lib/dns/hawk/hawk.h                |  297 +++
 lib/dns/hawk/hawk_config.h         |   35 +
 lib/dns/hawk/hawk_inner.h          |  353 +++
 lib/dns/hawk/hawk_kgen.c           |  318 +++
 lib/dns/hawk/hawk_sign.c           | 1978 ++++++++++++++
 lib/dns/hawk/hawk_vrfy.c           | 3838 ++++++++++++++++++++++++++++
 lib/dns/hawk/modq.h                | 1358 ++++++++++
 lib/dns/hawk/ng_config.h           |   30 +
 lib/dns/hawk/ng_fxp.c              | 1829 +++++++++++++
 lib/dns/hawk/ng_hawk.c             |  934 +++++++
 lib/dns/hawk/ng_inner.h            | 1796 +++++++++++++
 lib/dns/hawk/ng_mp31.c             | 1032 ++++++++
 lib/dns/hawk/ng_ntru.c             | 1555 +++++++++++
 lib/dns/hawk/ng_poly.c             | 1185 +++++++++
 lib/dns/hawk/ng_zint31.c           |  808 ++++++
 lib/dns/hawk/sha3.c                | 1537 +++++++++++
 lib/dns/hawk/sha3.h                |  126 +
 lib/dns/hawk_link.c                |  419 +++
 lib/dns/include/dns/keyvalues.h    |   24 +
 lib/dns/include/dst/dst.h          |    2 +-
 lib/dns/kasp.c                     |    4 +
 lib/dns/opensslecdsa_link.c        |    3 +-
 lib/dns/openssleddsa_link.c        |   22 +-
 lib/dns/opensslrsa_link.c          |    3 +-
 lib/dns/rcode.c                    |    1 +
 lib/isccfg/kaspconf.c              |    1 +
 38 files changed, 19649 insertions(+), 120 deletions(-)
 create mode 100644 lib/dns/hawk/hawk.h
 create mode 100644 lib/dns/hawk/hawk_config.h
 create mode 100644 lib/dns/hawk/hawk_inner.h
 create mode 100644 lib/dns/hawk/hawk_kgen.c
 create mode 100644 lib/dns/hawk/hawk_sign.c
 create mode 100644 lib/dns/hawk/hawk_vrfy.c
 create mode 100644 lib/dns/hawk/modq.h
 create mode 100644 lib/dns/hawk/ng_config.h
 create mode 100644 lib/dns/hawk/ng_fxp.c
 create mode 100644 lib/dns/hawk/ng_hawk.c
 create mode 100644 lib/dns/hawk/ng_inner.h
 create mode 100644 lib/dns/hawk/ng_mp31.c
 create mode 100644 lib/dns/hawk/ng_ntru.c
 create mode 100644 lib/dns/hawk/ng_poly.c
 create mode 100644 lib/dns/hawk/ng_zint31.c
 create mode 100644 lib/dns/hawk/sha3.c
 create mode 100644 lib/dns/hawk/sha3.h
 create mode 100644 lib/dns/hawk_link.c

diff --git a/bin/dnssec/dnssec-keyfromlabel.rst b/bin/dnssec/dnssec-keyfromlabel.rst
index 64d0ec720d..76ee33d7ec 100644
--- a/bin/dnssec/dnssec-keyfromlabel.rst
+++ b/bin/dnssec/dnssec-keyfromlabel.rst
@@ -43,7 +43,7 @@ Options
 
    This option selects the cryptographic algorithm. The value of ``algorithm`` must
    be one of RSASHA1, NSEC3RSASHA1, RSASHA256, RSASHA512,
-   ECDSAP256SHA256, ECDSAP384SHA384, ED25519, or ED448.
+   ECDSAP256SHA256, ECDSAP384SHA384, ED25519, ED448, or HAWK.
 
    These values are case-insensitive. In some cases, abbreviations are
    supported, such as ECDSA256 for ECDSAP256SHA256 and ECDSA384 for
diff --git a/bin/dnssec/dnssec-keygen.c b/bin/dnssec/dnssec-keygen.c
index b63ccb7740..a24ca83a2f 100644
--- a/bin/dnssec/dnssec-keygen.c
+++ b/bin/dnssec/dnssec-keygen.c
@@ -147,6 +147,7 @@ usage(void) {
 	fprintf(stderr, "        RSASHA256 | RSASHA512 |\n");
 	fprintf(stderr, "        ECDSAP256SHA256 | ECDSAP384SHA384 |\n");
 	fprintf(stderr, "        ED25519 | ED448\n");
+	fprintf(stderr, "        HAWK\n");
 	fprintf(stderr, "    -3: use NSEC3-capable algorithm\n");
 	fprintf(stderr, "    -b <key size in bits>:\n");
 	if (!isc_crypto_fips_mode()) {
@@ -161,6 +162,7 @@ usage(void) {
 	fprintf(stderr, "        ECDSAP384SHA384:\tignored\n");
 	fprintf(stderr, "        ED25519:\tignored\n");
 	fprintf(stderr, "        ED448:\tignored\n");
+	fprintf(stderr, "        HAWK:\tignored\n");
 	fprintf(stderr, "        (key size defaults are set according to\n"
 			"        algorithm and usage (ZSK or KSK)\n");
 	fprintf(stderr, "    -c <class>: (default: IN)\n");
@@ -292,6 +294,7 @@ keygen(keygen_ctx_t *ctx, isc_mem_t *mctx, int argc, char **argv) {
 			case DST_ALG_ECDSA384:
 			case DST_ALG_ED25519:
 			case DST_ALG_ED448:
+			case DST_ALG_HAWK:
 				break;
 			default:
 				fatal("algorithm %s is incompatible with NSEC3"
@@ -324,6 +327,7 @@ keygen(keygen_ctx_t *ctx, isc_mem_t *mctx, int argc, char **argv) {
 			case DST_ALG_ECDSA384:
 			case DST_ALG_ED25519:
 			case DST_ALG_ED448:
+			case DST_ALG_HAWK:
 				break;
 			default:
 				fatal("key size not specified (-b option)");
@@ -482,6 +486,9 @@ keygen(keygen_ctx_t *ctx, isc_mem_t *mctx, int argc, char **argv) {
 	case DST_ALG_ED448:
 		ctx->size = 456;
 		break;
+	case DST_ALG_HAWK:
+		ctx->size = DNS_KEY_HAWKSIZE;
+		break;
 	}
 
 	if ((ctx->options & DST_TYPE_KEY) == 0) {
@@ -515,6 +522,7 @@ keygen(keygen_ctx_t *ctx, isc_mem_t *mctx, int argc, char **argv) {
 	case DST_ALG_ECDSA384:
 	case DST_ALG_ED25519:
 	case DST_ALG_ED448:
+	case DST_ALG_HAWK:
 		show_progress = true;
 		break;
 	}
diff --git a/bin/dnssec/dnssec-keygen.rst b/bin/dnssec/dnssec-keygen.rst
index ef12dbb134..36245c2bab 100644
--- a/bin/dnssec/dnssec-keygen.rst
+++ b/bin/dnssec/dnssec-keygen.rst
@@ -47,7 +47,7 @@ Options
 
    This option selects the cryptographic algorithm. For DNSSEC keys, the value of
    ``algorithm`` must be one of RSASHA1, NSEC3RSASHA1, RSASHA256,
-   RSASHA512, ECDSAP256SHA256, ECDSAP384SHA384, ED25519, or ED448.
+   RSASHA512, ECDSAP256SHA256, ECDSAP384SHA384, ED25519, ED448, or HAWK.
 
    These values are case-insensitive. In some cases, abbreviations are
    supported, such as ECDSA256 for ECDSAP256SHA256 and ECDSA384 for
diff --git a/bin/dnssec/dnssec-ksr.c b/bin/dnssec/dnssec-ksr.c
index 2d1c072a61..3f90098058 100644
--- a/bin/dnssec/dnssec-ksr.c
+++ b/bin/dnssec/dnssec-ksr.c
@@ -375,6 +375,9 @@ create_key(ksr_ctx_t *ksr, dns_kasp_t *kasp, dns_kasp_key_t *kaspkey,
 	case DST_ALG_ED448:
 		ksr->size = 456;
 		break;
+	case DST_ALG_HAWK:
+		ksr->size = DNS_KEY_HAWKSIZE;
+		break;
 	default:
 		show_progress = false;
 		break;
diff --git a/bin/dnssec/dnssec-signzone.c b/bin/dnssec/dnssec-signzone.c
index 177fb0140c..13b44fba8a 100644
--- a/bin/dnssec/dnssec-signzone.c
+++ b/bin/dnssec/dnssec-signzone.c
@@ -279,7 +279,7 @@ signwithkey(dns_name_t *name, dns_rdataset_t *rdataset, dst_key_t *key,
 	isc_stdtime_t jendtime, expiry;
 	char keystr[DST_KEY_FORMATSIZE];
 	dns_rdata_t trdata = DNS_RDATA_INIT;
-	unsigned char array[BUFSIZE];
+	unsigned char array[BUFSIZE * 5];
 	isc_buffer_t b;
 	dns_difftuple_t *tuple;
 
diff --git a/configure.ac b/configure.ac
index ae38d9432f..b40fe5c4d9 100644
--- a/configure.ac
+++ b/configure.ac
@@ -689,7 +689,7 @@ AX_RESTORE_FLAGS([openssl])
 
 AC_SUBST([OPENSSL_CFLAGS])
 AC_SUBST([OPENSSL_LIBS])
-
+		 
 AC_CHECK_FUNCS([clock_gettime])
 
 # [pairwise: --with-gssapi=yes, --with-gssapi=auto, --without-gssapi]
diff --git a/lib/dns/Makefile.am b/lib/dns/Makefile.am
index d76a2028d2..8144a0e463 100644
--- a/lib/dns/Makefile.am
+++ b/lib/dns/Makefile.am
@@ -238,6 +238,7 @@ libdns_la_SOURCES =			\
 	sdlz.c				\
 	skr.c				\
 	soa.c				\
+	hawk_link.c			\
 	ssu.c				\
 	ssu_external.c			\
 	stats.c				\
@@ -254,7 +255,24 @@ libdns_la_SOURCES =			\
 	zone.c				\
 	zone_p.h			\
 	zoneverify.c			\
-	zt.c
+	zt.c				\
+	hawk/hawk_config.h \
+	hawk/hawk.h \
+	hawk/hawk_inner.h \
+	hawk/hawk_kgen.c \
+	hawk/hawk_sign.c \
+	hawk/hawk_vrfy.c \
+	hawk/modq.h \
+	hawk/ng_config.h \
+	hawk/ng_fxp.c \
+	hawk/ng_hawk.c \
+	hawk/ng_inner.h \
+	hawk/ng_mp31.c \
+	hawk/ng_ntru.c \
+	hawk/ng_poly.c \
+	hawk/ng_zint31.c \
+	hawk/sha3.c \
+	hawk/sha3.h
 
 if HAVE_GSSAPI
 libdns_la_SOURCES +=			\
@@ -272,7 +290,8 @@ libdns_la_CPPFLAGS =		\
 	$(LIBISC_CFLAGS)	\
 	$(LIBURCU_CFLAGS)	\
 	$(LIBUV_CFLAGS)		\
-	$(OPENSSL_CFLAGS)
+	$(OPENSSL_CFLAGS)	\
+	-Ihawk
 
 libdns_la_LDFLAGS =		\
 	$(AM_LDFLAGS)		\
diff --git a/lib/dns/dst_api.c b/lib/dns/dst_api.c
index 9426f9c260..804576b9de 100644
--- a/lib/dns/dst_api.c
+++ b/lib/dns/dst_api.c
@@ -212,6 +212,8 @@ dst__lib_initialize(void) {
 	dst__openssleddsa_init(&dst_t_func[DST_ALG_ED448], DST_ALG_ED448);
 #endif /* ifdef HAVE_OPENSSL_ED448 */
 
+	dst__hawk_init(&dst_t_func[DST_ALG_HAWK], DST_ALG_HAWK);
+
 #if HAVE_GSSAPI
 	dst__gssapi_init(&dst_t_func[DST_ALG_GSSAPI]);
 #endif /* HAVE_GSSAPI */
@@ -1316,6 +1318,9 @@ dst_key_sigsize(const dst_key_t *key, unsigned int *n) {
 	case DST_ALG_ED448:
 		*n = DNS_SIG_ED448SIZE;
 		break;
+	case DST_ALG_HAWK:
+		*n = DNS_SIG_HAWKSIZE;
+		break;
 	case DST_ALG_HMACMD5:
 		*n = isc_md_type_get_size(ISC_MD_MD5);
 		break;
@@ -1801,6 +1806,7 @@ issymmetric(const dst_key_t *key) {
 	case DST_ALG_ECDSA384:
 	case DST_ALG_ED25519:
 	case DST_ALG_ED448:
+	case DST_ALG_HAWK:
 		return false;
 	case DST_ALG_HMACMD5:
 	case DST_ALG_HMACSHA1:
diff --git a/lib/dns/dst_internal.h b/lib/dns/dst_internal.h
index 049d634987..a5964284ac 100644
--- a/lib/dns/dst_internal.h
+++ b/lib/dns/dst_internal.h
@@ -29,6 +29,7 @@
 
 #pragma once
 
+#include <hawk/hawk.h>
 #include <inttypes.h>
 #include <stdbool.h>
 
@@ -98,6 +99,10 @@ struct dst_key {
 			EVP_PKEY *pub;
 			EVP_PKEY *priv;
 		} pkeypair;
+		struct {
+			uint8_t *pub;
+			uint8_t *priv;
+		} keypair;
 	} keydata; /*%< pointer to key in crypto pkg fmt */
 
 	isc_stdtime_t times[DST_MAX_TIMES + 1]; /*%< timing metadata */
@@ -142,6 +147,7 @@ struct dst_context {
 		dst_gssapi_signverifyctx_t *gssctx;
 		isc_hmac_t *hmac_ctx;
 		EVP_MD_CTX *evp_md_ctx;
+		shake_context shake_context;
 	} ctxdata;
 };
 
@@ -202,6 +208,8 @@ dst__openssleddsa_init(struct dst_func **funcp, unsigned char algorithm);
 void
 dst__gssapi_init(struct dst_func **funcp);
 #endif /* HAVE_GSSAPI*/
+void
+dst__hawk_init(dst_func_t **funcp, unsigned char algorithm);
 
 /*%
  * Secure private file handling
diff --git a/lib/dns/dst_parse.c b/lib/dns/dst_parse.c
index b897407c41..489cd2702f 100644
--- a/lib/dns/dst_parse.c
+++ b/lib/dns/dst_parse.c
@@ -90,6 +90,9 @@ static struct parse_map map[] = { { TAG_RSA_MODULUS, "Modulus:" },
 				  { TAG_EDDSA_ENGINE, "Engine:" },
 				  { TAG_EDDSA_LABEL, "Label:" },
 
+				  { TAG_HAWK_PUBLICKEY, "PublicKey:" },
+				  { TAG_HAWK_SECRETKEY, "SecretKey:" },
+
 				  { TAG_HMACMD5_KEY, "Key:" },
 				  { TAG_HMACMD5_BITS, "Bits:" },
 
@@ -160,19 +163,24 @@ find_numericdata(const char *s) {
 	return find_metadata(s, numerictags, NUMERIC_NTAGS);
 }
 
-static int
-check_rsa(const dst_private_t *priv, bool external) {
-	int i, j;
-	bool have[RSA_NTAGS];
-	bool ok;
-	unsigned int mask;
-
-	if (external) {
-		return (priv->nelements == 0) ? 0 : -1;
+static isc_result_t
+check_external(const dst_private_t *priv) {
+	if (priv->nelements == 0) {
+		return ISC_R_SUCCESS;
 	}
 
-	for (i = 0; i < RSA_NTAGS; i++) {
-		have[i] = false;
+	return DST_R_INVALIDPRIVATEKEY;
+}
+
+static isc_result_t
+check_rsa(const dst_private_t *priv, bool external) {
+	int i, j;
+	bool have[RSA_NTAGS] = { 0 };
+	bool ok;
+	unsigned int mask = (1ULL << TAG_SHIFT) - 1;
+
+	if (external) {
+		return check_external(priv);
 	}
 
 	for (j = 0; j < priv->nelements; j++) {
@@ -182,13 +190,11 @@ check_rsa(const dst_private_t *priv, bool external) {
 			}
 		}
 		if (i == RSA_NTAGS) {
-			return -1;
+			return DST_R_INVALIDPRIVATEKEY;
 		}
 		have[i] = true;
 	}
 
-	mask = (1ULL << TAG_SHIFT) - 1;
-
 	if (have[TAG_RSA_LABEL & mask]) {
 		ok = have[TAG_RSA_MODULUS & mask] &&
 		     have[TAG_RSA_PUBLICEXPONENT & mask];
@@ -202,23 +208,23 @@ check_rsa(const dst_private_t *priv, bool external) {
 		     have[TAG_RSA_EXPONENT2 & mask] &&
 		     have[TAG_RSA_COEFFICIENT & mask];
 	}
-	return ok ? 0 : -1;
+	if (!ok) {
+		return DST_R_INVALIDPRIVATEKEY;
+	}
+
+	return ISC_R_SUCCESS;
 }
 
 static int
 check_ecdsa(const dst_private_t *priv, bool external) {
 	int i, j;
-	bool have[ECDSA_NTAGS];
-	bool ok;
-	unsigned int mask;
+	bool have[ECDSA_NTAGS] = { 0 };
+	unsigned int mask = (1ULL << TAG_SHIFT) - 1;
 
 	if (external) {
-		return (priv->nelements == 0) ? 0 : -1;
+		return check_external(priv);
 	}
 
-	for (i = 0; i < ECDSA_NTAGS; i++) {
-		have[i] = false;
-	}
 	for (j = 0; j < priv->nelements; j++) {
 		for (i = 0; i < ECDSA_NTAGS; i++) {
 			if (priv->elements[j].tag == TAG(DST_ALG_ECDSA256, i)) {
@@ -226,27 +232,26 @@ check_ecdsa(const dst_private_t *priv, bool external) {
 			}
 		}
 		if (i == ECDSA_NTAGS) {
-			return -1;
+			return DST_R_INVALIDPRIVATEKEY;
 		}
 		have[i] = true;
 	}
 
-	mask = (1ULL << TAG_SHIFT) - 1;
+	if (have[TAG_ECDSA_LABEL & mask] || have[TAG_ECDSA_PRIVATEKEY & mask]) {
+		return ISC_R_SUCCESS;
+	}
 
-	ok = have[TAG_ECDSA_LABEL & mask] || have[TAG_ECDSA_PRIVATEKEY & mask];
-
-	return ok ? 0 : -1;
+	return DST_R_INVALIDPRIVATEKEY;
 }
 
 static int
 check_eddsa(const dst_private_t *priv, bool external) {
 	int i, j;
 	bool have[EDDSA_NTAGS];
-	bool ok;
 	unsigned int mask;
 
 	if (external) {
-		return (priv->nelements == 0) ? 0 : -1;
+		return check_external(priv);
 	}
 
 	for (i = 0; i < EDDSA_NTAGS; i++) {
@@ -259,16 +264,50 @@ check_eddsa(const dst_private_t *priv, bool external) {
 			}
 		}
 		if (i == EDDSA_NTAGS) {
-			return -1;
+			return DST_R_INVALIDPRIVATEKEY;
 		}
 		have[i] = true;
 	}
 
 	mask = (1ULL << TAG_SHIFT) - 1;
 
-	ok = have[TAG_EDDSA_LABEL & mask] || have[TAG_EDDSA_PRIVATEKEY & mask];
+	if (have[TAG_EDDSA_LABEL & mask] || have[TAG_EDDSA_PRIVATEKEY & mask]) {
+		return ISC_R_SUCCESS;
+	}
 
-	return ok ? 0 : -1;
+	return DST_R_INVALIDPRIVATEKEY;
+}
+
+static int
+check_hawk(const dst_private_t *priv, bool external) {
+	bool have[HAWK_NTAGS] = { 0 };
+	unsigned int mask;
+
+	if (external) {
+		return check_external(priv);
+	}
+
+	for (size_t j = 0; j < priv->nelements; j++) {
+		size_t i;
+		for (i = 0; i < HAWK_NTAGS; i++) {
+			if (priv->elements[j].tag == TAG(DST_ALG_HAWK, i)) {
+				break;
+			}
+		}
+		if (i == HAWK_NTAGS) {
+			return DST_R_INVALIDPRIVATEKEY;
+		}
+		have[i] = true;
+	}
+
+	mask = (1ULL << TAG_SHIFT) - 1;
+
+	if (have[TAG_HAWK_PUBLICKEY & mask] && have[TAG_HAWK_SECRETKEY & mask])
+	{
+		return ISC_R_SUCCESS;
+	}
+
+	return DST_R_INVALIDPRIVATEKEY;
 }
 
 static int
@@ -283,9 +322,9 @@ check_hmac_md5(const dst_private_t *priv, bool old) {
 		if (old && priv->nelements == OLD_HMACMD5_NTAGS &&
 		    priv->elements[0].tag == TAG_HMACMD5_KEY)
 		{
-			return 0;
+			return ISC_R_SUCCESS;
 		}
-		return -1;
+		return DST_R_INVALIDPRIVATEKEY;
 	}
 	/*
 	 * We must be new format at this point.
@@ -297,10 +336,10 @@ check_hmac_md5(const dst_private_t *priv, bool old) {
 			}
 		}
 		if (j == priv->nelements) {
-			return -1;
+			return DST_R_INVALIDPRIVATEKEY;
 		}
 	}
-	return 0;
+	return ISC_R_SUCCESS;
 }
 
 static int
@@ -308,7 +347,7 @@ check_hmac_sha(const dst_private_t *priv, unsigned int ntags,
 	       unsigned int alg) {
 	unsigned int i, j;
 	if (priv->nelements != ntags) {
-		return -1;
+		return DST_R_INVALIDPRIVATEKEY;
 	}
 	for (i = 0; i < ntags; i++) {
 		for (j = 0; j < priv->nelements; j++) {
@@ -317,13 +356,13 @@ check_hmac_sha(const dst_private_t *priv, unsigned int ntags,
 			}
 		}
 		if (j == priv->nelements) {
-			return -1;
+			return DST_R_INVALIDPRIVATEKEY;
 		}
 	}
-	return 0;
+	return ISC_R_SUCCESS;
 }
 
-static int
+static isc_result_t
 check_data(const dst_private_t *priv, const unsigned int alg, bool old,
 	   bool external) {
 	switch (alg) {
@@ -339,6 +378,8 @@ check_data(const dst_private_t *priv, const unsigned int alg, bool old,
 	case DST_ALG_ED25519:
 	case DST_ALG_ED448:
 		return check_eddsa(priv, external);
+	case DST_ALG_HAWK:
+		return check_hawk(priv, external);
 	case DST_ALG_HMACMD5:
 		return check_hmac_md5(priv, old);
 	case DST_ALG_HMACSHA1:
@@ -382,7 +423,7 @@ dst__privstruct_parse(dst_key_t *key, unsigned int alg, isc_lex_t *lex,
 	unsigned char *data = NULL;
 	unsigned int opt = ISC_LEXOPT_EOL;
 	isc_stdtime_t when;
-	isc_result_t ret;
+	isc_result_t result;
 	bool external = false;
 
 	REQUIRE(priv != NULL);
@@ -390,20 +431,22 @@ dst__privstruct_parse(dst_key_t *key, unsigned int alg, isc_lex_t *lex,
 	priv->nelements = 0;
 	memset(priv->elements, 0, sizeof(priv->elements));
 
-#define NEXTTOKEN(lex, opt, token)                       \
-	do {                                             \
-		ret = isc_lex_gettoken(lex, opt, token); \
-		if (ret != ISC_R_SUCCESS)                \
-			goto fail;                       \
+#define NEXTTOKEN(lex, opt, token)                          \
+	do {                                                \
+		result = isc_lex_gettoken(lex, opt, token); \
+		if (result != ISC_R_SUCCESS) {              \
+			goto fail;                          \
+		}                                           \
 	} while (0)
 
-#define READLINE(lex, opt, token)                        \
-	do {                                             \
-		ret = isc_lex_gettoken(lex, opt, token); \
-		if (ret == ISC_R_EOF)                    \
-			break;                           \
-		else if (ret != ISC_R_SUCCESS)           \
-			goto fail;                       \
+#define READLINE(lex, opt, token)                           \
+	do {                                                \
+		result = isc_lex_gettoken(lex, opt, token); \
+		if (result == ISC_R_EOF) {                  \
+			break;                              \
+		} else if (result != ISC_R_SUCCESS) {       \
+			goto fail;                          \
+		}                                           \
 	} while ((*token).type != isc_tokentype_eol)
 
 	/*
@@ -413,23 +456,23 @@ dst__privstruct_parse(dst_key_t *key, unsigned int alg, isc_lex_t *lex,
 	if (token.type != isc_tokentype_string ||
 	    strcmp(DST_AS_STR(token), PRIVATE_KEY_STR) != 0)
 	{
-		ret = DST_R_INVALIDPRIVATEKEY;
+		result = DST_R_INVALIDPRIVATEKEY;
 		goto fail;
 	}
 
 	NEXTTOKEN(lex, opt, &token);
 	if (token.type != isc_tokentype_string || (DST_AS_STR(token))[0] != 'v')
 	{
-		ret = DST_R_INVALIDPRIVATEKEY;
+		result = DST_R_INVALIDPRIVATEKEY;
 		goto fail;
 	}
 	if (sscanf(DST_AS_STR(token), "v%d.%d", &major, &minor) != 2) {
-		ret = DST_R_INVALIDPRIVATEKEY;
+		result = DST_R_INVALIDPRIVATEKEY;
 		goto fail;
 	}
 
 	if (major > DST_MAJOR_VERSION) {
-		ret = DST_R_INVALIDPRIVATEKEY;
+		result = DST_R_INVALIDPRIVATEKEY;
 		goto fail;
 	}
 
@@ -447,7 +490,7 @@ dst__privstruct_parse(dst_key_t *key, unsigned int alg, isc_lex_t *lex,
 	if (token.type != isc_tokentype_string ||
 	    strcmp(DST_AS_STR(token), ALGORITHM_STR) != 0)
 	{
-		ret = DST_R_INVALIDPRIVATEKEY;
+		result = DST_R_INVALIDPRIVATEKEY;
 		goto fail;
 	}
 
@@ -455,7 +498,7 @@ dst__privstruct_parse(dst_key_t *key, unsigned int alg, isc_lex_t *lex,
 	if (token.type != isc_tokentype_number ||
 	    token.value.as_ulong != (unsigned long)dst_key_alg(key))
 	{
-		ret = DST_R_INVALIDPRIVATEKEY;
+		result = DST_R_INVALIDPRIVATEKEY;
 		goto fail;
 	}
 
@@ -468,17 +511,17 @@ dst__privstruct_parse(dst_key_t *key, unsigned int alg, isc_lex_t *lex,
 		int tag;
 		isc_region_t r;
 		do {
-			ret = isc_lex_gettoken(lex, opt, &token);
-			if (ret == ISC_R_EOF) {
+			result = isc_lex_gettoken(lex, opt, &token);
+			if (result == ISC_R_EOF) {
 				goto done;
 			}
-			if (ret != ISC_R_SUCCESS) {
+			if (result != ISC_R_SUCCESS) {
 				goto fail;
 			}
 		} while (token.type == isc_tokentype_eol);
 
 		if (token.type != isc_tokentype_string) {
-			ret = DST_R_INVALIDPRIVATEKEY;
+			result = DST_R_INVALIDPRIVATEKEY;
 			goto fail;
 		}
 
@@ -494,7 +537,7 @@ dst__privstruct_parse(dst_key_t *key, unsigned int alg, isc_lex_t *lex,
 
 			NEXTTOKEN(lex, opt | ISC_LEXOPT_NUMBER, &token);
 			if (token.type != isc_tokentype_number) {
-				ret = DST_R_INVALIDPRIVATEKEY;
+				result = DST_R_INVALIDPRIVATEKEY;
 				goto fail;
 			}
 
@@ -509,12 +552,12 @@ dst__privstruct_parse(dst_key_t *key, unsigned int alg, isc_lex_t *lex,
 
 			NEXTTOKEN(lex, opt, &token);
 			if (token.type != isc_tokentype_string) {
-				ret = DST_R_INVALIDPRIVATEKEY;
+				result = DST_R_INVALIDPRIVATEKEY;
 				goto fail;
 			}
 
-			ret = dns_time32_fromtext(DST_AS_STR(token), &when);
-			if (ret != ISC_R_SUCCESS) {
+			result = dns_time32_fromtext(DST_AS_STR(token), &when);
+			if (result != ISC_R_SUCCESS) {
 				goto fail;
 			}
 
@@ -528,7 +571,7 @@ dst__privstruct_parse(dst_key_t *key, unsigned int alg, isc_lex_t *lex,
 		if (tag < 0 && minor > DST_MINOR_VERSION) {
 			goto next;
 		} else if (tag < 0) {
-			ret = DST_R_INVALIDPRIVATEKEY;
+			result = DST_R_INVALIDPRIVATEKEY;
 			goto fail;
 		}
 
@@ -537,8 +580,8 @@ dst__privstruct_parse(dst_key_t *key, unsigned int alg, isc_lex_t *lex,
 		data = isc_mem_get(mctx, MAXFIELDSIZE);
 
 		isc_buffer_init(&b, data, MAXFIELDSIZE);
-		ret = isc_base64_tobuffer(lex, &b, -1);
-		if (ret != ISC_R_SUCCESS) {
+		result = isc_base64_tobuffer(lex, &b, -1);
+		if (result != ISC_R_SUCCESS) {
 			goto fail;
 		}
 
@@ -554,16 +597,13 @@ dst__privstruct_parse(dst_key_t *key, unsigned int alg, isc_lex_t *lex,
 
 done:
 	if (external && priv->nelements != 0) {
-		ret = DST_R_INVALIDPRIVATEKEY;
+		result = DST_R_INVALIDPRIVATEKEY;
 		goto fail;
 	}
 
 	check = check_data(priv, alg, true, external);
-	if (check < 0) {
-		ret = DST_R_INVALIDPRIVATEKEY;
-		goto fail;
-	} else if (check != ISC_R_SUCCESS) {
-		ret = check;
+	if (check != ISC_R_SUCCESS) {
+		result = check;
 		goto fail;
 	}
 
@@ -577,7 +617,7 @@ fail:
 		isc_mem_put(mctx, data, MAXFIELDSIZE);
 	}
 
-	return ret;
+	return result;
 }
 
 isc_result_t
@@ -596,15 +636,13 @@ dst__privstruct_writefile(const dst_key_t *key, const dst_private_t *priv,
 	isc_region_t r;
 	int major, minor;
 	mode_t mode;
-	int i, ret;
+	int i;
 
 	REQUIRE(priv != NULL);
 
-	ret = check_data(priv, dst_key_alg(key), false, key->external);
-	if (ret < 0) {
-		return DST_R_INVALIDPRIVATEKEY;
-	} else if (ret != ISC_R_SUCCESS) {
-		return ret;
+	result = check_data(priv, dst_key_alg(key), false, key->external);
+	if (result != ISC_R_SUCCESS) {
+		return result;
 	}
 
 	isc_buffer_init(&fileb, filename, sizeof(filename));
@@ -676,6 +714,9 @@ dst__privstruct_writefile(const dst_key_t *key, const dst_private_t *priv,
 	case DST_ALG_ED448:
 		fprintf(fp, "(ED448)\n");
 		break;
+	case DST_ALG_HAWK:
+		fprintf(fp, "(HAWK)\n");
+		break;
 	case DST_ALG_HMACMD5:
 		fprintf(fp, "(HMAC_MD5)\n");
 		break;
diff --git a/lib/dns/dst_parse.h b/lib/dns/dst_parse.h
index 80612ec253..94f16cf615 100644
--- a/lib/dns/dst_parse.h
+++ b/lib/dns/dst_parse.h
@@ -32,7 +32,7 @@
 
 #include <dst/dst.h>
 
-#define MAXFIELDSIZE 512
+#define MAXFIELDSIZE 1025
 
 /*
  * Maximum number of fields in a private file is 18 (12 algorithm-
@@ -67,6 +67,10 @@
 #define TAG_EDDSA_ENGINE     ((DST_ALG_ED25519 << TAG_SHIFT) + 1)
 #define TAG_EDDSA_LABEL	     ((DST_ALG_ED25519 << TAG_SHIFT) + 2)
 
+#define HAWK_NTAGS	   3
+#define TAG_HAWK_PUBLICKEY ((DST_ALG_HAWK << TAG_SHIFT) + 0)
+#define TAG_HAWK_SECRETKEY ((DST_ALG_HAWK << TAG_SHIFT) + 1)
+
 #define OLD_HMACMD5_NTAGS 1
 #define HMACMD5_NTAGS	  2
 #define TAG_HMACMD5_KEY	  ((DST_ALG_HMACMD5 << TAG_SHIFT) + 0)
diff --git a/lib/dns/gssapi_link.c b/lib/dns/gssapi_link.c
index ffe428601f..d96215ecd1 100644
--- a/lib/dns/gssapi_link.c
+++ b/lib/dns/gssapi_link.c
@@ -63,13 +63,11 @@ struct dst_gssapi_signverifyctx {
  * or verifying.
  */
 static isc_result_t
-gssapi_create_signverify_ctx(dst_key_t *key, dst_context_t *dctx) {
-	dst_gssapi_signverifyctx_t *ctx;
-
-	UNUSED(key);
-
-	ctx = isc_mem_get(dctx->mctx, sizeof(dst_gssapi_signverifyctx_t));
-	ctx->buffer = NULL;
+gssapi_create_signverify_ctx(dst_key_t *key ISC_ATTR_UNUSED,
+			     dst_context_t *dctx) {
+	dst_gssapi_signverifyctx_t *ctx =
+		isc_mem_get(dctx->mctx, sizeof(dst_gssapi_signverifyctx_t));
+	*ctx = (dst_gssapi_signverifyctx_t){ 0 };
 	isc_buffer_allocate(dctx->mctx, &ctx->buffer, INITIAL_BUFFER_SIZE);
 
 	dctx->ctxdata.gssctx = ctx;
diff --git a/lib/dns/hawk/hawk.h b/lib/dns/hawk/hawk.h
new file mode 100644
index 0000000000..bf60639af8
--- /dev/null
+++ b/lib/dns/hawk/hawk.h
@@ -0,0 +1,297 @@
+#ifndef HAWK_H__
+#define HAWK_H__
+
+#include <stddef.h>
+#include <stdint.h>
+
+#include "sha3.h"
+
+/*
+ * All sizes below are in bytes.
+ *
+ * Private keys have an inherently fixed size: they contain the seed used
+ * to (re)generate (f,g), the polynomials F mod 2 and G mod 2 (n bits = n/8
+ * bytes each), and a hash of the public key. The seed length is 16, 24 or
+ * 40 bytes, for n = 256, 512 or 1024. The public key hash uses SHAKE256
+ * with an output of the same size as the seed.
+ *
+ * Internally, public keys and signatures have varying sizes. We pad them
+ * with zeros to a fixed length (padding bytes are verified to be zero upon
+ * decoding); keygen and sign enforce the lengths by restarting in case the
+ * obtained result does not fit. The fixed sizes are set so that such
+ * restarts are rare enough that their practical impact on average
+ * performance is negligible.
+ *
+ * Measured average sizes and standard deviations (over 10000 key pairs and
+ * 1000 signatures per key pair):
+ *   degree (n)         public key          signature
+ *       256          444.07 (1.902)      243.98 (0.916)
+ *       512         1015.93 (6.353)      536.55 (3.691)
+ *      1024         2404.44 (17.274)    1193.12 (5.588)
+ *
+ * We enforce a maximum size on public keys. If the obtained public key is
+ * larger than this size, we discard the key pair and generate a new one.
+ * The maximum size is chosen such that retries do not occur frequently and
+ * key generation is not slowed down by the retries.
+ *
+ * Similarly, signatures also have a maximum size. If a generated signature
+ * is too large, signing is restarted with a different salt. The maximum
+ * size is chosen as 5 standard deviations above the average. A retry
+ * happens less than once in a million.
+ *
+ *   degree (n)    private key    public key    signature
+ *       256            96            450           249
+ *       512           184           1024           555
+ *      1024           360           2440          1221
+ */
+
+/*
+ * Private key length (in bytes).
+ */
+#define HAWK_PRIVKEY_SIZE(logn) \
+	(8u + (11u << ((logn) - 5)))
+
+/*
+ * Public key length (in bytes).
+ */
+#define HAWK_PUBKEY_SIZE(logn)   (450u \
+	+ 574u * (2u >> (10 - (logn))) + 842u * ((1u >> (10 - (logn)))))
+
+/*
+ * Signature length (in bytes).
+ */
+#define HAWK_SIG_SIZE(logn)      (249u \
+	+ 306u * (2u >> (10 - (logn))) + 360u * ((1u >> (10 - (logn)))))
+
+/*
+ * Temporary buffer size for key pair generation.
+ */
+#define HAWK_TMPSIZE_KEYGEN(logn)         ((26u << (logn)) + 7)
+
+/*
+ * Temporary buffer size for signature generation.
+ */
+#define HAWK_TMPSIZE_SIGN(logn)           ((6u << (logn)) + 7)
+
+/*
+ * Temporary buffer size for signature verification (minimum supported size).
+ */
+#define HAWK_TMPSIZE_VERIFY(logn)         ((10u << (logn)) + 7)
+
+/*
+ * Temporary buffer size for slightly faster signature verification. Using
+ * this buffer size yields the same verification performance as decoding
+ * the signature and public key first, then calling the verification
+ * function.
+ */
+#define HAWK_TMPSIZE_VERIFY_FAST(logn) \
+	((15u << (logn)) + 64 + 64u * (1u >> (10 - (logn))) + 7)
+
+/*
+ * Type for a random source (with a context structure).
+ * The RNG is invoked with a destination buffer 'dst' of size 'len' bytes.
+ * It should fill it with random bytes. The RNG uses the context pointer
+ * ('ctx') in any way it sees fit.
+ */
+typedef void (*hawk_rng)(void *ctx, void *dst, size_t len);
+
+/*
+ * Generate a new public/private key pair.
+ *
+ * Degree logarithm is provided as logn (8, 9 or 10). The private key
+ * is written into 'priv' (size HAWK_PRIVKEY_SIZE(logn), exactly), while
+ * the public key is written into 'pub' (size HAWK_PUBKEY_SIZE(logn),
+ * exactly).
+ *
+ * The private and public key are also copied, in that order, in the tmp
+ * buffer. The priv and/or pub parameters may be set to NULL, in which
+ * case the caller will have to retrieve the corresponding values from the
+ * tmp buffer afterwards.
+ *
+ * On error, 0 is returned. An error is reported if logn is not a
+ * supported value, of if the temporary buffer is too short. On success,
+ * 1 is returned.
+ *
+ * The temporary buffer is provided as tmp, with size tmp_len bytes.
+ * In all generality, providing at least 26*(2^logn) + 7 bytes is always
+ * sufficient (this is the value returned by HAWK_TMPSIZE_KEYGEN).
+ * If tmp is 8-byte aligned, then 26*(2^logn) bytes are enough.
+ */
+int
+hawk_keygen(unsigned logn, void *priv, void *pub,
+	hawk_rng rng, void *rng_context, void *tmp, size_t tmp_len);
+
+/*
+ * Start a signature process by initializing the provided SHAKE context.
+ * This is equivalent to calling shake_init(sc_data, 256). The caller
+ * should then inject the message data into the context, using the
+ * SHAKE API (shake_inject() function). The same function works for both
+ * signature generation and verification.
+ */
+void hawk_sign_start(shake_context *sc_data);
+#define hawk_verify_start   hawk_sign_start
+
+/*
+ * Generate an Hawk signature.
+ *    logn          log of degree (8 to 10, for n = 256 to 1024)
+ *    rng           random source to use for sampling
+ *    rng_context   context for the random source
+ *    sig           signature output buffer (may be NULL)
+ *    sc_data       SHAKE256 context with message injected (_not_ flipped)
+ *    priv          private key (encoded)
+ *    tmp           temporary buffer
+ *    tmp_len       temporary buffer length (in bytes)
+ *
+ * Returned value is 1 on success, 0 on error. An error is returned if
+ * logn is unsupported (not in the 8 to 10 range) or if the temporary
+ * buffer is too short.
+ *
+ * The signature is always returned in the tmp buffer. If the sig
+ * parameter is not NULL, then the signature is also written in that
+ * buffer. The signature length is HAWK_SIG_SIZE(logn).
+ *
+ * sc_data contains the SHAKE256 context with the message injected. It
+ * MUST NOT have been flipped to extraction state. sc_data is unmodified
+ * by this function.
+ *
+ * Minimum temporary buffer size: 6*2^logn + 7 bytes
+ * (This is the size returned by HAWK_TMPSIZE_SIGN(logn).)
+ */
+int hawk_sign_finish(unsigned logn,
+	hawk_rng rng, void *rng_context,
+	void *sig, const shake_context *sc_data,
+	const void *priv, void *tmp, size_t tmp_len);
+
+/*
+ * A variant of hawk_sign_finish(), with two changes:
+ *
+ *   - The provided random source is used directly, instead of being
+ *     used to initialize some internal SHAKE instances.
+ *
+ *   - The private key may be provided in a pre-decoded format, with the
+ *     polynomials f (n bytes), g (n bytes), F mod 2 (n bits = n/8 bytes)
+ *     and G mod 2 (n bits = n/8 bytes), in that order. The
+ *     hawk_decode_private_key() function can perform that decoding step.
+ *     The private key can also be provided in encoded format; the
+ *     priv_len parameter specifies the length (in bytes) of the key.
+ *
+ * The produced signatures are not identical to the ones generated with
+ * hawk_sign_finish(), but they are fully interoperable. This function is
+ * intended to support hardware platforms for which the cost of SHAKE is
+ * high, but a different and fast RNG is available (e.g. leveraging
+ * AES hardware). In that mode, the provided RNG is always invokved with
+ * an output length of
+ */
+int hawk_sign_finish_alt(unsigned logn,
+	hawk_rng rng, void *rng_context,
+	void *sig, const shake_context *sc_data,
+	const void *priv, size_t priv_len, void *tmp, size_t tmp_len);
+
+/*
+ * Decode the private key. The input private key (priv) has size
+ * exactly HAWK_PRIVKEY_SIZE(logn) bytes; the output decoded key (priv_dec)
+ * contains the f, g, F mod 2 and G mod 2 polynomials, in that order,
+ * for a total size of 2.25*2^logn bytes (n bytes for each of f and g,
+ * n/8 bytes for each of F mod 2 and G mod 2). The decoded private key
+ * can be used with the hawk_sign_finish_alt() function.
+ */
+void hawk_decode_private_key(unsigned logn, void *priv_dec, const void *priv);
+
+/*
+ * Decoded private key length (in bytes). Decoded private keys can be used
+ * with hawk_sign_finish_alt(). A decoded private key contains the f, g,
+ * F mod 2 and G mod 2 polynomials, and public key hash, in that order (f
+ * and g use n bytes each, F mod 2 and G mod 2 use n bits = n/8 bytes each).
+ */
+#define HAWK_PRIVKEY_DECODED_SIZE(logn) \
+	(37u << ((logn) - 4))
+
+/*
+ * Decode a signature value. The encoded signature is provided in sig,
+ * with size sig_len. If the length is not exactly HAWK_SIG_SIZE(logn)
+ * bytes, then an error is reported.
+ *
+ * The signature polynomial s1 (int16_t[n]) and the salt (14, 24 or 40
+ * bytes) are written into s1_and_salt, in that order. The
+ * HAWK_DECODED_SIG_LENGTH() macro yields the number of produced 16-bit
+ * values, for a given degree.
+ *
+ * Returned value is 1 on success, 0 on error.
+ *
+ * Note: this function does _not_ verify the sym-break condition; this is
+ * done within hawk_verify(). However, it checks that all elements of s1
+ * are within the allowed range.
+ */
+int hawk_decode_signature(unsigned logn,
+	int16_t *s1_and_salt, const void *sig, size_t sig_len);
+
+/*
+ * Get the number of 16-bit elements for a decoded signature in RAM.
+ */
+#define HAWK_DECODED_SIG_LENGTH(logn)   ((1u << (logn)) + 7u \
+	+ 5u * (2u >> (10 - (logn))) + 3u * (1u >> (10 - (logn))))
+
+/*
+ * Decode a public key. The encoded public key is provided in pub,
+ * with size pub_len. If the length is not exactly HAWK_PUBKEY_SIZE(logn)
+ * bytes, then an error is reported.
+ *
+ * The decoded polynomials q00 (n/2 elements) and q01 (n elements) are
+ * written into q00_q01_hpk, in that order, followed by the hash of the
+ * public key. The total output size (expressed in 16-bit units) can
+ * be obtained from HAWK_DECODED_PUB_LENGTH.
+ *
+ * Returned value is 1 on success, 0 on error.
+ *
+ * Note: this function checks that all elements of q00 and q01 are within
+ * the respective allowed ranges.
+ */
+int hawk_decode_public_key(unsigned logn,
+	int16_t *q00_q01_hpk, const void *pub, size_t pub_len);
+
+/*
+ * Get the number of 16-bit elements for a decode public key in RAM.
+ */
+#define HAWK_DECODED_PUB_LENGTH(logn) \
+	((3u << ((logn) - 1)) + (1u << ((logn) - 5)))
+
+/*
+ * Verify an Hawk signature.
+ *    logn          log of degree (8 to 10, for n = 256 to 1024)
+ *    sig           signature (encoded or decoded)
+ *    sig_len       signature length, or (size_t)-1 for a decoded signature
+ *    sc_data       SHAKE256 context with message injected (_not_ flipped)
+ *    pub           public key (encoded or decoded)
+ *    pub_len       public key length, or (size_t)-1 for a decoded public key
+ *    tmp           temporary buffer
+ *    tmp_len       temporary buffer length (in bytes)
+ *
+ * If sig_len is equal to -1 (cast to size_t), then the sig parameter
+ * should point at the decoded signature, as obtained by calling
+ * hawk_decode_signature(). Otherwise, sig should contain the encoded
+ * signature, of length exactly sig_len bytes. This function checks that,
+ * in the latter case, the length is equal to HAWK_SIG_SIZE(logn).
+ *
+ * If pub_len is equal to -1 (cast to size_t), then the pub parameter
+ * should point at the decoded public key, as obtained by calling
+ * hawk_decode_public_key(). Otherwise, pub should contain the encoded
+ * public key, of length exactly pub_len bytes. This function checks that,
+ * in the latter case, the length is equal to HAWK_PUBKEY_SIZE(logn).
+ *
+ * Returned value is 1 on success (signature matches the provided public key
+ * and hashed message), 0 on failure.
+ *
+ * Minimum temporary buffer size: 10*2^logn + 7 bytes
+ * (This is the size returned by HAWK_TMPSIZE_VERIFY(logn).)
+ * If the temporary buffer is larger, then this function may use the extra
+ * space to store the decoded version of the public key and/or the signature,
+ * which slightly speeds up the process. HAWK_TMPSIZE_VERIFY_FAST(logn) is
+ * the buffer size that provides the fastest processing.
+ */
+int hawk_verify_finish(unsigned logn,
+	const void *sig, size_t sig_len,
+	const shake_context *sc_data,
+	const void *pub, size_t pub_len,
+	void *tmp, size_t tmp_len);
+
+#endif
diff --git a/lib/dns/hawk/hawk_config.h b/lib/dns/hawk/hawk_config.h
new file mode 100644
index 0000000000..9dfe23c67d
--- /dev/null
+++ b/lib/dns/hawk/hawk_config.h
@@ -0,0 +1,35 @@
+#ifndef HAWK_CONFIG_H__
+#define HAWK_CONFIG_H__
+
+/*
+ * AVX2 support. If defined to 1, then this enables use of AVX2 intrinsics,
+ * even if the compiler does not explicitly target AVX2-only architectures.
+ * Use of AVX2 intrinsics makes the compiled output not compatible with CPUs
+ * that do not support AVX2.
+ * Defining HAWK_AVX2 to 0 forcibly disables use of AVX2 intrinsics
+ * (the compiler might still use AVX2 opcodes as part of its automatic
+ * vectorization of loops).
+ * If HAWK_AVX2 is not defined here, then use of AVX2 intrinsics will be
+ * automatically detected based on the compilation target architecture
+ * (e.g. use of the '-mavx2' compiler flag). There is no runtime detection
+ * of the abilities of the CPU that actually runs the code.
+ *
+#define HAWK_AVX2   1
+ */
+#define HAWK_AVX2 1
+
+/*
+ * Name prefixing. If this macro is defined to a identifier, then all
+ * symbols corresponding to non-static functions and global data will
+ * have a name starting with that identifier, followed by an underscore.
+ * If undefined, then the default prefix is 'hawk'.
+ * This is meant to be used for integration into larger applications, to
+ * avoid name collisions. Also, the outer application might compile this
+ * code twice, with and without AVX2 support, with different name prefixes,
+ * and link both versions in the runtime, selecting at runtime which one to
+ * call based on the current CPU abilities.
+ *
+#define HAWK_PREFIX   hawk
+ */
+
+#endif
diff --git a/lib/dns/hawk/hawk_inner.h b/lib/dns/hawk/hawk_inner.h
new file mode 100644
index 0000000000..8819490690
--- /dev/null
+++ b/lib/dns/hawk/hawk_inner.h
@@ -0,0 +1,353 @@
+#ifndef HAWK_INNER_H__
+#define HAWK_INNER_H__
+
+/* ==================================================================== */
+
+#include <stddef.h>
+#include <stdint.h>
+#include <string.h>
+
+#include "hawk_config.h"
+
+#ifndef HAWK_PREFIX
+#define HAWK_PREFIX   hawk
+#endif
+#define Zh(name)             Zh_(HAWK_PREFIX, name)
+#define Zh_(prefix, name)    Zh__(prefix, name)
+#define Zh__(prefix, name)   prefix ## _ ## name
+
+#include "hawk.h"
+
+/*
+ * MSVC 2015 does not known the C99 keyword 'restrict'.
+ */
+#if defined _MSC_VER && _MSC_VER
+#ifndef restrict
+#define restrict   __restrict
+#endif
+#endif
+
+/* ==================================================================== */
+/*
+ * Functions imported from NTRUGEN; redeclared here to avoid importing
+ * the NTRUGEN inner files.
+ */
+
+typedef void (*ntrugen_rng)(void *ctx, void *dst, size_t len);
+
+#define Hawk_keygen   Zh(ntrugen_Hawk_keygen)
+int Hawk_keygen(unsigned logn,
+	int8_t *restrict f, int8_t *restrict g,
+	int8_t *restrict F, int8_t *restrict G,
+	int16_t *restrict q00, int16_t *restrict q01, int32_t *restrict q11,
+	void *restrict seed, ntrugen_rng rng, void *restrict rng_context,
+	void *restrict tmp, size_t tmp_len);
+
+#define Hawk_regen_fg   Zh(ntrugen_Hawk_regen_fg)
+void Hawk_regen_fg(unsigned logn,
+	int8_t *restrict f, int8_t *restrict g, const void *seed);
+
+/* ==================================================================== */
+
+#ifndef HAWK_AVX2
+/*
+ * Auto-detection of AVX2 support, if not overridden by configuration.
+ */
+#if defined __AVX2__ && __AVX2__
+#define HAWK_AVX2   1
+#else
+#define HAWK_AVX2   0
+#endif
+#endif // HAWK_AVX2
+
+#if HAWK_AVX2
+/*
+ * This implementation uses AVX2 intrinsics.
+ */
+#include <immintrin.h>
+#if defined __GNUC__ || defined __clang__
+#include <x86intrin.h>
+#endif
+#ifndef HAWK_LE
+#define HAWK_LE   1
+#endif
+#ifndef HAWK_UNALIGNED
+#define HAWK_UNALIGNED   1
+#endif
+#if defined __GNUC__
+#define TARGET_AVX2         __attribute__((target("avx2,bmi,pclmul")))
+#define TARGET_AVX2_ONLY    __attribute__((target("avx2")))
+#define ALIGNED_AVX2   __attribute__((aligned(32)))
+#elif defined _MSC_VER && _MSC_VER
+#pragma warning( disable : 4752 )
+#endif
+#endif // HAWK_AVX2
+
+#ifndef TARGET_AVX2
+#define TARGET_AVX2
+#endif
+#ifndef TARGET_AVX2_ONLY
+#define TARGET_AVX2_ONLY
+#endif
+#ifndef ALIGNED_AVX2
+#define ALIGNED_AVX2
+#endif
+
+/*
+ * Disable warning on applying unary minus on an unsigned type.
+ */
+#if defined _MSC_VER && _MSC_VER
+#pragma warning( disable : 4146 )
+#pragma warning( disable : 4244 )
+#pragma warning( disable : 4267 )
+#pragma warning( disable : 4334 )
+#endif
+
+/*
+ * Auto-detect 64-bit architectures.
+ */
+#ifndef HAWK_64
+#if defined __x86_64__ || defined _M_X64 \
+        || defined __ia64 || defined __itanium__ || defined _M_IA64 \
+        || defined __powerpc64__ || defined __ppc64__ || defined __PPC64__ \
+        || defined __64BIT__ || defined _LP64 || defined __LP64__ \
+        || defined __sparc64__ \
+        || defined __aarch64__ || defined _M_ARM64 \
+        || defined __mips64
+#define HAWK_64   1
+#else
+#define HAWK_64   0
+#endif
+#endif
+
+/*
+ * Auto-detect endianness and support of unaligned accesses.
+ */
+#if defined __i386__ || defined _M_IX86 \
+        || defined __x86_64__ || defined _M_X64 \
+	|| defined __aarch64__ || defined _M_ARM64 || defined _M_ARM64EC \
+        || (defined _ARCH_PWR8 \
+                && (defined __LITTLE_ENDIAN || defined __LITTLE_ENDIAN__))
+
+#ifndef HAWK_LE
+#define HAWK_LE   1
+#endif
+#ifndef HAWK_UNALIGNED
+#define HAWK_UNALIGNED   1
+#endif
+
+#elif (defined __LITTLE_ENDIAN || defined __LITTLE_ENDIAN__) \
+        || (defined __BYTE_ORDER__ && defined __ORDER_LITTLE_ENDIAN__ \
+                && __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__)
+
+#ifndef HAWK_LE
+#define HAWK_LE   1
+#endif
+#ifndef HAWK_UNALIGNED
+#define HAWK_UNALIGNED   0
+#endif
+
+#else
+
+#ifndef HAWK_LE
+#define HAWK_LE   0
+#endif
+#ifndef HAWK_UNALIGNED
+#define HAWK_UNALIGNED   0
+#endif
+
+#endif
+
+/* No debug output by default. */
+#ifndef HAWK_DEBUG
+#define HAWK_DEBUG   0
+#endif
+
+/* ==================================================================== */
+
+static inline unsigned
+dec16le(const void *src)
+{
+#if HAWK_LE && HAWK_UNALIGNED
+	return *(const uint16_t *)src;
+#else
+	const uint8_t *buf = src;
+	return (unsigned)buf[0]
+		| ((unsigned)buf[1] << 8);
+#endif
+}
+
+static inline void
+enc16le(void *dst, unsigned x)
+{
+#if HAWK_LE && HAWK_UNALIGNED
+	*(uint16_t *)dst = x;
+#else
+	uint8_t *buf = dst;
+	buf[0] = (uint8_t)x;
+	buf[1] = (uint8_t)(x >> 8);
+#endif
+}
+
+static inline uint32_t
+dec32le(const void *src)
+{
+#if HAWK_LE && HAWK_UNALIGNED
+	return *(const uint32_t *)src;
+#else
+	const uint8_t *buf = src;
+	return (uint32_t)buf[0]
+		| ((uint32_t)buf[1] << 8)
+		| ((uint32_t)buf[2] << 16)
+		| ((uint32_t)buf[3] << 24);
+#endif
+}
+
+static inline void
+enc32le(void *dst, uint32_t x)
+{
+#if HAWK_LE && HAWK_UNALIGNED
+	*(uint32_t *)dst = x;
+#else
+	uint8_t *buf = dst;
+	buf[0] = (uint8_t)x;
+	buf[1] = (uint8_t)(x >> 8);
+	buf[2] = (uint8_t)(x >> 16);
+	buf[3] = (uint8_t)(x >> 24);
+#endif
+}
+
+static inline uint64_t
+dec64le(const void *src)
+{
+#if HAWK_LE && HAWK_UNALIGNED
+	return *(const uint64_t *)src;
+#else
+	const uint8_t *buf = src;
+	return (uint64_t)buf[0]
+		| ((uint64_t)buf[1] << 8)
+		| ((uint64_t)buf[2] << 16)
+		| ((uint64_t)buf[3] << 24)
+		| ((uint64_t)buf[4] << 32)
+		| ((uint64_t)buf[5] << 40)
+		| ((uint64_t)buf[6] << 48)
+		| ((uint64_t)buf[7] << 56);
+#endif
+}
+
+static inline void
+enc64le(void *dst, uint64_t x)
+{
+#if HAWK_LE && HAWK_UNALIGNED
+	*(uint64_t *)dst = x;
+#else
+	uint8_t *buf = dst;
+	buf[0] = (uint8_t)x;
+	buf[1] = (uint8_t)(x >> 8);
+	buf[2] = (uint8_t)(x >> 16);
+	buf[3] = (uint8_t)(x >> 24);
+	buf[4] = (uint8_t)(x >> 32);
+	buf[5] = (uint8_t)(x >> 40);
+	buf[6] = (uint8_t)(x >> 48);
+	buf[7] = (uint8_t)(x >> 56);
+#endif
+}
+
+/*
+ * Extend the top bit (sign bit) of the input into a full word (i.e.
+ * result is 0xFFFFFFFF or 0x00000000).
+ */
+static inline uint32_t
+tbmask(uint32_t x)
+{
+	return (uint32_t)(*(int32_t *)&x >> 31);
+}
+
+#if HAWK_DEBUG
+/*
+ * Debug functions, to print internal values. Do NOT use in production code.
+ * The 'u1' variant uses one bit per coefficient.
+ */
+#include <stdio.h>
+
+static inline void
+print_blob(const char *name, const void *data, size_t len)
+{
+	printf("%s = ", name);
+	for (size_t u = 0; u < len; u ++) {
+		printf("%02x", ((const uint8_t *)data)[u]);
+	}
+	printf("\n");
+}
+
+static inline void
+print_u1(unsigned logn, const char *name, const uint8_t *p)
+{
+	printf("%s = [%u", name, p[0] & 1);
+	size_t n = (size_t)1 << logn;
+	for (size_t u = 1; u < n; u ++) {
+		printf(",%u", (p[u >> 3] >> (u & 7)) & 1);
+	}
+	printf("]\n");
+}
+
+static inline void
+print_i8(unsigned logn, const char *name, const int8_t *p)
+{
+	printf("%s = [%d", name, p[0]);
+	size_t n = (size_t)1 << logn;
+	for (size_t u = 1; u < n; u ++) {
+		printf(",%d", p[u]);
+	}
+	printf("]\n");
+}
+
+static inline void
+print_i16(unsigned logn, const char *name, const int16_t *p)
+{
+	printf("%s = [%d", name, p[0]);
+	size_t n = (size_t)1 << logn;
+	for (size_t u = 1; u < n; u ++) {
+		printf(",%d", p[u]);
+	}
+	printf("]\n");
+}
+
+static inline void
+print_u16(unsigned logn, const char *name, const uint16_t *p)
+{
+	printf("%s = [%u", name, p[0]);
+	size_t n = (size_t)1 << logn;
+	for (size_t u = 1; u < n; u ++) {
+		printf(",%u", p[u]);
+	}
+	printf("]\n");
+}
+
+static inline void
+print_i32(unsigned logn, const char *name, const int32_t *p)
+{
+	printf("%s = [%ld", name, (long)p[0]);
+	size_t n = (size_t)1 << logn;
+	for (size_t u = 1; u < n; u ++) {
+		printf(",%ld", (long)p[u]);
+	}
+	printf("]\n");
+}
+
+static inline void
+print_u32(unsigned logn, const char *name, const uint32_t *p)
+{
+	printf("%s = [%lu", name, (unsigned long)p[0]);
+	size_t n = (size_t)1 << logn;
+	for (size_t u = 1; u < n; u ++) {
+		printf(",%lu", (unsigned long)p[u]);
+	}
+	printf("]\n");
+}
+
+#endif
+
+/* ==================================================================== */
+
+#endif
diff --git a/lib/dns/hawk/hawk_kgen.c b/lib/dns/hawk/hawk_kgen.c
new file mode 100644
index 0000000000..51231e8526
--- /dev/null
+++ b/lib/dns/hawk/hawk_kgen.c
@@ -0,0 +1,318 @@
+#include "hawk_inner.h"
+
+/*
+ * Reduce f modulo 2; output is a sequence of n bits (n/8 bytes).
+ */
+TARGET_AVX2
+static void
+extract_lowbit(unsigned logn, uint8_t *restrict f2, const int8_t *restrict f)
+{
+	size_t n = (size_t)1 << logn;
+#if HAWK_AVX2
+	for (size_t u = 0; u < n; u += 32) {
+		__m256i x = _mm256_loadu_si256((const __m256i *)(f + u));
+		x = _mm256_slli_epi16(x, 7);
+		*(uint32_t *)(f2 + (u >> 3)) = _mm256_movemask_epi8(x);
+	}
+#else // HAWK_AVX2
+	const uint8_t *fu = (const uint8_t *)f;
+	for (size_t u = 0; u < n; u += 8) {
+		f2[u >> 3] = (fu[u + 0] & 1u)
+			| ((fu[u + 1] & 1u) << 1)
+			| ((fu[u + 2] & 1u) << 2)
+			| ((fu[u + 3] & 1u) << 3)
+			| ((fu[u + 4] & 1u) << 4)
+			| ((fu[u + 5] & 1u) << 5)
+			| ((fu[u + 6] & 1u) << 6)
+			| ((fu[u + 7] & 1u) << 7);
+	}
+#endif // HAWK_AVX2
+}
+
+/*
+ * Encode the private key.
+ */
+static size_t
+encode_private(unsigned logn, void *restrict dst,
+	const void *seed, const int8_t *restrict F, const int8_t *restrict G,
+	const void *pub, size_t pub_len)
+{
+	size_t n = (size_t)1 << logn;
+	uint8_t *buf = (uint8_t *)dst;
+
+	/*
+	 * Seed.
+	 */
+	size_t seed_len = 8 + ((size_t)1 << (logn - 5));
+	memcpy(buf, seed, seed_len);
+	buf += seed_len;
+
+	/*
+	 * F mod 2 and G mod 2.
+	 */
+	extract_lowbit(logn, buf, F);
+	buf += (n >> 3);
+	extract_lowbit(logn, buf, G);
+	buf += (n >> 3);
+
+	/*
+	 * Public key hash (SHAKE256; output length is {128, 256, 512} bits
+	 * depending on degree).
+	 */
+	size_t hpub_len = (size_t)1 << (logn - 4);
+	shake_context sc;
+	shake_init(&sc, 256);
+	shake_inject(&sc, pub, pub_len);
+	shake_flip(&sc);
+	shake_extract(&sc, buf, hpub_len);
+	buf += hpub_len;
+
+	return (size_t)(buf - (uint8_t *)dst);
+}
+
+/*
+ * Width of the fixed-sized part of the encoding of a coefficient of q00
+ * or q01 (excluding the sign bit).
+ */
+static const int8_t low_bits_q00[11] = {
+	0, 0, 0, 0, 0, 0, 0, 0,  5,  5,  6
+};
+static const int8_t low_bits_q01[11] = {
+	0, 0, 0, 0, 0, 0, 0, 0,  8,  9, 10
+};
+
+/*
+ * Golomb-Rice encoding, with part segregation (sign bits, fixed-size
+ * parts, variable-size parts). The fixed-size part has size 'low' bits.
+ * The ignored bits in the last byte are set to 0, and their number is
+ * written in *num_ignored (0 to 7). The total number of written bytes
+ * is returned.
+ *
+ * If the encoded size would exceed dst_len, then the process fails
+ * and the function returns 0.
+ */
+static size_t
+encode_gr(unsigned logn, void *restrict dst, size_t dst_len,
+	const int16_t *restrict a, int low, int *num_ignored)
+{
+	size_t n = (size_t)1 << logn;
+	uint8_t *buf = (uint8_t *)dst;
+	size_t buf_len = dst_len;
+	if (buf_len < ((size_t)(low + 1) << (logn - 3))) {
+		return 0;
+	}
+
+	/* Sign bits */
+	for (size_t u = 0; u < n; u += 8) {
+		unsigned x = 0;
+		for (size_t v = 0; v < 8; v ++) {
+			x |= (*(const uint16_t *)(a + u + v) >> 15) << v;
+		}
+		buf[u >> 3] = x;
+	}
+	buf += (n >> 3);
+	buf_len -= (n >> 3);
+
+	/* Fixed-size parts */
+	uint32_t low_mask = ((uint32_t)1 << low) - 1;
+	if (low <= 8) {
+		for (size_t u = 0; u < n; u += 8) {
+			uint64_t x = 0;
+			for (size_t v = 0, vv = 0; v < 8; v ++, vv += low) {
+				uint32_t w = (uint32_t)a[u + v];
+				w ^= tbmask(w);
+				x |= (uint64_t)(w & low_mask) << vv;
+			}
+			for (int i = 0; i < (low << 3); i += 8) {
+				*buf ++ = (uint8_t)(x >> i);
+			}
+		}
+	} else {
+		for (size_t u = 0; u < n; u += 8) {
+			uint64_t x0 = 0;
+			for (size_t v = 0, vv = 0; v < 4; v ++, vv += low) {
+				uint32_t w = (uint32_t)a[u + v];
+				w ^= tbmask(w);
+				x0 |= (uint64_t)(w & low_mask) << vv;
+			}
+			uint64_t x1 = 0;
+			for (size_t v = 4, vv = 0; v < 8; v ++, vv += low)
+			{
+				uint32_t w = (uint32_t)a[u + v];
+				w ^= tbmask(w);
+				x1 |= (uint64_t)(w & low_mask) << vv;
+			}
+			x0 |= x1 << (low << 2);
+			x1 >>= 64 - (low << 2);
+			for (int i = 0; i < 64; i += 8) {
+				*buf ++ = (uint8_t)(x0 >> i);
+			}
+			for (int i = 0; i < (low << 3) - 64; i += 8) {
+				*buf ++ = (uint8_t)(x1 >> i);
+			}
+		}
+	}
+	buf_len -= (size_t)low << (logn - 3);
+
+	/* Variable-size parts */
+	uint32_t acc = 0;
+	int acc_len = 0;
+	for (size_t u = 0; u < n; u ++) {
+		uint32_t w = (uint32_t)a[u];
+		int k = (int)((w ^ tbmask(w)) >> low);
+		acc |= (uint32_t)1 << (acc_len + k);
+		acc_len += 1 + k;
+		while (acc_len >= 8) {
+			if (buf_len == 0) {
+				return 0;
+			}
+			*buf ++ = (uint8_t)acc;
+			buf_len --;
+			acc >>= 8;
+			acc_len -= 8;
+		}
+	}
+	if (acc_len > 0) {
+		if (buf_len == 0) {
+			return 0;
+		}
+		*buf ++ = (uint8_t)acc;
+		buf_len --;
+	}
+	if (num_ignored != NULL) {
+		*num_ignored = (unsigned)-acc_len & 7;
+	}
+
+	return dst_len - buf_len;
+}
+
+/*
+ * Encode the public key. Returned value is 1 on success, 0 on error;
+ * an error is returned if the encoded key size would exceed the fixed
+ * public key size (HAWK_PUBKEY_SIZE(logn)).
+ *
+ * Note: this function temporarily modifies q00[0], but resets it to
+ * its original value afterwards.
+ */
+static int
+encode_public(unsigned logn,
+	void *restrict dst, size_t dst_len,
+	int16_t *restrict q00, const int16_t *restrict q01)
+{
+	/*
+	 * General format:
+	 *   q00
+	 *   q01
+	 *   padding
+	 * q00 and q01 both use Golomb-Rice coding.
+	 *
+	 * Special handling of q00[0]: since it has a larger possible
+	 * range than the rest of the coefficients, it is temporarily
+	 * downscaled (q00[0] is modified, but the original value is put
+	 * back afterwards). The extra bits are appended at the end of the
+	 * encoding of q00.
+	 */
+
+	int low00 = low_bits_q00[logn];
+	int low01 = low_bits_q01[logn];
+	int eb00_len = 16 - (low00 + 4);
+	uint8_t *buf = (uint8_t *)dst;
+	size_t buf_len = dst_len;
+
+	/* q00 */
+	int ni;
+	int sav_q00 = q00[0];
+	q00[0] >>= eb00_len;
+	size_t len00 = encode_gr(logn - 1, buf, buf_len, q00, low00, &ni);
+	q00[0] = sav_q00;
+	if (len00 == 0) {
+		return 0;
+	}
+	/* Extra bits of q00[0] */
+	uint32_t eb00 = (uint32_t)sav_q00 & (((uint32_t)1 << eb00_len) - 1);
+	if (eb00_len <= ni) {
+		buf[len00 - 1] |= eb00 << (8 - ni);
+	} else {
+		if (len00 >= buf_len) {
+			return 0;
+		}
+		buf[len00 - 1] |= eb00 << (8 - ni);
+		buf[len00] = eb00 >> ni;
+		len00 ++;
+	}
+	buf += len00;
+	buf_len -= len00;
+
+	size_t len01 = encode_gr(logn, buf, buf_len, q01, low01, NULL);
+	if (len01 == 0) {
+		return 0;
+	}
+	buf += len01;
+	buf_len -= len01;
+
+	/* Padding to the requested length. */
+	memset(buf, 0, buf_len);
+	return 1;
+}
+
+/* see hawk.h */
+int
+Zh(keygen)(unsigned logn, void *restrict priv, void *restrict pub,
+	hawk_rng rng, void *restrict rng_context,
+	void *restrict tmp, size_t tmp_len)
+{
+	if (logn < 8 || logn > 10) {
+		return 0;
+	}
+	size_t n = (size_t)1 << logn;
+	if (tmp_len < 7 + 2 * n) {
+		return 0;
+	}
+	int8_t *f = (int8_t *)(((uintptr_t)tmp + 7) & ~(uintptr_t)7);
+	int8_t *g = f + n;
+	int8_t *tt8 = g + n;
+	int8_t *F = tt8;
+	int8_t *G = F + n;
+	int16_t *q00 = (int16_t *)(G + n);
+	int16_t *q01 = q00 + n;
+	int32_t *q11 = (int32_t *)(q01 + n);
+	uint8_t *seed = (uint8_t *)(q11 + n);
+	size_t seed_len = 8 + ((size_t)1 << (logn - 5));
+	size_t priv_len = HAWK_PRIVKEY_SIZE(logn);
+	size_t pub_len = HAWK_PUBKEY_SIZE(logn);
+	uint8_t *tpriv = seed + seed_len;
+	uint8_t *tpub = tpriv + priv_len;
+	for (;;) {
+		if (Hawk_keygen(logn, f, g, 0, 0, 0, 0, 0, 0, rng, rng_context,
+			tt8, (size_t)(((int8_t *)tmp + tmp_len) - tt8)) != 0)
+		{
+			return 0;
+		}
+
+		if (encode_public(logn, tpub, pub_len, q00, q01)) {
+			(void)encode_private(logn, tpriv,
+				seed, F, G, tpub, pub_len);
+#if HAWK_DEBUG
+			printf("#### Keygen (n=%u):\n", 1u << logn);
+			print_blob("kgseed", seed, seed_len);
+			print_i8(logn, "f", f);
+			print_i8(logn, "g", g);
+			print_i8(logn, "F", F);
+			print_i8(logn, "G", G);
+			print_i16(logn, "q00", q00);
+			print_i16(logn, "q01", q01);
+			print_i32(logn, "q11", q11);
+			print_blob("priv", tpriv, priv_len);
+			print_blob("pub", tpub, pub_len);
+#endif
+			if (priv != NULL) {
+				memcpy(priv, tpriv, priv_len);
+			}
+			if (pub != NULL) {
+				memcpy(pub, tpub, pub_len);
+			}
+			memmove(tmp, tpriv, priv_len + pub_len);
+			return 1;
+		}
+	}
+}
diff --git a/lib/dns/hawk/hawk_sign.c b/lib/dns/hawk/hawk_sign.c
new file mode 100644
index 0000000000..b3fc53ffae
--- /dev/null
+++ b/lib/dns/hawk/hawk_sign.c
@@ -0,0 +1,1978 @@
+#include "hawk_inner.h"
+
+/*
+ * We use computations with polynomials modulo X^n+1 and modulo 18433.
+ */
+#define Q   18433
+#include "modq.h"
+
+#if HAWK_AVX2
+/*
+ * Binary polynomials (GF(2)[X]); inputs a and b have size N bits;
+ * output d has size N or 2*N bits, depending on the operation:
+ *
+ *    bp_xor_N():     d <- a XOR b
+ *    bp_mul_N():     d <- a*b
+ *    bp_mulmod_N():  d <- a*b mod X^N+1
+ *
+ * Multiplications use Karatsuba:
+ * If:
+ *    a = a0 + a1*(X^(N/2))
+ *    b = b0 + b1*(X^(N/2))
+ * Then:
+ *    d0 <- a0*b0
+ *    d1 <- a1*b1
+ *    d2 <- (a0 + a1)*(b0 + b1) - d0 - d1
+ *    d <- d0 + d1*X^(N/2) + d2*X^N
+ * Addition and subtraction are both XOR for binary polynomials; there is
+ * no carry, so no need for an extra bit.
+ *
+ * AVX2-aware systems also support the pclmulq opcode (which appeared with
+ * the AES extensions, to support GCM); that opcode computes a 64x64->128
+ * binary polynomial multiplication. It words only over the low lanes of
+ * registers, so we stick to SSE2 (128-bit registers, not 256-bit).
+ *
+ * The 'tmp' parameter provided to bp_mulmod_N() is ignored.
+ */
+
+TARGET_AVX2
+static inline void
+bp_xor_256(uint8_t *d, const uint8_t *a, const uint8_t *b)
+{
+	__m256i ya = _mm256_loadu_si256((const __m256i *)a);
+	__m256i yb = _mm256_loadu_si256((const __m256i *)b);
+	_mm256_storeu_si256((__m256i *)d, _mm256_xor_si256(ya, yb));
+}
+
+TARGET_AVX2
+static inline void
+bp_xor_512(uint8_t *d, const uint8_t *a, const uint8_t *b)
+{
+	__m256i ya0 = _mm256_loadu_si256((const __m256i *)a + 0);
+	__m256i ya1 = _mm256_loadu_si256((const __m256i *)a + 1);
+	__m256i yb0 = _mm256_loadu_si256((const __m256i *)b + 0);
+	__m256i yb1 = _mm256_loadu_si256((const __m256i *)b + 1);
+	_mm256_storeu_si256((__m256i *)d + 0, _mm256_xor_si256(ya0, yb0));
+	_mm256_storeu_si256((__m256i *)d + 1, _mm256_xor_si256(ya1, yb1));
+}
+
+TARGET_AVX2
+static inline void
+bp_xor_1024(uint8_t *d, const uint8_t *a, const uint8_t *b)
+{
+	__m256i ya0 = _mm256_loadu_si256((const __m256i *)a + 0);
+	__m256i ya1 = _mm256_loadu_si256((const __m256i *)a + 1);
+	__m256i ya2 = _mm256_loadu_si256((const __m256i *)a + 2);
+	__m256i ya3 = _mm256_loadu_si256((const __m256i *)a + 3);
+	__m256i yb0 = _mm256_loadu_si256((const __m256i *)b + 0);
+	__m256i yb1 = _mm256_loadu_si256((const __m256i *)b + 1);
+	__m256i yb2 = _mm256_loadu_si256((const __m256i *)b + 2);
+	__m256i yb3 = _mm256_loadu_si256((const __m256i *)b + 3);
+	_mm256_storeu_si256((__m256i *)d + 0, _mm256_xor_si256(ya0, yb0));
+	_mm256_storeu_si256((__m256i *)d + 1, _mm256_xor_si256(ya1, yb1));
+	_mm256_storeu_si256((__m256i *)d + 2, _mm256_xor_si256(ya2, yb2));
+	_mm256_storeu_si256((__m256i *)d + 3, _mm256_xor_si256(ya3, yb3));
+}
+
+TARGET_AVX2
+static inline void
+bp_mul_256(__m128i *xd, const __m128i *xa, const __m128i *xb)
+{
+	/* (e0, e1, e2) <- (a0l*b0l, a0h*b0h, a0l*b0h + a0h*b0l) */
+	__m128i xe0 = _mm_clmulepi64_si128(xa[0], xb[0], 0x00);
+	__m128i xe1 = _mm_clmulepi64_si128(xa[0], xb[0], 0x11);
+	__m128i xe2 = _mm_xor_si128(
+		_mm_clmulepi64_si128(xa[0], xb[0], 0x01),
+		_mm_clmulepi64_si128(xa[0], xb[0], 0x10));
+
+	/* (f0, f1, f2) <- (a1l*b1l, a1h*b1h, a1l*b1h + a1h*b1l) */
+	__m128i xf0 = _mm_clmulepi64_si128(xa[1], xb[1], 0x00);
+	__m128i xf1 = _mm_clmulepi64_si128(xa[1], xb[1], 0x11);
+	__m128i xf2 = _mm_xor_si128(
+		_mm_clmulepi64_si128(xa[1], xb[1], 0x01),
+		_mm_clmulepi64_si128(xa[1], xb[1], 0x10));
+
+	/* (u, v) <- (a0 + a1, b0 + b1)
+	   (g0, g1, g2) <- (ul*vl, uh*vh, ul*vh + uh*bl */
+	__m128i xaa = _mm_xor_si128(xa[0], xa[1]);
+	__m128i xbb = _mm_xor_si128(xb[0], xb[1]);
+	__m128i xg0 = _mm_clmulepi64_si128(xaa, xbb, 0x00);
+	__m128i xg1 = _mm_clmulepi64_si128(xaa, xbb, 0x11);
+	__m128i xg2 = _mm_xor_si128(
+		_mm_clmulepi64_si128(xaa, xbb, 0x01),
+		_mm_clmulepi64_si128(xaa, xbb, 0x10));
+
+	/* a0*b0 = e0 + e2*X^64 + e1*X^128
+	   a1*b1 = f0 + f2*X^64 + f1*X^128
+	   a0*b1 + a1*b0 = g0 + g2*X^64 + g1*X^128 + a0*b0 + a1*b1 */
+	__m128i xhh = _mm_xor_si128(xe1, xf0);
+	__m128i xjj = _mm_xor_si128(xg2, _mm_xor_si128(xe2, xf2));
+	xd[0] = _mm_xor_si128(xe0, _mm_bslli_si128(xe2, 8));
+	xd[1] = _mm_xor_si128(
+		_mm_xor_si128(xe0, xhh),
+		_mm_xor_si128(xg0, _mm_alignr_epi8(xjj, xe2, 8)));
+	xd[2] = _mm_xor_si128(
+		_mm_xor_si128(xhh, xf1),
+		_mm_xor_si128(xg1, _mm_alignr_epi8(xf2, xjj, 8)));
+	xd[3] = _mm_xor_si128(xf1, _mm_bsrli_si128(xf2, 8));
+}
+
+TARGET_AVX2
+static void
+bp_mul_512(__m128i *xd, const __m128i *xa, const __m128i *xb)
+{
+	__m128i xaa[2], xbb[2], xe[4], xf[4], xg[4];
+	bp_mul_256(xe, xa, xb);
+	bp_mul_256(xf, xa + 2, xb + 2);
+	for (int i = 0; i < 2; i ++) {
+		xaa[i] = _mm_xor_si128(xa[i], xa[i + 2]);
+		xbb[i] = _mm_xor_si128(xb[i], xb[i + 2]);
+	}
+	bp_mul_256(xg, xaa, xbb);
+	for (int i = 0; i < 4; i ++) {
+		xg[i] = _mm_xor_si128(xg[i], _mm_xor_si128(xe[i], xf[i]));
+	}
+	xd[0] = xe[0];
+	xd[1] = xe[1];
+	xd[2] = _mm_xor_si128(xe[2], xg[0]);
+	xd[3] = _mm_xor_si128(xe[3], xg[1]);
+	xd[4] = _mm_xor_si128(xf[0], xg[2]);
+	xd[5] = _mm_xor_si128(xf[1], xg[3]);
+	xd[6] = xf[2];
+	xd[7] = xf[3];
+}
+
+TARGET_AVX2
+static void
+bp_mul_1024(__m128i *xd, const __m128i *xa, const __m128i *xb)
+{
+	__m128i xaa[4], xbb[4], xe[8], xf[8], xg[8];
+	bp_mul_512(xe, xa, xb);
+	bp_mul_512(xf, xa + 4, xb + 4);
+	for (int i = 0; i < 4; i ++) {
+		xaa[i] = _mm_xor_si128(xa[i], xa[i + 4]);
+		xbb[i] = _mm_xor_si128(xb[i], xb[i + 4]);
+	}
+	bp_mul_512(xg, xaa, xbb);
+	for (int i = 0; i < 8; i ++) {
+		xg[i] = _mm_xor_si128(xg[i], _mm_xor_si128(xe[i], xf[i]));
+	}
+	for (int i = 0; i < 4; i ++) {
+		xd[i] = xe[i];
+	}
+	for (int i = 4; i < 8; i ++) {
+		xd[i] = _mm_xor_si128(xe[i], xg[i - 4]);
+	}
+	for (int i = 8; i < 12; i ++) {
+		xd[i] = _mm_xor_si128(xf[i - 8], xg[i - 4]);
+	}
+	for (int i = 12; i < 16; i ++) {
+		xd[i] = xf[i - 8];
+	}
+}
+
+TARGET_AVX2
+static void
+bp_mulmod_256(uint8_t *d, const uint8_t *a, const uint8_t *b, uint8_t *tmp)
+{
+	(void)tmp;
+	__m128i xa[2], xb[2], xd[4];
+	xa[0] = _mm_loadu_si128((const __m128i *)a + 0);
+	xa[1] = _mm_loadu_si128((const __m128i *)a + 1);
+	xb[0] = _mm_loadu_si128((const __m128i *)b + 0);
+	xb[1] = _mm_loadu_si128((const __m128i *)b + 1);
+	bp_mul_256(xd, xa, xb);
+	_mm_storeu_si128((__m128i *)d + 0, _mm_xor_si128(xd[0], xd[2]));
+	_mm_storeu_si128((__m128i *)d + 1, _mm_xor_si128(xd[1], xd[3]));
+}
+
+TARGET_AVX2
+static void
+bp_mulmod_512(uint8_t *d, const uint8_t *a, const uint8_t *b, uint8_t *tmp)
+{
+	(void)tmp;
+	__m128i xa[4], xb[4], xd[8];
+	for (int i = 0; i < 4; i ++) {
+		xa[i] = _mm_loadu_si128((const __m128i *)a + i);
+		xb[i] = _mm_loadu_si128((const __m128i *)b + i);
+	}
+	bp_mul_512(xd, xa, xb);
+	for (int i = 0; i < 4; i ++) {
+		_mm_storeu_si128((__m128i *)d + i,
+			_mm_xor_si128(xd[i], xd[i + 4]));
+	}
+}
+
+TARGET_AVX2
+static void
+bp_mulmod_1024(uint8_t *d, const uint8_t *a, const uint8_t *b, uint8_t *tmp)
+{
+	(void)tmp;
+	__m128i xa[8], xb[8], xd[16];
+	for (int i = 0; i < 8; i ++) {
+		xa[i] = _mm_loadu_si128((const __m128i *)a + i);
+		xb[i] = _mm_loadu_si128((const __m128i *)b + i);
+	}
+	bp_mul_1024(xd, xa, xb);
+	for (int i = 0; i < 8; i ++) {
+		_mm_storeu_si128((__m128i *)d + i,
+			_mm_xor_si128(xd[i], xd[i + 8]));
+	}
+}
+
+/*
+ * Reduce a polynomial modulo 2 (output f2 is n/8 bytes = n bits).
+ */
+TARGET_AVX2
+static void
+extract_lowbit(unsigned logn, uint8_t *f2, const int8_t *f)
+{
+	size_t n = (size_t)1 << logn;
+	for (size_t u = 0; u < n; u += 32) {
+		__m256i x = _mm256_loadu_si256((const __m256i *)(f + u));
+		x = _mm256_slli_epi16(x, 7);
+		*(uint32_t *)(f2 + (u >> 3)) = _mm256_movemask_epi8(x);
+	}
+}
+
+#else // HAWK_AVX2
+/*
+ * Binary polynomials (GF(2)[X]); inputs a and b have size N bits;
+ * output d has size N or 2*N bits, depending on the operation:
+ *
+ *    bp_xor_N():     d <- a XOR b
+ *    bp_mul_N():     d <- a*b
+ *    bp_muladd_N():  d <- d + a*b
+ *    bp_mulmod_N():  d <- a*b mod X^N+1
+ *
+ * Multiplications use Karatsuba:
+ * If:
+ *    a = a0 + a1*(X^(N/2))
+ *    b = b0 + b1*(X^(N/2))
+ * Then:
+ *    d0 <- a0*b0
+ *    d1 <- a1*b1
+ *    d2 <- (a0 + a1)*(b0 + b1) - d0 - d1
+ *    d <- d0 + d1*X^(N/2) + d2*X^N
+ * Addition and subtraction are both XOR for binary polynomials; there is
+ * no carry, so no need for an extra bit.
+ *
+ * bp_muladd_N() may use up to 4*N bits in the provided 'tmp' buffer.
+ * bp_modadd_N() may use up to 3*N bits in the provided 'tmp' buffer.
+ */
+
+/*
+ * Multiplication of two binary polynomials ("carryless multiplication")
+ * of degree less than 32, result has degree less than 64.
+ */
+static uint64_t
+bp_mul_32(uint32_t x, uint32_t y)
+{
+	/*
+	 * Classic technique (rediscovered many times): integer
+	 * multiplications with "holes" to absorb unwanted carries.
+	 */
+	uint32_t x0 = x & 0x11111111;
+	uint32_t x1 = x & 0x22222222;
+	uint32_t x2 = x & 0x44444444;
+	uint32_t x3 = x & 0x88888888;
+
+	uint32_t y0 = y & 0x11111111;
+	uint32_t y1 = y & 0x22222222;
+	uint32_t y2 = y & 0x44444444;
+	uint32_t y3 = y & 0x88888888;
+
+#define M(a, b)   ((uint64_t)(a) * (uint64_t)(b))
+
+	uint64_t z0 = M(x0, y0) ^ M(x1, y3) ^ M(x2, y2) ^ M(x3, y1);
+	uint64_t z1 = M(x0, y1) ^ M(x1, y0) ^ M(x2, y3) ^ M(x3, y2);
+	uint64_t z2 = M(x0, y2) ^ M(x1, y1) ^ M(x2, y0) ^ M(x3, y3);
+	uint64_t z3 = M(x0, y3) ^ M(x1, y2) ^ M(x2, y1) ^ M(x3, y0);
+
+#undef M
+
+	z0 &= (uint64_t)0x1111111111111111;
+	z1 &= (uint64_t)0x2222222222222222;
+	z2 &= (uint64_t)0x4444444444444444;
+	z3 &= (uint64_t)0x8888888888888888;
+	return z0 | z1 | z2 | z3;
+}
+
+static inline void
+bp_xor_64(uint8_t *d, const uint8_t *a, const uint8_t *b)
+{
+#if HAWK_UNALIGNED
+	*(uint64_t *)d = *(const uint64_t *)a ^ *(const uint64_t *)b;
+#else
+	for (size_t u = 0; u < 8; u ++) {
+		d[u] = a[u] ^ b[u];
+	}
+#endif
+}
+
+static inline void
+bp_xor_128(uint8_t *d, const uint8_t *a, const uint8_t *b)
+{
+#if HAWK_UNALIGNED
+	const uint64_t *aq = (uint64_t *)a;
+	const uint64_t *bq = (uint64_t *)b;
+	uint64_t *dq = (uint64_t *)d;
+	for (size_t u = 0; u < 2; u ++) {
+		dq[u] = aq[u] ^ bq[u];
+	}
+#else
+	for (size_t u = 0; u < 16; u ++) {
+		d[u] = a[u] ^ b[u];
+	}
+#endif
+}
+
+static inline void
+bp_xor_256(uint8_t *d, const uint8_t *a, const uint8_t *b)
+{
+#if HAWK_UNALIGNED
+	const uint64_t *aq = (uint64_t *)a;
+	const uint64_t *bq = (uint64_t *)b;
+	uint64_t *dq = (uint64_t *)d;
+	for (size_t u = 0; u < 4; u ++) {
+		dq[u] = aq[u] ^ bq[u];
+	}
+#else
+	for (size_t u = 0; u < 32; u ++) {
+		d[u] = a[u] ^ b[u];
+	}
+#endif
+}
+
+static inline void
+bp_xor_512(uint8_t *d, const uint8_t *a, const uint8_t *b)
+{
+#if HAWK_UNALIGNED
+	const uint64_t *aq = (uint64_t *)a;
+	const uint64_t *bq = (uint64_t *)b;
+	uint64_t *dq = (uint64_t *)d;
+	for (size_t u = 0; u < 8; u ++) {
+		dq[u] = aq[u] ^ bq[u];
+	}
+#else
+	for (size_t u = 0; u < 64; u ++) {
+		d[u] = a[u] ^ b[u];
+	}
+#endif
+}
+
+static inline void
+bp_xor_1024(uint8_t *d, const uint8_t *a, const uint8_t *b)
+{
+#if HAWK_UNALIGNED
+	const uint64_t *aq = (uint64_t *)a;
+	const uint64_t *bq = (uint64_t *)b;
+	uint64_t *dq = (uint64_t *)d;
+	for (size_t u = 0; u < 16; u ++) {
+		dq[u] = aq[u] ^ bq[u];
+	}
+#else
+	for (size_t u = 0; u < 128; u ++) {
+		d[u] = a[u] ^ b[u];
+	}
+#endif
+}
+
+static void
+bp_muladd_64(uint8_t *restrict d, const uint8_t *restrict a,
+	const uint8_t *restrict b, uint8_t *restrict tmp)
+{
+	(void)tmp;
+
+	uint32_t a0 = dec32le(a);
+	uint32_t a1 = dec32le(a + 4);
+	uint32_t b0 = dec32le(b);
+	uint32_t b1 = dec32le(b + 4);
+
+	uint64_t c0 = bp_mul_32(a0, b0);
+	uint64_t c1 = bp_mul_32(a1, b1);
+	uint64_t c2 = bp_mul_32(a0 ^ a1, b0 ^ b1) ^ c0 ^ c1;
+
+	enc64le(d, dec64le(d) ^ c0 ^ (c2 << 32));
+	enc64le(d + 8, dec64le(d + 8) ^ c1 ^ (c2 >> 32));
+}
+
+#define MKBP_MULADD(n, hn)    MKBP_MULADD_(n, hn)
+#define MKBP_MULADD_(n, hn) \
+static void \
+bp_muladd_ ## n(uint8_t *restrict d, const uint8_t *restrict a, \
+	const uint8_t *restrict b, uint8_t *restrict tmp) \
+{ \
+	uint8_t *t1 = tmp; \
+	uint8_t *t2 = t1 + (n / 8); \
+	uint8_t *t3 = t2 + (n / 8); \
+	/* t1 <- (a0 + a1)*(b0 + b1) + d0 + d1 */ \
+	bp_xor_ ## hn(t2, a, a + (hn / 8)); \
+	bp_xor_ ## hn(t2 + (hn / 8), b, b + (hn / 8)); \
+	bp_xor_ ## n(t1, d, d + (n / 8)); \
+	bp_muladd_ ## hn(t1, t2, t2 + (hn / 8), t3); \
+	/* d0 <- d0 + a0*b0 */ \
+	bp_muladd_ ## hn(d, a, b, t2); \
+	/* d1 <- d1 + a1*b1 */ \
+	bp_muladd_ ## hn(d + (n / 8), a + (hn / 8), b + (hn / 8), t2); \
+	/* t1 <- t1 + d0 + d1 = a0*b1 + a1*b0 */ \
+	bp_xor_ ## n(t1, t1, d); \
+	bp_xor_ ## n(t1, t1, d + (n / 8)); \
+	/* d <- d + (x^{n/2})*t1 */ \
+	bp_xor_ ## n(d + (hn / 8), d + (hn / 8), t1); \
+}
+
+MKBP_MULADD(128, 64)
+MKBP_MULADD(256, 128)
+MKBP_MULADD(512, 256)
+
+#define MKBP_MULMOD(n, hn)    MKBP_MULMOD_(n, hn)
+#define MKBP_MULMOD_(n, hn) \
+static void \
+bp_mulmod_ ## n(uint8_t *restrict d, const uint8_t *restrict a, \
+	const uint8_t *restrict b, uint8_t *restrict tmp) \
+{ \
+	uint8_t *t1 = tmp; \
+	uint8_t *t2 = t1 + (n / 8); \
+	/* t1 <- (a0 + a1)*(b0 + b1) */ \
+	bp_xor_ ## hn(d, a, a + (hn / 8)); \
+	bp_xor_ ## hn(d + (hn / 8), b, b + (hn / 8)); \
+	memset(t1, 0, (n / 8)); \
+	bp_muladd_ ## hn(t1, d, d + (hn / 8), t2); \
+	/* d <- a0*b0 + a1*b1 */ \
+	memset(d, 0, (n / 8)); \
+	bp_muladd_ ## hn(d, a, b, t2); \
+	bp_muladd_ ## hn(d, a + (hn / 8), b + (hn / 8), t2); \
+	/* t1 <- t1 + d = a0*b1 + a1*b0 */ \
+	bp_xor_ ## n(t1, t1, d); \
+	/* d <- d + rotate_{n/2}(t1) */ \
+	bp_xor_ ## hn(d, d, t1 + (hn / 8)); \
+	bp_xor_ ## hn(d + (hn / 8), d + (hn / 8), t1); \
+}
+
+MKBP_MULMOD(256, 128)
+MKBP_MULMOD(512, 256)
+MKBP_MULMOD(1024, 512)
+
+/*
+ * Reduce a polynomial modulo 2 (output f2 is n/8 bytes = n bits).
+ */
+static void
+extract_lowbit(unsigned logn, uint8_t *f2, const int8_t *f)
+{
+	size_t n = (size_t)1 << logn;
+	const uint8_t *fu = (const uint8_t *)f;
+	for (size_t u = 0; u < n; u += 8) {
+		f2[u >> 3] = (fu[u + 0] & 1)
+			| ((fu[u + 1] & 1) << 1)
+			| ((fu[u + 2] & 1) << 2)
+			| ((fu[u + 3] & 1) << 3)
+			| ((fu[u + 4] & 1) << 4)
+			| ((fu[u + 5] & 1) << 5)
+			| ((fu[u + 6] & 1) << 6)
+			| ((fu[u + 7] & 1) << 7);
+	}
+}
+
+#endif // HAWK_AVX2
+
+/*
+ * (t0, t1) <- B*h  (mod 2)
+ * B = [[f F],[g G]]
+ * Only the low bit of each coefficient of f, g, F and G is provided;
+ * parameters f2, g2, F2 and G2 are arrays of n/8 bytes (n bits) each.
+ * Inputs h0, h1 and outputs t0, t1 are also arrays of n/8 bytes.
+ *
+ * tmp usage: at most n/2 bytes (4*n bits)
+ */
+static void
+basis_m2_mul(unsigned logn, uint8_t *restrict t0, uint8_t *restrict t1,
+	const uint8_t *restrict h0, const uint8_t *restrict h1,
+	const uint8_t *restrict f2, const uint8_t *restrict g2,
+	const uint8_t *restrict F2, const uint8_t *restrict G2,
+	uint8_t *tmp)
+{
+	/*
+	 * t0 = f*h0 + F*h1
+	 * t1 = g*h0 + G*h1
+	 */
+	size_t n = (size_t)1 << logn;
+	uint8_t *w1 = tmp;
+	uint8_t *w2 = w1 + (n >> 3);
+	switch (logn) {
+	case 8:
+		bp_mulmod_256(t0, h0, f2, w2);
+		bp_mulmod_256(w1, h1, F2, w2);
+		bp_xor_256(t0, t0, w1);
+		bp_mulmod_256(t1, h0, g2, w2);
+		bp_mulmod_256(w1, h1, G2, w2);
+		bp_xor_256(t1, t1, w1);
+		break;
+	case 9:
+		bp_mulmod_512(t0, h0, f2, w2);
+		bp_mulmod_512(w1, h1, F2, w2);
+		bp_xor_512(t0, t0, w1);
+		bp_mulmod_512(t1, h0, g2, w2);
+		bp_mulmod_512(w1, h1, G2, w2);
+		bp_xor_512(t1, t1, w1);
+		break;
+	case 10:
+		bp_mulmod_1024(t0, h0, f2, w2);
+		bp_mulmod_1024(w1, h1, F2, w2);
+		bp_xor_1024(t0, t0, w1);
+		bp_mulmod_1024(t1, h0, g2, w2);
+		bp_mulmod_1024(w1, h1, G2, w2);
+		bp_xor_1024(t1, t1, w1);
+		break;
+	}
+}
+
+/*
+ * Tables for the Gaussian sampler: we have two distributions over 2*Z,
+ * with the same standard deviation 2*sigma. Given:
+ *    p(x) = exp(-(x^2) / 2*(2*sigma)^2)
+ * Then, for integers k:
+ *    D0(2*k) = p(2*k) / \sum_j p(2*j)
+ *    D1(2*k) = p(2*k-1) / \sum_j p(2*j-1)
+ * D0 is centred on 0, while D1 is centred on 1. Both distributions only
+ * return even integers.
+ *
+ * Let P0(x) = P(|X0| >= x)  (with X0 selected with distribution D0).
+ * Let P1(x) = P(|X1| >= x)  (with X1 selected with distribution D1).
+ * For integers k >= 0, we define the table T:
+ *    T[2*k]   = floor(P0(2*(k+1)) * 2^78)
+ *    T[2*k+1] = floor(P1(2*(k+1)+1) * 2^78)
+ * Each 78-bit value is split into a high part ("hi") of 15 bits, and
+ * a low part ("lo") of 63 bits.
+ */
+
+static const uint16_t sig_gauss_hi_Hawk_256[] = {
+	0x4D70, 0x268B,
+	0x0F80, 0x04FA,
+	0x0144, 0x0041,
+	0x000A, 0x0001
+};
+#define SG_MAX_HI_Hawk_256  ((sizeof sig_gauss_hi_Hawk_256) / sizeof(uint16_t))
+
+ALIGNED_AVX2
+static const uint64_t sig_gauss_lo_Hawk_256[] = {
+	0x71FBD58485D45050, 0x1408A4B181C718B1,
+	0x54114F1DC2FA7AC9, 0x614569CC54722DC9,
+	0x42F74ADDA0B5AE61, 0x151C5CDCBAFF49A3,
+	0x252E2152AB5D758B, 0x23460C30AC398322,
+	0x0FDE62196C1718FC, 0x01355A8330C44097,
+	0x00127325DDF8CEBA, 0x0000DC8DE401FD12,
+	0x000008100822C548, 0x0000003B0FFB28F0,
+	0x0000000152A6E9AE, 0x0000000005EFCD99,
+	0x000000000014DA4A, 0x0000000000003953,
+	0x000000000000007B, 0x0000000000000000
+};
+#define SG_MAX_LO_Hawk_256  ((sizeof sig_gauss_lo_Hawk_256) / sizeof(uint64_t))
+
+static const uint16_t sig_gauss_hi_Hawk_512[] = {
+	0x580B, 0x35F9,
+	0x1D34, 0x0DD7,
+	0x05B7, 0x020C,
+	0x00A2, 0x002B,
+	0x000A, 0x0001
+};
+#define SG_MAX_HI_Hawk_512  ((sizeof sig_gauss_hi_Hawk_512) / sizeof(uint16_t))
+
+ALIGNED_AVX2
+static const uint64_t sig_gauss_lo_Hawk_512[] = {
+	0x0C27920A04F8F267, 0x3C689D9213449DC9,
+	0x1C4FF17C204AA058, 0x7B908C81FCE3524F,
+	0x5E63263BE0098FFD, 0x4EBEFD8FF4F07378,
+	0x56AEDFB0876A3BD8, 0x4628BC6B23887196,
+	0x061E21D588CC61CC, 0x7F769211F07B326F,
+	0x2BA568D92EEC18E7, 0x0668F461693DFF8F,
+	0x00CF0F8687D3B009, 0x001670DB65964485,
+	0x000216A0C344EB45, 0x00002AB6E11C2552,
+	0x000002EDF0B98A84, 0x0000002C253C7E81,
+	0x000000023AF3B2E7, 0x0000000018C14ABF,
+	0x0000000000EBCC6A, 0x000000000007876E,
+	0x00000000000034CF, 0x000000000000013D,
+	0x0000000000000006, 0x0000000000000000
+};
+#define SG_MAX_LO_Hawk_512  ((sizeof sig_gauss_lo_Hawk_512) / sizeof(uint64_t))
+
+static const uint16_t sig_gauss_hi_Hawk_1024[] = {
+	0x58B0, 0x36FE,
+	0x1E3A, 0x0EA0,
+	0x0632, 0x024A,
+	0x00BC, 0x0034,
+	0x000C, 0x0002
+};
+#define SG_MAX_HI_Hawk_1024 ((sizeof sig_gauss_hi_Hawk_1024) / sizeof(uint16_t))
+
+ALIGNED_AVX2
+static const uint64_t sig_gauss_lo_Hawk_1024[] = {
+	0x3AAA2EB76504E560, 0x01AE2B17728DF2DE,
+	0x70E1C03E49BB683E, 0x6A00B82C69624C93,
+	0x55CDA662EF2D1C48, 0x2685DB30348656A4,
+	0x31E874B355421BB7, 0x430192770E205503,
+	0x57C0676C029895A7, 0x5353BD4091AA96DB,
+	0x3D4D67696E51F820, 0x09915A53D8667BEE,
+	0x014A1A8A93F20738, 0x0026670030160D5F,
+	0x0003DAF47E8DFB21, 0x0000557CD1C5F797,
+	0x000006634617B3FF, 0x0000006965E15B13,
+	0x00000005DBEFB646, 0x0000000047E9AB38,
+	0x0000000002F93038, 0x00000000001B2445,
+	0x000000000000D5A7, 0x00000000000005AA,
+	0x0000000000000021, 0x0000000000000000
+};
+#define SG_MAX_LO_Hawk_1024 ((sizeof sig_gauss_lo_Hawk_1024) / sizeof(uint64_t))
+
+/*
+ * Generate x with the right Gaussian, for the specified parity bits.
+ * x is formally generated with center t/2 and standard deviation sigma_sign
+ * (with sigma_sign = 1.010, 1.278 or 1.299, depending on degree); this
+ * function generates 2*x.
+ *
+ * Returned value is the squared norm of x.
+ *
+ * Internally, a random 40-byte seed is obtained from the RNG, to initialize
+ * (along with a counter) four internal SHAKE instances. If sc_extra is not
+ * NULL, then it should be a non-flipped SHAKE context which will be used
+ * as basis for the four instances; otherwise, empty contexts will be used.
+ */
+TARGET_AVX2
+static uint32_t
+sig_gauss(unsigned logn,
+	void (*rng)(void *ctx, void *dst, size_t len), void *rng_context,
+	const shake_context *sc_extra, int8_t *x, const uint8_t *t)
+{
+	const uint16_t *tab_hi;
+	const uint64_t *tab_lo;
+	size_t hi_len, lo_len;
+
+	switch (logn) {
+	case 8:
+		tab_hi = sig_gauss_hi_Hawk_256;
+		tab_lo = sig_gauss_lo_Hawk_256;
+		hi_len = SG_MAX_HI_Hawk_256;
+		lo_len = SG_MAX_LO_Hawk_256;
+		break;
+	case 9:
+		tab_hi = sig_gauss_hi_Hawk_512;
+		tab_lo = sig_gauss_lo_Hawk_512;
+		hi_len = SG_MAX_HI_Hawk_512;
+		lo_len = SG_MAX_LO_Hawk_512;
+		break;
+	default: /* 10 */
+		tab_hi = sig_gauss_hi_Hawk_1024;
+		tab_lo = sig_gauss_lo_Hawk_1024;
+		hi_len = SG_MAX_HI_Hawk_1024;
+		lo_len = SG_MAX_LO_Hawk_1024;
+		break;
+	}
+
+	/*
+	 * We produce 2*n samples. Each sample requires two elements
+	 * from the RNG: a low part (64 bits) and a high part (16 bits).
+	 * A SHAKE instance produces 64-bit words in groups of five
+	 * (little-endian encoding is assumed); the first four words are
+	 * the low parts for four samples (lo0, lo1, lo2 and lo3), and the
+	 * fifth word contains the high parts for the four samples
+	 * (hi0 + (hi1 << 16) + (hi2 << 32) + (hi3 << 48)).
+	 *
+	 * We formally use four parallel SHAKE instances, instantiated
+	 * over seeds obtained from the source RNG. For j = 0 to 3,
+	 * instance j produces samples of index 16*i + 4*j + k, with
+	 * k = 0, 1, 2 or 3. A small, RAM-constrained implementation
+	 * will use the four instances sequentially, reusing the space
+	 * for the SHAKE context; a large system with vector instructions
+	 * (e.g. AVX2) might run the four SHAKE instances at the same time.
+	 */
+	size_t n = (size_t)1 << logn;
+	uint8_t seed[41];
+	rng(rng_context, seed, 40);
+#if HAWK_AVX2
+
+	/*
+	 * Prepare tables. We want to pre-expand values into AVX2
+	 * registers, and also to have fixed-length tables so that the
+	 * compiler can unroll loops where appropriate. Fortunately,
+	 * the tables for n=512 and n=1024 have the same length (this is
+	 * slightly suboptimal for n=256, which has smaller tables).
+	 */
+	__m256i ytab_hi[10];
+	__m256i ytab_lo[26];
+	for (size_t i = 0; i < hi_len; i ++) {
+		ytab_hi[i] = _mm256_set1_epi64x(tab_hi[i]);
+	}
+	for (size_t i = hi_len; i < 10; i ++) {
+		ytab_hi[i] = _mm256_setzero_si256();
+	}
+	for (size_t i = 0; i < lo_len; i ++) {
+		ytab_lo[i] = _mm256_set1_epi64x(tab_lo[i]);
+	}
+	for (size_t i = lo_len; i < 26; i ++) {
+		ytab_lo[i] = _mm256_setzero_si256();
+	}
+
+	/*
+	 * Prepare the four parallel SHAKE contexts.
+	 */
+	shake_context sc[4];
+	for (int i = 0; i < 4; i ++) {
+		if (sc_extra != NULL) {
+			sc[i] = *sc_extra;
+		} else {
+			shake_init(&sc[i], 256);
+		}
+		seed[40] = (uint8_t)i;
+		shake_inject(&sc[i], seed, 41);
+	}
+	shake_x4_context scx4;
+	shake_x4_flip(&scx4, sc);
+
+	__m256i y15 = _mm256_set1_epi64x(0x7FFF);
+	__m256i y63 = _mm256_set1_epi64x(0x7FFFFFFFFFFFFFFF);
+
+	/*
+	 * Squared norm: we accumulate 16 parallel sums.
+	 * Each sum can accumulate up to 128 values (for degree n = 1024,
+	 * we sample 2048 values, and 2048/16 = 128) and each value
+	 * can be up to 26^2, for a total of 93312, which does not fit
+	 * in 16 bits. It is extremely improbable that we ever reach that
+	 * value (since extreme values of coefficients of x are very
+	 * improbable) but, for completeness, we move the partial sums
+	 * at mid-course (when moving from x0 to x1) so that such counters
+	 * fit in 16 bits each.
+	 */
+	__m256i ysn1 = _mm256_setzero_si256();
+	__m256i ysn2 = _mm256_setzero_si256();
+
+	/*
+	 * Sample values by chunks of 16.
+	 */
+	for (size_t u = 0; u < (n << 1); u += 16) {
+		/*
+		 * Get 16*10 bytes from the parallel SHAKE instances.
+		 */
+		union {
+			__m256i y[5];
+			uint64_t q[20];
+		} buf;
+		shake_x4_extract_words(&scx4, buf.q, 5);
+		__m256i yhi_src = buf.y[4];
+		uint64_t tb0 = (uint64_t)t[(u >> 3) + 0] << 31;
+		uint64_t tb1 = tb0 >> 4;
+		uint64_t tb2 = (uint64_t)t[(u >> 3) + 1] << 31;
+		uint64_t tb3 = tb2 >> 4;
+		__m256i ypb = _mm256_setr_epi64x(tb0, tb1, tb2, tb3);
+
+		__m256i yxx = _mm256_setzero_si256();
+		for (size_t k = 0; k < 4; k ++) {
+			__m256i ylo = buf.y[k];
+			__m256i yhi = _mm256_and_si256(yhi_src, y15);
+			yhi_src = _mm256_srli_epi64(yhi_src, 16);
+
+			/* Extract sign bit. */
+			__m256i yneg = _mm256_shuffle_epi32(
+				_mm256_srai_epi32(ylo, 31), 0xF5);
+			ylo = _mm256_and_si256(ylo, y63);
+
+			/* Use even or odd column depending on
+			   parity of t. */
+			__m256i yodd = _mm256_shuffle_epi32(
+				_mm256_srai_epi32(ypb, 31), 0xA0);
+			ypb = _mm256_srli_epi64(ypb, 1);
+
+			__m256i yr = _mm256_setzero_si256();
+			for (size_t i = 0; i < 10; i += 2) {
+				__m256i ytlo = _mm256_or_si256(
+					_mm256_andnot_si256(yodd, ytab_lo[i]),
+					_mm256_and_si256(yodd, ytab_lo[i + 1]));
+				__m256i ycc = _mm256_srli_epi64(
+					_mm256_sub_epi64(ylo, ytlo), 63);
+				__m256i ythi = _mm256_or_si256(
+					_mm256_andnot_si256(yodd, ytab_hi[i]),
+					_mm256_and_si256(yodd, ytab_hi[i + 1]));
+				yr = _mm256_add_epi32(yr, _mm256_srli_epi32(
+					_mm256_sub_epi32(_mm256_sub_epi32(
+						yhi, ythi), ycc), 31));
+			}
+			__m256i yhinz = _mm256_cmpeq_epi32(
+				yhi, _mm256_setzero_si256());
+			for (size_t i = 10; i < 26; i += 2) {
+				__m256i ytlo = _mm256_or_si256(
+					_mm256_andnot_si256(yodd, ytab_lo[i]),
+					_mm256_and_si256(yodd, ytab_lo[i + 1]));
+				__m256i ycc = _mm256_srli_epi64(
+					_mm256_sub_epi64(ylo, ytlo), 63);
+				yr = _mm256_add_epi32(yr,
+					_mm256_and_si256(yhinz, ycc));
+			}
+
+			/* Multiply by 2 and apply parity. */
+			yr = _mm256_sub_epi32(_mm256_add_epi32(yr, yr), yodd);
+
+			/* Apply sign bit. */
+			yr = _mm256_sub_epi32(_mm256_xor_si256(yr, yneg), yneg);
+
+			yxx = _mm256_blend_epi16(
+				_mm256_slli_epi64(yxx, 16), yr, 0x11);
+		}
+
+		/* Squared norm support. */
+		if (u == n) {
+			ysn2 = ysn1;
+			ysn1 = _mm256_setzero_si256();
+		}
+		ysn1 = _mm256_add_epi16(ysn1, _mm256_mullo_epi16(yxx, yxx));
+
+		/* In each 64-bit word, we have four output values, over
+		   16 bits each, and in reverse order. We need bytes, and
+		   to put them back in the right order. */
+		yxx = _mm256_shuffle_epi8(yxx, _mm256_setr_epi8(
+			6, 4, 2, 0, 14, 12, 10, 8,
+			-1, -1, -1, -1, -1, -1, -1, -1,
+			6, 4, 2, 0, 14, 12, 10, 8,
+			-1, -1, -1, -1, -1, -1, -1, -1));
+		yxx = _mm256_permute4x64_epi64(yxx, 0xD8);
+		_mm_storeu_si128((__m128i *)(x + u),
+			_mm256_castsi256_si128(yxx));
+	}
+
+	/* We have 32 partial sums to add for the square norm. */
+	__m256i ymlo = _mm256_set1_epi32(0xFFFF);
+	ysn1 = _mm256_add_epi32(
+		_mm256_and_si256(ysn1, ymlo),
+		_mm256_srli_epi32(ysn1, 16));
+	ysn2 = _mm256_add_epi32(
+		_mm256_and_si256(ysn2, ymlo),
+		_mm256_srli_epi32(ysn2, 16));
+	ysn1 = _mm256_add_epi32(ysn1, ysn2);
+	ysn1 = _mm256_add_epi32(ysn1, _mm256_srli_epi64(ysn1, 32));
+	ysn1 = _mm256_add_epi32(ysn1, _mm256_bsrli_epi128(ysn1, 8));
+	return _mm_cvtsi128_si32(_mm_add_epi32(
+		_mm256_castsi256_si128(ysn1),
+		_mm256_extracti128_si256(ysn1, 1)));
+
+#else // HAWK_AVX2
+
+	uint32_t sn = 0;
+	for (size_t j = 0; j < 4; j ++) {
+		shake_context sc;
+		if (sc_extra != NULL) {
+			sc = *sc_extra;
+		} else {
+			shake_init(&sc, 256);
+		}
+		seed[40] = (uint8_t)j;
+		shake_inject(&sc, seed, 41);
+		shake_flip(&sc);
+		for (size_t u = 0; u < (n << 1); u += 16) {
+			union {
+				uint8_t b[40];
+				uint16_t w[20];
+				uint64_t q[5];
+			} buf;
+			shake_extract(&sc, buf.b, 40);
+			for (size_t k = 0; k < 4; k ++) {
+				size_t v = u + (j << 2) + k;
+				uint64_t lo = dec64le(buf.b + (k << 3));
+				uint32_t hi = dec16le(buf.b + 32 + (k << 1));
+
+				/* Extract sign bit. */
+				uint32_t neg = -(uint32_t)(lo >> 63);
+				lo &= 0x7FFFFFFFFFFFFFFF;
+				hi &= 0x7FFF;
+
+				/* Use even or odd column depending on
+				   parity of t. */
+				uint32_t pbit = (t[v >> 3] >> (v & 7)) & 1;
+				uint64_t p_odd = -(uint64_t)pbit;
+				uint32_t p_oddw = (uint32_t)p_odd;
+
+				uint32_t r = 0;
+				for (size_t i = 0; i < hi_len; i += 2) {
+					uint64_t tlo0 = tab_lo[i + 0];
+					uint64_t tlo1 = tab_lo[i + 1];
+					uint64_t tlo = tlo0
+						^ (p_odd & (tlo0 ^ tlo1));
+					uint32_t cc =
+						(uint32_t)((lo - tlo) >> 63);
+					uint32_t thi0 = tab_hi[i + 0];
+					uint32_t thi1 = tab_hi[i + 1];
+					uint32_t thi = thi0
+						^ (p_oddw & (thi0 ^ thi1));
+					r += (hi - thi - cc) >> 31;
+				}
+				uint32_t hinz = (hi - 1) >> 31;
+				for (size_t i = hi_len; i < lo_len; i += 2) {
+					uint64_t tlo0 = tab_lo[i + 0];
+					uint64_t tlo1 = tab_lo[i + 1];
+					uint64_t tlo = tlo0
+						^ (p_odd & (tlo0 ^ tlo1));
+					uint32_t cc =
+						(uint32_t)((lo - tlo) >> 63);
+					r += hinz & cc;
+				}
+
+				/* Multiply by 2 and apply parity. */
+				r = (r << 1) - p_oddw;
+
+				/* Apply sign bit. */
+				r = (r ^ neg) - neg;
+
+				x[v] = (int8_t)*(int32_t *)&r;
+				sn += r * r;
+			}
+		}
+	}
+	return sn;
+
+#endif // HAWK_AVX2
+}
+
+/*
+ * Alternate function for sampling x; the same mechanism is used, but the
+ * provided RNG is used directly instead of instantiating four SHAKE
+ * instances in parallel.
+ *
+ * Returned value is the squared norm of x.
+ */
+TARGET_AVX2
+static uint32_t
+sig_gauss_alt(unsigned logn,
+	void (*rng)(void *ctx, void *dst, size_t len), void *rng_context,
+	int8_t *x, const uint8_t *t)
+{
+	const uint16_t *tab_hi;
+	const uint64_t *tab_lo;
+	size_t hi_len, lo_len;
+
+	switch (logn) {
+	case 8:
+		tab_hi = sig_gauss_hi_Hawk_256;
+		tab_lo = sig_gauss_lo_Hawk_256;
+		hi_len = SG_MAX_HI_Hawk_256;
+		lo_len = SG_MAX_LO_Hawk_256;
+		break;
+	case 9:
+		tab_hi = sig_gauss_hi_Hawk_512;
+		tab_lo = sig_gauss_lo_Hawk_512;
+		hi_len = SG_MAX_HI_Hawk_512;
+		lo_len = SG_MAX_LO_Hawk_512;
+		break;
+	default: /* 10 */
+		tab_hi = sig_gauss_hi_Hawk_1024;
+		tab_lo = sig_gauss_lo_Hawk_1024;
+		hi_len = SG_MAX_HI_Hawk_1024;
+		lo_len = SG_MAX_LO_Hawk_1024;
+		break;
+	}
+
+	/*
+	 * We produce 2*n samples. Each sample requires two elements
+	 * from the RNG: a low part (64 bits) and a high part (16 bits).
+	 * A SHAKE instance produces 64-bit words in groups of five
+	 * (little-endian encoding is assumed); the first four words are
+	 * the low parts for four samples (lo0, lo1, lo2 and lo3), and the
+	 * fifth word contains the high parts for the four samples
+	 * (hi0 + (hi1 << 16) + (hi2 << 32) + (hi3 << 48)).
+	 *
+	 * We formally use four parallel SHAKE instances, instantiated
+	 * over seeds obtained from the source RNG. For j = 0 to 3,
+	 * instance j produces samples of index 16*i + 4*j + k, with
+	 * k = 0, 1, 2 or 3. A small, RAM-constrained implementation
+	 * will use the four instances sequentially, reusing the space
+	 * for the SHAKE context; a large system with vector instructions
+	 * (e.g. AVX2) might run the four SHAKE instances at the same time.
+	 */
+	size_t n = (size_t)1 << logn;
+#if HAWK_AVX2
+
+	/*
+	 * Prepare tables. We want to pre-expand values into AVX2
+	 * registers, and also to have fixed-length tables so that the
+	 * compiler can unroll loops where appropriate. Fortunately,
+	 * the tables for n=512 and n=1024 have the same length (this is
+	 * slightly suboptimal for n=256, which has smaller tables).
+	 */
+	__m256i ytab_hi[10];
+	__m256i ytab_lo[26];
+	for (size_t i = 0; i < hi_len; i ++) {
+		ytab_hi[i] = _mm256_set1_epi64x(tab_hi[i]);
+	}
+	for (size_t i = hi_len; i < 10; i ++) {
+		ytab_hi[i] = _mm256_setzero_si256();
+	}
+	for (size_t i = 0; i < lo_len; i ++) {
+		ytab_lo[i] = _mm256_set1_epi64x(tab_lo[i]);
+	}
+	for (size_t i = lo_len; i < 26; i ++) {
+		ytab_lo[i] = _mm256_setzero_si256();
+	}
+
+	__m256i y15 = _mm256_set1_epi64x(0x7FFF);
+	__m256i y63 = _mm256_set1_epi64x(0x7FFFFFFFFFFFFFFF);
+
+	/*
+	 * Squared norm: we accumulate 16 parallel sums.
+	 * Each sum can accumulate up to 128 values (for degree n = 1024,
+	 * we sample 2048 values, and 2048/16 = 128) and each value
+	 * can be up to 26^2, for a total of 93312, which does not fit
+	 * in 16 bits. It is extremely improbable that we ever reach that
+	 * value (since extreme values of coefficients of x are very
+	 * improbable) but, for completeness, we move the partial sums
+	 * at mid-course (when moving from x0 to x1) so that such counters
+	 * fit in 16 bits each.
+	 */
+	__m256i ysn1 = _mm256_setzero_si256();
+	__m256i ysn2 = _mm256_setzero_si256();
+
+	/*
+	 * Sample values by chunks of 16.
+	 */
+	for (size_t u = 0; u < (n << 1); u += 16) {
+		/*
+		 * Get 16*10 bytes from the random source.
+		 */
+		union {
+			__m256i y[5];
+			uint8_t b[160];
+		} buf;
+		rng(rng_context, buf.b, sizeof buf.b);
+		__m256i yhi_src = buf.y[4];
+		uint64_t tb0 = (uint64_t)t[(u >> 3) + 0] << 31;
+		uint64_t tb1 = tb0 >> 4;
+		uint64_t tb2 = (uint64_t)t[(u >> 3) + 1] << 31;
+		uint64_t tb3 = tb2 >> 4;
+		__m256i ypb = _mm256_setr_epi64x(tb0, tb1, tb2, tb3);
+
+		__m256i yxx = _mm256_setzero_si256();
+		for (size_t k = 0; k < 4; k ++) {
+			__m256i ylo = buf.y[k];
+			__m256i yhi = _mm256_and_si256(yhi_src, y15);
+			yhi_src = _mm256_srli_epi64(yhi_src, 16);
+
+			/* Extract sign bit. */
+			__m256i yneg = _mm256_shuffle_epi32(
+				_mm256_srai_epi32(ylo, 31), 0xF5);
+			ylo = _mm256_and_si256(ylo, y63);
+
+			/* Use even or odd column depending on
+			   parity of t. */
+			__m256i yodd = _mm256_shuffle_epi32(
+				_mm256_srai_epi32(ypb, 31), 0xA0);
+			ypb = _mm256_srli_epi64(ypb, 1);
+
+			__m256i yr = _mm256_setzero_si256();
+			for (size_t i = 0; i < 10; i += 2) {
+				__m256i ytlo = _mm256_or_si256(
+					_mm256_andnot_si256(yodd, ytab_lo[i]),
+					_mm256_and_si256(yodd, ytab_lo[i + 1]));
+				__m256i ycc = _mm256_srli_epi64(
+					_mm256_sub_epi64(ylo, ytlo), 63);
+				__m256i ythi = _mm256_or_si256(
+					_mm256_andnot_si256(yodd, ytab_hi[i]),
+					_mm256_and_si256(yodd, ytab_hi[i + 1]));
+				yr = _mm256_add_epi32(yr, _mm256_srli_epi32(
+					_mm256_sub_epi32(_mm256_sub_epi32(
+						yhi, ythi), ycc), 31));
+			}
+			__m256i yhinz = _mm256_cmpeq_epi32(
+				yhi, _mm256_setzero_si256());
+			for (size_t i = 10; i < 26; i += 2) {
+				__m256i ytlo = _mm256_or_si256(
+					_mm256_andnot_si256(yodd, ytab_lo[i]),
+					_mm256_and_si256(yodd, ytab_lo[i + 1]));
+				__m256i ycc = _mm256_srli_epi64(
+					_mm256_sub_epi64(ylo, ytlo), 63);
+				yr = _mm256_add_epi32(yr,
+					_mm256_and_si256(yhinz, ycc));
+			}
+
+			/* Multiply by 2 and apply parity. */
+			yr = _mm256_sub_epi32(_mm256_add_epi32(yr, yr), yodd);
+
+			/* Apply sign bit. */
+			yr = _mm256_sub_epi32(_mm256_xor_si256(yr, yneg), yneg);
+
+			yxx = _mm256_blend_epi16(
+				_mm256_slli_epi64(yxx, 16), yr, 0x11);
+		}
+
+		/* Squared norm support. */
+		if (u == n) {
+			ysn2 = ysn1;
+			ysn1 = _mm256_setzero_si256();
+		}
+		ysn1 = _mm256_add_epi16(ysn1, _mm256_mullo_epi16(yxx, yxx));
+
+		/* In each 64-bit word, we have four output values, over
+		   16 bits each, and in reverse order. We need bytes, and
+		   to put them back in the right order. */
+		yxx = _mm256_shuffle_epi8(yxx, _mm256_setr_epi8(
+			6, 4, 2, 0, 14, 12, 10, 8,
+			-1, -1, -1, -1, -1, -1, -1, -1,
+			6, 4, 2, 0, 14, 12, 10, 8,
+			-1, -1, -1, -1, -1, -1, -1, -1));
+		yxx = _mm256_permute4x64_epi64(yxx, 0xD8);
+		_mm_storeu_si128((__m128i *)(x + u),
+			_mm256_castsi256_si128(yxx));
+	}
+
+	/* We have 32 partial sums to add for the square norm. */
+	__m256i ymlo = _mm256_set1_epi32(0xFFFF);
+	ysn1 = _mm256_add_epi32(
+		_mm256_and_si256(ysn1, ymlo),
+		_mm256_srli_epi32(ysn1, 16));
+	ysn2 = _mm256_add_epi32(
+		_mm256_and_si256(ysn2, ymlo),
+		_mm256_srli_epi32(ysn2, 16));
+	ysn1 = _mm256_add_epi32(ysn1, ysn2);
+	ysn1 = _mm256_add_epi32(ysn1, _mm256_srli_epi64(ysn1, 32));
+	ysn1 = _mm256_add_epi32(ysn1, _mm256_bsrli_epi128(ysn1, 8));
+	return _mm_cvtsi128_si32(_mm_add_epi32(
+		_mm256_castsi256_si128(ysn1),
+		_mm256_extracti128_si256(ysn1, 1)));
+
+#else // HAWK_AVX2
+
+	uint32_t sn = 0;
+	for (size_t u = 0; u < (n << 1); u += 16) {
+		union {
+			uint8_t b[160];
+			uint16_t w[80];
+			uint64_t q[20];
+		} buf;
+		rng(rng_context, buf.b, sizeof buf.b);
+		for (size_t j = 0; j < 4; j ++) {
+			for (size_t k = 0; k < 4; k ++) {
+				size_t v = u + (j << 2) + k;
+				uint64_t lo = dec64le(
+					buf.b + (j << 3) + (k << 5));
+				uint32_t hi = dec16le(
+					buf.b + (j << 3) + 128 + (k << 1));
+
+				/* Extract sign bit. */
+				uint32_t neg = -(uint32_t)(lo >> 63);
+				lo &= 0x7FFFFFFFFFFFFFFF;
+				hi &= 0x7FFF;
+
+				/* Use even or odd column depending on
+				   parity of t. */
+				uint32_t pbit = (t[v >> 3] >> (v & 7)) & 1;
+				uint64_t p_odd = -(uint64_t)pbit;
+				uint32_t p_oddw = (uint32_t)p_odd;
+
+				uint32_t r = 0;
+				for (size_t i = 0; i < hi_len; i += 2) {
+					uint64_t tlo0 = tab_lo[i + 0];
+					uint64_t tlo1 = tab_lo[i + 1];
+					uint64_t tlo = tlo0
+						^ (p_odd & (tlo0 ^ tlo1));
+					uint32_t cc =
+						(uint32_t)((lo - tlo) >> 63);
+					uint32_t thi0 = tab_hi[i + 0];
+					uint32_t thi1 = tab_hi[i + 1];
+					uint32_t thi = thi0
+						^ (p_oddw & (thi0 ^ thi1));
+					r += (hi - thi - cc) >> 31;
+				}
+				uint32_t hinz = (hi - 1) >> 31;
+				for (size_t i = hi_len; i < lo_len; i += 2) {
+					uint64_t tlo0 = tab_lo[i + 0];
+					uint64_t tlo1 = tab_lo[i + 1];
+					uint64_t tlo = tlo0
+						^ (p_odd & (tlo0 ^ tlo1));
+					uint32_t cc =
+						(uint32_t)((lo - tlo) >> 63);
+					r += hinz & cc;
+				}
+
+				/* Multiply by 2 and apply parity. */
+				r = (r << 1) - p_oddw;
+
+				/* Apply sign bit. */
+				r = (r ^ neg) - neg;
+
+				x[v] = (int8_t)*(int32_t *)&r;
+				sn += r * r;
+			}
+		}
+	}
+	return sn;
+
+#endif // HAWK_AVX2
+}
+
+/*
+ * Returned value:
+ *   1   first non-zero coefficient of s is positive
+ *  -1   first non-zero coefficient of s is negative
+ *   0   s is entirely zero
+ */
+TARGET_AVX2
+static int32_t
+poly_symbreak(unsigned logn, const int16_t *s)
+{
+	size_t n = (size_t)1 << logn;
+#if HAWK_AVX2
+	uint32_t rp = 0, rn = 0;
+	uint32_t c = 0xFFFFFFFF;
+	for (size_t u = 0; u < n; u += 16) {
+		__m256i yy = _mm256_loadu_si256((const __m256i *)(s + u));
+		__m256i yp = _mm256_cmpgt_epi16(yy, _mm256_setzero_si256());
+		__m256i yn = _mm256_cmpgt_epi16(_mm256_setzero_si256(), yy);
+		uint32_t mp = _mm256_movemask_epi8(yp);
+		uint32_t mn = _mm256_movemask_epi8(yn);
+		rp |= c & mp;
+		rn |= c & mn;
+		uint32_t x = mp | mn;
+		c &= ~tbmask(x | -x);
+	}
+	uint32_t t = rp | rn | 0x80000000;
+#if defined _MSC_VER
+	unsigned long k;
+	(void)_BitScanForward(&k, t);
+#else
+	unsigned k = _bit_scan_forward(t);
+#endif
+	return ((rp >> k) & 1) - ((rn >> k) & 1);
+#else // HAWK_AVX2
+	uint32_t r = 0;
+	uint32_t c = 0xFFFFFFFF;
+	for (size_t u = 0; u < n; u ++) {
+		uint32_t x = (uint32_t)s[u];
+		uint32_t nz = c & tbmask(x | -x);
+		c &= ~nz;
+		r |= nz & (tbmask(x) | 1);
+	}
+	return r;
+#endif // HAWK_AVX2
+}
+
+/*
+ * Encode the signature, with output length exactly sig_len bytes.
+ * Padding is applied if necessary. Returned value is 1 on success, 0
+ * on error; an error is reported if the signature does not fit in the
+ * provided buffer.
+ */
+TARGET_AVX2
+static int
+encode_sig(unsigned logn, void *sig, size_t sig_len,
+	const uint8_t *salt, size_t salt_len, const int16_t *s1)
+{
+	size_t n = (size_t)1 << logn;
+	int low = (logn == 10) ? 6 : 5;
+	uint8_t *buf = (uint8_t *)sig;
+	size_t buf_len = sig_len;
+
+	/* We check the minimal size, including at least n bits for the
+	   variable part. */
+	if (buf_len < salt_len + ((uint32_t)(low + 2) << (logn - 3))) {
+		return 0;
+	}
+
+	/* Salt */
+	memcpy(buf, salt, salt_len);
+	buf += salt_len;
+	buf_len -= salt_len;
+
+	/* Sign bits */
+#if HAWK_AVX2
+	__m256i ys1 = _mm256_setr_epi8(
+		1, 3, 5, 7, 9, 11, 13, 15, -1, -1, -1, -1, -1, -1, -1, -1,
+		1, 3, 5, 7, 9, 11, 13, 15, -1, -1, -1, -1, -1, -1, -1, -1);
+	for (size_t u = 0; u < n; u += 32) {
+		__m256i y0 = _mm256_loadu_si256((__m256i *)(s1 + u +  0));
+		__m256i y1 = _mm256_loadu_si256((__m256i *)(s1 + u + 16));
+		y0 = _mm256_shuffle_epi8(y0, ys1);
+		y1 = _mm256_shuffle_epi8(y1, ys1);
+		__m256i yy = _mm256_unpacklo_epi64(y0, y1);
+		yy = _mm256_permute4x64_epi64(yy, 0xD8);
+		*(uint32_t *)(buf + (u >> 3)) = _mm256_movemask_epi8(yy);
+	}
+#else // HAWK_AVX2
+	for (size_t u = 0; u < n; u += 8) {
+		unsigned x = 0;
+		for (size_t v = 0; v < 8; v ++) {
+			x |= (*(const uint16_t *)(s1 + u + v) >> 15) << v;
+		}
+		buf[u >> 3] = x;
+	}
+#endif // HAWK_AVX2
+	buf += (n >> 3);
+	buf_len -= (n >> 3);
+
+	/* Fixed-size parts */
+#if HAWK_AVX2
+	if (low == 5) {
+		/*
+		 * 16 values yield 80 bits = 10 bytes.
+		 * The variable part has size at least 256 bits (32 bytes)
+		 * beyond the fixed-size part, so we can write full 16-byte
+		 * chunks.
+		 */
+		__m256i yp0 = _mm256_setr_epi8(
+			0, 2, 4, 6, 8, 10, 12, 14,
+			-1, -1, -1, -1, -1, -1, -1, -1,
+			0, 2, 4, 6, 8, 10, 12, 14,
+			-1, -1, -1, -1, -1, -1, -1, -1);
+		__m256i ym1 = _mm256_set1_epi16(0x001F);
+		__m256i ym2 = _mm256_set1_epi16(0x1F00);
+		__m256i ym3 = _mm256_set1_epi32(0x000003FF);
+		__m256i ym4 = _mm256_set1_epi32(0x03FF0000);
+		__m256i ym5 = _mm256_set1_epi64x(0x00000000000FFFFF);
+		__m256i ym6 = _mm256_set1_epi64x(0x000FFFFF00000000);
+		__m128i xp1 = _mm_setr_epi8(
+			0, 1, 2, 3, 4, 8, 9, 10, 11, 12,
+			-1, -1, -1, -1, -1, -1);
+
+		for (size_t u = 0; u < n; u += 16) {
+			__m256i yy = _mm256_loadu_si256((__m256i *)(s1 + u));
+			yy = _mm256_xor_si256(yy, _mm256_srai_epi16(yy, 15));
+			yy = _mm256_shuffle_epi8(yy, yp0);
+			yy = _mm256_or_si256(
+				_mm256_and_si256(yy, ym1),
+				_mm256_srli_epi16(
+					_mm256_and_si256(yy, ym2), 3));
+			yy = _mm256_or_si256(
+				_mm256_and_si256(yy, ym3),
+				_mm256_srli_epi32(
+					_mm256_and_si256(yy, ym4), 6));
+			yy = _mm256_or_si256(
+				_mm256_and_si256(yy, ym5),
+				_mm256_srli_epi64(
+					_mm256_and_si256(yy, ym6), 12));
+			yy = _mm256_permute4x64_epi64(yy, 0xD8);
+			__m128i xx = _mm_shuffle_epi8(
+				_mm256_castsi256_si128(yy), xp1);
+			_mm_storeu_si128((__m128i *)buf, xx);
+			buf += 10;
+		}
+	} else {
+		/*
+		 * low = 6
+		 * 16 values yield 96 bits = 12 bytes.
+		 * The variable part has size at least 256 bits (32 bytes)
+		 * beyond the fixed-size part, so we can write full 16-byte
+		 * chunks.
+		 */
+		__m256i yp0 = _mm256_setr_epi8(
+			0, 2, 4, 6, 8, 10, 12, 14,
+			-1, -1, -1, -1, -1, -1, -1, -1,
+			0, 2, 4, 6, 8, 10, 12, 14,
+			-1, -1, -1, -1, -1, -1, -1, -1);
+		__m256i ym1 = _mm256_set1_epi16(0x003F);
+		__m256i ym2 = _mm256_set1_epi16(0x3F00);
+		__m256i ym3 = _mm256_set1_epi32(0x00000FFF);
+		__m256i ym4 = _mm256_set1_epi32(0x0FFF0000);
+		__m128i xp1 = _mm_setr_epi8(
+			0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, -1, -1, -1, -1);
+		for (size_t u = 0; u < n; u += 16) {
+			__m256i yy = _mm256_loadu_si256((__m256i *)(s1 + u));
+			yy = _mm256_xor_si256(yy, _mm256_srai_epi16(yy, 15));
+			yy = _mm256_shuffle_epi8(yy, yp0);
+			yy = _mm256_or_si256(
+				_mm256_and_si256(yy, ym1),
+				_mm256_srli_epi16(
+					_mm256_and_si256(yy, ym2), 2));
+			yy = _mm256_or_si256(
+				_mm256_and_si256(yy, ym3),
+				_mm256_srli_epi32(
+					_mm256_and_si256(yy, ym4), 4));
+			yy = _mm256_permute4x64_epi64(yy, 0xD8);
+			__m128i xx = _mm_shuffle_epi8(
+				_mm256_castsi256_si128(yy), xp1);
+			_mm_storeu_si128((__m128i *)buf, xx);
+			buf += 12;
+		}
+	}
+#else // HAWK_AVX2
+	uint32_t low_mask = ((uint32_t)1 << low) - 1;
+	for (size_t u = 0; u < n; u += 8) {
+		uint64_t x = 0;
+		for (size_t v = 0, vv = 0; v < 8; v ++, vv += low) {
+			uint32_t w = (uint32_t)s1[u + v];
+			w ^= tbmask(w);
+			x |= (uint64_t)(w & low_mask) << vv;
+		}
+		for (int i = 0; i < (low << 3); i += 8) {
+			*buf ++ = (uint8_t)(x >> i);
+		}
+	}
+#endif // HAWK_AVX2
+	buf_len -= (size_t)low << (logn - 3);
+
+	/* Variable-size parts */
+#if HAWK_AVX2
+#if defined __x86_64__ || defined _M_X86
+	/*
+	 * In 64-bit mode, we can first handle most values by groups of 3
+	 * (with at most 7 accumulated bits, 3 values add up to a maximum
+	 * of 7 + 3*16 = 55 bits), and write 64-bit words. This avoids
+	 * conditional jumps (the cost of writing the same bytes several
+	 * times is absorbed by the write buffer; mispredicted conditional
+	 * jumps are expensive). We have to stop before the last 64 values
+	 * to encode, since these might need fewer than 8 bytes to write.
+	 */
+	uint64_t accq = 0;
+	unsigned accq_off = 0;
+	size_t j;
+	for (j = 0; j <= (n - 64); j += 3) {
+		uint32_t w0 = (uint32_t)s1[j + 0];
+		uint32_t w1 = (uint32_t)s1[j + 1];
+		uint32_t w2 = (uint32_t)s1[j + 2];
+		unsigned k0 = (w0 ^ tbmask(w0)) >> low;
+		unsigned k1 = (w1 ^ tbmask(w1)) >> low;
+		unsigned k2 = (w2 ^ tbmask(w2)) >> low;
+		accq_off += k0;
+		accq |= (uint64_t)1 << accq_off;
+		accq_off += 1 + k1;
+		accq |= (uint64_t)1 << accq_off;
+		accq_off += 1 + k2;
+		accq |= (uint64_t)1 << accq_off;
+		accq_off ++;
+		if (buf_len < 8) {
+			return 0;
+		}
+		*(uint64_t *)buf = accq;
+		unsigned tt = accq_off & ~0x7u;
+		buf += tt >> 3;
+		buf_len -= tt >> 3;
+		accq >>= tt;
+		accq_off -= tt;
+	}
+
+	uint32_t acc = (uint32_t)accq;
+	int acc_len = (int)accq_off;
+#else
+	uint32_t acc = 0;
+	int acc_len = 0;
+	size_t j = 0;
+#endif
+#else // HAWK_AVX2
+	uint32_t acc = 0;
+	int acc_len = 0;
+	size_t j = 0;
+#endif // HAWK_AVX2
+	for (size_t u = j; u < n; u ++) {
+		uint32_t w = (uint32_t)s1[u];
+		int k = (int)((w ^ tbmask(w)) >> low);
+		acc |= (uint32_t)1 << (acc_len + k);
+		acc_len += 1 + k;
+		while (acc_len >= 8) {
+			if (buf_len == 0) {
+				return 0;
+			}
+			*buf ++ = (uint8_t)acc;
+			buf_len --;
+			acc >>= 8;
+			acc_len -= 8;
+		}
+	}
+	if (acc_len > 0) {
+		if (buf_len == 0) {
+			return 0;
+		}
+		*buf ++ = (uint8_t)acc;
+		buf_len --;
+	}
+
+	/* Padding. */
+	memset(buf, 0, buf_len);
+	return 1;
+}
+
+/* see hawk.h */
+void
+hawk_sign_start(shake_context *sc_data)
+{
+	shake_init(sc_data, 256);
+}
+
+TARGET_AVX2
+static int
+sign_finish_inner(unsigned logn, int use_shake,
+	void (*rng)(void *ctx, void *dst, size_t len), void *rng_context,
+	void *sig, const shake_context *sc_data,
+	const void *priv, size_t priv_len, void *tmp, size_t tmp_len)
+{
+	/*
+	 * Ensure that the tmp[] buffer has proper alignment for 64-bit
+	 * access.
+	 */
+	if (tmp_len < 7) {
+		return 0;
+	}
+	if (logn < 8 || logn > 10) {
+		return 0;
+	}
+	uintptr_t utmp1 = (uintptr_t)tmp;
+	uintptr_t utmp2 = (utmp1 + 7) & ~(uintptr_t)7;
+	tmp_len -= (size_t)(utmp2 - utmp1);
+	uint32_t *tt32 = (uint32_t *)utmp2;
+	if (tmp_len < ((size_t)6 << logn)) {
+		return 0;
+	}
+
+	/*
+	 * Check whether the private key is decoded or encoded.
+	 */
+	int priv_decoded;
+	if (priv_len == HAWK_PRIVKEY_SIZE(logn)) {
+		priv_decoded = 0;
+	} else if (priv_len == HAWK_PRIVKEY_DECODED_SIZE(logn)) {
+		priv_decoded = 1;
+	} else {
+		return 0;
+	}
+
+	/*
+	 * Hawk parameters from: https://eprint.iacr.org/2022/1155
+	 *
+	 *   logn                   8         9       10
+	 *   salt_len (bits)      112       192      320
+	 *   sigma_sign         1.010     1.278    1.299
+	 *   sigma_ver          1.042     1.425    1.571
+	 *   sigma_sec          1.042     1.425    1.974
+	 *   sigma_pk             1.1       1.5      2.0
+	 *
+	 * Note: sigma_ver is changed to 1.042 for logn=8 and 1.571 for
+	 * logn=10 (compared to the eprint).
+	 *
+	 * Maximum squared norm of x = 2*n*(sigma_ver^2).
+	 * max_xnorm is used for the squared norm of 2*x (as returned
+	 * by sig_gauss()).
+	 */
+	size_t n = (size_t)1 << logn;
+	size_t salt_len;
+	uint32_t max_xnorm;
+	switch (logn) {
+	case 8:
+		salt_len = 14;
+		max_xnorm = 2223;
+		break;
+	case 9:
+		salt_len = 24;
+		max_xnorm = 8317;
+		break;
+	case 10:
+		salt_len = 40;
+		max_xnorm = 20218;
+		break;
+	default:
+		return 0;
+	}
+	size_t seed_len = 8 + ((size_t)1 << (logn - 5));
+	size_t hpub_len = (size_t)1 << (logn - 4);
+
+	/*
+	 * Memory layout:
+	 *    g    n bytes
+	 *    ww   2*n bytes
+	 *    x0   n bytes
+	 *    x1   n bytes
+	 *    f    n bytes
+	 *
+	 * ww is the area where we will perform mod 2 computations.
+	 */
+	int8_t *g = (int8_t *)tt32;
+	uint8_t *ww = (uint8_t *)(g + n);
+	int8_t *x0 = (int8_t *)(ww + 2 * n);
+	int8_t *x1 = x0 + n;
+	int8_t *f = x1 + n;
+
+	/*
+	 * Re-expand the private key.
+	 */
+	const uint8_t *F2;
+	const uint8_t *G2;
+	const void *hpub;
+	if (priv_decoded) {
+		f = (int8_t *)priv;
+		g = f + n;
+		F2 = (const uint8_t *)(g + n);
+		G2 = F2 + (n >> 3);
+		hpub = G2 + (n >> 3);
+	} else {
+		const uint8_t *secbuf = (const uint8_t *)priv;
+		Hawk_regen_fg(logn, f, g, secbuf);
+		F2 = secbuf + seed_len;
+		G2 = F2 + (n >> 3);
+		hpub = G2 + (n >> 3);
+	}
+
+#if HAWK_DEBUG
+	printf("#### Sign (n=%u):\n", 1u << logn);
+	print_i8(logn, "f", f);
+	print_i8(logn, "g", g);
+	print_u1(logn, "F2", F2);
+	print_u1(logn, "G2", G2);
+	print_blob("hpub", hpub, hpub_len);
+#endif
+
+	/*
+	 * SHAKE256 computations will use this context structure. We do
+	 * not modify the input context value.
+	 */
+	shake_context scd;
+
+	/*
+	 * hm <- SHAKE256(message || hpub)
+	 */
+	uint8_t hm[64];
+	scd = *sc_data;
+	shake_inject(&scd, hpub, hpub_len);
+	shake_flip(&scd);
+	shake_extract(&scd, hm, sizeof hm);
+
+#if HAWK_DEBUG
+	printf("# hm = SHAKE256(message || hpub) (64 bytes)\n");
+	print_blob("hm", hm, sizeof hm);
+#endif
+
+	for (uint32_t attempt = 0;; attempt += 2) {
+		/*
+		 * We get temporary values (of n/8 bytes each) inside ww.
+		 *    t0    n/8 bytes
+		 *    t1    n/8 bytes
+		 *    h0    n/8 bytes
+		 *    h1    n/8 bytes
+		 *    f2    n/8 bytes
+		 *    g2    n/8 bytes
+		 *    xx    10*n/8 bytes
+		 */
+		uint8_t *t0 = ww;
+		uint8_t *t1 = t0 + (n >> 3);
+		uint8_t *h0 = t1 + (n >> 3);
+		uint8_t *h1 = h0 + (n >> 3);
+		uint8_t *f2 = h1 + (n >> 3);
+		uint8_t *g2 = f2 + (n >> 3);
+		uint8_t *xx = g2 + (n >> 3);
+
+		/*
+		 * Generate the salt.
+		 *
+		 * In normal mode (with SHAKE-based sampling), we use
+		 * SHAKE256(hm || kgseed || counter || rand), i.e. the
+		 * concatenation of the message, the private key seed, an
+		 * attempt counter, and some extra randomness (from the
+		 * provided RNG); this ensures security even if the RNG
+		 * does not provide good randomness. If (f,g) were provided
+		 * already decoded, we use f and g directly instead of
+		 * kgseed.
+		 *
+		 * In alternate mode, as per the API, we use the RNG directly.
+		 */
+		uint8_t salt[40];
+		rng(rng_context, salt, salt_len);
+		if (use_shake) {
+			uint8_t tbuf[4];
+			enc32le(tbuf, attempt);
+			shake_init(&scd, 256);
+			shake_inject(&scd, hm, sizeof hm);
+			if (priv_decoded) {
+				shake_inject(&scd, priv, n << 1);
+			} else {
+				shake_inject(&scd, priv, seed_len);
+			}
+			shake_inject(&scd, tbuf, sizeof tbuf);
+			shake_inject(&scd, salt, salt_len);
+			shake_flip(&scd);
+			shake_extract(&scd, salt, salt_len);
+		}
+
+#if HAWK_DEBUG
+		print_blob("salt", salt, salt_len);
+#endif
+
+		/*
+		 * h <- SHAKE256(hm || salt)  (out: 2*n bits)
+		 */
+		shake_init(&scd, 256);
+		shake_inject(&scd, hm, sizeof hm);
+		shake_inject(&scd, salt, salt_len);
+		shake_flip(&scd);
+		shake_extract(&scd, h0, n >> 2);
+
+#if HAWK_DEBUG
+		printf("# h = SHAKE256(hm || salt) (2*n bits)\n");
+		print_u1(logn, "h0", h0);
+		print_u1(logn, "h1", h1);
+#endif
+
+		/*
+		 * t <- B*h  (mod 2)
+		 */
+		extract_lowbit(logn, f2, f);
+		extract_lowbit(logn, g2, g);
+		basis_m2_mul(logn, t0, t1, h0, h1, f2, g2, F2, G2, xx);
+
+#if HAWK_DEBUG
+		printf("# t = B*h (mod 2)\n");
+		print_u1(logn, "t0", t0);
+		print_u1(logn, "t1", t1);
+#endif
+
+		/*
+		 * x <- D_{t/2}(sigma_sign)
+		 * Reject if the squared norm of x is larger than
+		 * 2*n*(sigma_ver^2) (this is quite improbable).
+		 *
+		 * When using SHAKE for the sampling, we also inject
+		 * the message, the private key (seed) and the attempt
+		 * counter into the SHAKE instances, so that security is
+		 * maintained even if the RNG source is poor.
+		 */
+		uint32_t xsn;
+		if (use_shake) {
+			uint8_t tbuf[4];
+			enc32le(tbuf, attempt + 1);
+			shake_init(&scd, 256);
+			shake_inject(&scd, hm, sizeof hm);
+			if (priv_decoded) {
+				shake_inject(&scd, priv, n << 1);
+			} else {
+				shake_inject(&scd, priv, seed_len);
+			}
+			shake_inject(&scd, tbuf, sizeof tbuf);
+			xsn = sig_gauss(logn, rng, rng_context, &scd, x0, t0);
+		} else {
+			xsn = sig_gauss_alt(logn, rng, rng_context, x0, t0);
+		}
+#if HAWK_DEBUG
+		printf("# (dx0, dx1) = (2*x0, 2*x1)\n");
+		print_i8(logn, "dx0", x0);
+		print_i8(logn, "dx1", x1);
+		printf("l2norm(2*x)^2 = %lu\n", (unsigned long)xsn);
+#endif
+		if (xsn > max_xnorm) {
+			continue;
+		}
+
+		/*
+		 * (x0*adj(f) + x1*adj(g))/(f*adj(f) + g*adj(g)) should
+		 * have all its coefficients in [-1/2, +1/2]. For
+		 * degrees n=512 and n=1024, the key has been generated
+		 * so that it is extremely improbable (less than
+		 * 2^(-105) and 2^(-315), respectively) that this
+		 * property is not met, and we can omit the test (the
+		 * risk of miscomputation through a stray cosmic ray is
+		 * many orders of magnitude higher).
+		 *
+		 * For degree n=256, the property may be false with
+		 * probability about 2^(-39.5), though the actual
+		 * verification failure rate may be lower. Since n=256
+		 * is a "challenge" variant, we can tolerate that
+		 * failure rate.
+		 */
+
+		/*
+		 * s1 = (1/2)*h1 + g*x0 - f*x1
+		 * We compute 2*(f*x1 - g*x0) = h1 - 2*s1
+		 * (we already have 2*x in (x0,x1)).
+		 *
+		 * We no longer need the mod 2 values, except h1, which we
+		 * move to a stack buffer (it has size at most 128 bytes,
+		 * we can afford to put it on the stack).
+		 *
+		 * Memory layout:
+		 *    g    n bytes   w1
+		 *    --   n bytes   w1
+		 *    --   n bytes   w2
+		 *    x0   n bytes   w2
+		 *    x1   n bytes   w3
+		 *    f    n bytes   w3
+		 */
+		uint8_t h1buf[128];
+		memcpy(h1buf, h1, n >> 3);
+
+		/* w1 <- 2*g*x0 (NTT) */
+		uint16_t *w1 = (uint16_t *)tt32;
+		uint16_t *w2 = w1 + n;
+		uint16_t *w3 = w2 + n;
+		if (priv_decoded) {
+			mq18433_poly_set_small(logn, w1, g);
+		} else {
+			mq18433_poly_set_small_inplace_low(logn, w1);
+		}
+		mq18433_poly_set_small_inplace_high(logn, w2);
+		mq18433_NTT(logn, w1);
+		mq18433_NTT(logn, w2);
+#if HAWK_AVX2
+		for (size_t u = 0; u < n; u += 16) {
+			__m256i y1 = _mm256_loadu_si256((__m256i *)&w1[u]);
+			__m256i y2 = _mm256_loadu_si256((__m256i *)&w2[u]);
+			y1 = mq18433_montymul_x16(y1, y2);
+			_mm256_storeu_si256((__m256i *)&w1[u], y1);
+		}
+#else // HAWK_AVX2
+		for (size_t u = 0; u < n; u ++) {
+			w1[u] = mq18433_montymul(w1[u], w2[u]);
+		}
+#endif // HAWK_AVX2
+
+		/* w3 <- 2*(f*x1 - g*x0) = h1 - 2*s1 */
+		mq18433_poly_set_small(logn, w2, x1);
+		if (priv_decoded) {
+			mq18433_poly_set_small(logn, w3, f);
+		} else {
+			mq18433_poly_set_small_inplace_high(logn, w3);
+		}
+		mq18433_NTT(logn, w2);
+		mq18433_NTT(logn, w3);
+#if HAWK_AVX2
+		for (size_t u = 0; u < n; u += 16) {
+			__m256i y1 = _mm256_loadu_si256((__m256i *)&w1[u]);
+			__m256i y2 = _mm256_loadu_si256((__m256i *)&w2[u]);
+			__m256i y3 = _mm256_loadu_si256((__m256i *)&w3[u]);
+			y3 = mq18433_tomonty_x16(mq18433_sub_x16(
+				mq18433_montymul_x16(y2, y3), y1));
+			_mm256_storeu_si256((__m256i *)&w3[u], y3);
+		}
+#else // HAWK_AVX2
+		for (size_t u = 0; u < n; u ++) {
+			w3[u] = mq18433_tomonty(mq18433_sub(
+				mq18433_montymul(w2[u], w3[u]), w1[u]));
+		}
+#endif // HAWK_AVX2
+		mq18433_iNTT(logn, w3);
+		mq18433_poly_snorm(logn, w3);
+
+#if HAWK_DEBUG
+		printf("# w = h1 - 2*s1\n");
+		print_i16(logn, "w", (int16_t *)w3);
+#endif
+
+		/*
+		 * sym-break(): if h1 - 2*s1 (currently in w3) is
+		 * positive, then we return s1 = (h1 - w3)/2. Otherwise,
+		 * we return s1 = h1 - (h1 - w3)/2 = (h1 + w3)/2.
+		 * Thus, this uses a conditional negation of w3. We
+		 * set nm = -1 if the negation must happen, 0 otherwise.
+		 *
+		 * We also enforce a limit on the individual elements of s1.
+		 */
+		int16_t *s1 = (int16_t *)w3;
+		uint32_t ps = poly_symbreak(logn, s1);
+#if HAWK_DEBUG
+		printf("symbreak = %d\n", (int)ps);
+#endif
+		int lim = 1 << ((logn == 10) ? 10 : 9);
+#if HAWK_AVX2
+		__m256i ynm = _mm256_set1_epi16(-*(int32_t *)&ps);
+		__m256i yplim = _mm256_set1_epi16(lim);
+		__m256i ynlim = _mm256_set1_epi16(-lim - 1);
+		__m256i ysh0 = _mm256_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14);
+		__m256i ysh1 = _mm256_setr_epi32(1, 3, 5, 7, 9, 11, 13, 15);
+		__m256i y1 = _mm256_set1_epi16(1);
+		__m256i ygg = _mm256_set1_epi16(-1);
+		for (size_t u = 0; u < n; u += 16) {
+			__m256i yz = _mm256_loadu_si256(
+				(const __m256i *)(s1 + u));
+			yz = _mm256_sign_epi16(yz, ynm);
+			__m256i yh = _mm256_set1_epi32(
+				h1buf[u >> 3] | (h1buf[(u >> 3) + 1] << 8));
+			__m256i yh0 = _mm256_srlv_epi32(yh, ysh0);
+			__m256i yh1 = _mm256_srlv_epi32(yh, ysh1);
+			yh = _mm256_blend_epi16(
+				yh0, _mm256_slli_epi32(yh1, 16), 0xAA);
+			yh = _mm256_and_si256(yh, y1);
+			yz = _mm256_srai_epi16(_mm256_add_epi16(yz, yh), 1);
+			ygg = _mm256_and_si256(ygg,
+				_mm256_and_si256(
+					_mm256_cmpgt_epi16(yplim, yz),
+					_mm256_cmpgt_epi16(yz, ynlim)));
+			_mm256_storeu_si256((__m256i *)(s1 + u), yz);
+		}
+		if (_mm256_movemask_epi8(ygg) != -1) {
+			if (!priv_decoded) {
+				Hawk_regen_fg(logn, f, g, priv);
+			}
+			continue;
+		}
+#else // HAWK_AVX2
+		uint32_t nm = ~tbmask(ps - 1);
+		for (size_t u = 0; u < n; u ++) {
+			uint32_t z = (uint32_t)s1[u];
+			z = ((z ^ nm) - nm) + ((h1buf[u >> 3] >> (u & 7)) & 1);
+			int32_t y = *(int32_t *)&z >> 1;
+			if (y < -lim || y >= lim) {
+				lim = 0;
+				break;
+			}
+			s1[u] = y;
+		}
+		if (lim == 0) {
+#if HAWK_DEBUG
+			printf("# limit on s1 exceeded, restarting\n");
+#endif
+			if (!priv_decoded) {
+				Hawk_regen_fg(logn, f, g, priv);
+			}
+			continue;
+		}
+#endif // HAWK_AVX2
+
+#if HAWK_DEBUG
+		print_i16(logn, "s1", (int16_t *)s1);
+#endif
+
+		/*
+		 * We have the signature (in s1). We encode it into
+		 * the temporary buffer, and (optionally) into the provided
+		 * output buffer.
+		 */
+		size_t sig_len = HAWK_SIG_SIZE(logn);
+		if (encode_sig(logn, tmp, sig_len, salt, salt_len, s1)) {
+#if HAWK_DEBUG
+			print_blob("sig", tmp, sig_len);
+#endif
+			if (sig != NULL) {
+				memcpy(sig, tmp, sig_len);
+			}
+			return 1;
+		}
+#if HAWK_DEBUG
+		printf("# signature too large, restarting");
+#endif
+		if (!priv_decoded) {
+			Hawk_regen_fg(logn, f, g, priv);
+		}
+	}
+}
+
+/* see hawk.h */
+int
+hawk_sign_finish(unsigned logn,
+	void (*rng)(void *ctx, void *dst, size_t len), void *rng_context,
+	void *sig, const shake_context *sc_data,
+	const void *priv, void *tmp, size_t tmp_len)
+{
+	return sign_finish_inner(logn, 1, rng, rng_context, sig, sc_data,
+		priv, HAWK_PRIVKEY_SIZE(logn), tmp, tmp_len);
+}
+
+/* see hawk.h */
+int
+hawk_sign_finish_alt(unsigned logn,
+	void (*rng)(void *ctx, void *dst, size_t len), void *rng_context,
+	void *sig, const shake_context *sc_data,
+	const void *priv, size_t priv_len, void *tmp, size_t tmp_len)
+{
+	return sign_finish_inner(logn, 0, rng, rng_context, sig, sc_data,
+		priv, priv_len, tmp, tmp_len);
+}
+
+/* see hawk.h */
+void
+hawk_decode_private_key(unsigned logn, void *priv_dec, const void *priv)
+{
+	size_t n = (size_t)1 << logn;
+	int8_t *f = (int8_t *)priv_dec;
+	int8_t *g = f + n;
+	size_t seed_len = 8 + ((size_t)1 << (logn - 5));
+	size_t hpub_len = (size_t)1 << (logn - 4);
+	uint8_t seed[40];
+	memcpy(seed, priv, seed_len);
+	memmove(g + n, (const uint8_t *)priv + seed_len, (n >> 2) + hpub_len);
+	Hawk_regen_fg(logn, f, g, seed);
+}
diff --git a/lib/dns/hawk/hawk_vrfy.c b/lib/dns/hawk/hawk_vrfy.c
new file mode 100644
index 0000000000..2014720a0a
--- /dev/null
+++ b/lib/dns/hawk/hawk_vrfy.c
@@ -0,0 +1,3838 @@
+#include "hawk_inner.h"
+
+/* ==================================================================== */
+/*
+ * Modular integers.
+ * Modulus is p such that:
+ *   p-1 is a multiple of 2048
+ *   3*2^29 < p < 2^31
+ * Montgomery representation of x is x*R mod p with R = 2^32.
+ * Values are kept in the [0..p-1] range.
+ *
+ * The key pair generation implementation uses very similar functions;
+ * here, we will use them for exactly two moduli: 2147473409 and 2147389441.
+ */
+
+static inline uint32_t
+mp_add(uint32_t a, uint32_t b, uint32_t p)
+{
+	uint32_t d = a + b - p;
+	return d + (p & tbmask(d));
+}
+
+#if HAWK_AVX2
+TARGET_AVX2
+static inline __m256i
+mp_add_x8(__m256i ya, __m256i yb, __m256i yp)
+{
+	__m256i yd = _mm256_sub_epi32(_mm256_add_epi32(ya, yb), yp);
+	return _mm256_add_epi32(yd, _mm256_and_si256(yp,
+		_mm256_srai_epi32(yd, 31)));
+}
+#endif // HAWK_AVX2
+
+static inline uint32_t
+mp_sub(uint32_t a, uint32_t b, uint32_t p)
+{
+	uint32_t d = a - b;
+	return d + (p & tbmask(d));
+}
+
+#if HAWK_AVX2
+TARGET_AVX2
+static inline __m256i
+mp_sub_x8(__m256i ya, __m256i yb, __m256i yp)
+{
+	return _mm256_add_epi32(
+		_mm256_sub_epi32(ya, yb),
+		_mm256_and_si256(yp, _mm256_cmpgt_epi32(yb, ya)));
+}
+#endif // HAWK_AVX2
+
+static inline uint32_t
+mp_montymul(uint32_t a, uint32_t b, uint32_t p, uint32_t p0i)
+{
+	uint64_t z = (uint64_t)a * (uint64_t)b;
+	uint32_t w = (uint32_t)z * p0i;
+	uint32_t d = (uint32_t)((z + (uint64_t)w * (uint64_t)p) >> 32) - p;
+	return d + (p & tbmask(d));
+}
+
+#if HAWK_AVX2
+/*
+ * Input:   a0:--:a1:--:a2:--:a3:--
+ *          b0:--:b1:--:b2:--:b3:--
+ * Output:  d0:00:d1:00:d2:00:d3:00
+ */
+TARGET_AVX2
+static inline __m256i
+mp_montymul_x4(__m256i ya, __m256i yb, __m256i yp, __m256i yp0i)
+{
+	__m256i yd = _mm256_mul_epu32(ya, yb);
+	__m256i ye = _mm256_mul_epu32(yd, yp0i);
+	ye = _mm256_mul_epu32(ye, yp);
+	yd = _mm256_srli_epi64(_mm256_add_epi64(yd, ye), 32);
+	yd = _mm256_sub_epi32(yd, yp);
+	return _mm256_add_epi32(yd, _mm256_and_si256(yp,
+		_mm256_srai_epi32(yd, 31)));
+}
+
+TARGET_AVX2
+static inline __m256i
+mp_montymul_x8(__m256i ya, __m256i yb, __m256i yp, __m256i yp0i)
+{
+	/* yd0 <- a0*b0 : a2*b2 (+high lane) */
+	__m256i yd0 = _mm256_mul_epu32(ya, yb);
+	/* yd1 <- a1*b1 : a3*b3 (+high lane) */
+	__m256i yd1 = _mm256_mul_epu32(
+		_mm256_srli_epi64(ya, 32),
+		_mm256_srli_epi64(yb, 32));
+
+	__m256i ye0 = _mm256_mul_epu32(yd0, yp0i);
+	__m256i ye1 = _mm256_mul_epu32(yd1, yp0i);
+	ye0 = _mm256_mul_epu32(ye0, yp);
+	ye1 = _mm256_mul_epu32(ye1, yp);
+	yd0 = _mm256_add_epi64(yd0, ye0);
+	yd1 = _mm256_add_epi64(yd1, ye1);
+
+	/* yf0 <- lo(d0) : lo(d1) : hi(d0) : hi(d1) (+high lane) */
+	__m256i yf0 = _mm256_unpacklo_epi32(yd0, yd1);
+	/* yf1 <- lo(d2) : lo(d3) : hi(d2) : hi(d3) (+high lane) */
+	__m256i yf1 = _mm256_unpackhi_epi32(yd0, yd1);
+	/* yg <- hi(d0) : hi(d1) : hi(d2) : hi(d3) (+high lane) */
+	__m256i yg = _mm256_unpackhi_epi64(yf0, yf1);
+	/*
+	 * Alternate version (instead of the three unpack above) but it
+	 * seems to be slightly slower.
+	__m256i yg = _mm256_blend_epi32(_mm256_srli_epi64(yd0, 32), yd1, 0xAA);
+	 */
+
+	yg = _mm256_sub_epi32(yg, yp);
+	return _mm256_add_epi32(yg, _mm256_and_si256(yp,
+		_mm256_srai_epi32(yg, 31)));
+}
+#endif // HAWK_AVX2
+
+#if HAWK_AVX2
+/*
+ * Return (u*f + v*g)/R.
+ * |f| <= 2^30
+ * |g| <= 2^30
+ */
+TARGET_AVX2
+static inline __m256i
+mp_lin_x8(__m256i yu, __m256i yv, __m256i yf, __m256i yg,
+	__m256i yp, __m256i yp0i)
+{
+	__m256i ysf = _mm256_srai_epi32(yf, 31);
+	yf = _mm256_sub_epi32(_mm256_xor_si256(yf, ysf), ysf);
+	__m256i ysg = _mm256_srai_epi32(yg, 31);
+	yg = _mm256_sub_epi32(_mm256_xor_si256(yg, ysg), ysg);
+
+	yu = _mm256_add_epi32(yu, _mm256_and_si256(ysf,
+		_mm256_sub_epi32(yp, _mm256_add_epi32(yu, yu))));
+	yv = _mm256_add_epi32(yv, _mm256_and_si256(ysg,
+		_mm256_sub_epi32(yp, _mm256_add_epi32(yv, yv))));
+
+	return mp_sub_x8(
+		mp_montymul_x8(yu, yf, yp, yp0i),
+		mp_montymul_x8(yv, yg, yp, yp0i), yp);
+}
+
+/*
+ * Division: return num/den mod p.
+ * Parameter m16 is the Montgomery representation of 16 (i.e. 16*R mod p).
+ */
+TARGET_AVX2
+static __m256i
+mp_div_x8(__m256i ynum, __m256i yden, __m256i yp, __m256i yp0i, __m256i ym16)
+{
+	/*
+	 * Binary GCD between den and p:
+	 * Init:
+	 *    a <- den
+	 *    b <- p
+	 *    u <- num
+	 *    v <- 0
+	 * Invariants:
+	 *    a*num = u*den mod p
+	 *    b*num = v*den mod p
+	 *    b is odd
+	 * Operations:
+	 *    if a is odd:
+	 *        if a > b:
+	 *            (a, b, u, v) <- (b, a, v, u)
+	 *        (a, u) <- (a - b, u - v mod p)
+	 *    (a, u) <- (a/2, u/2 mod p)
+	 * Since p < 2^31, we always reach b = 1 in at most 60 iterations
+	 * (unless den = 0, in which case b remains equal to p). Then, we
+	 * have v = num/den mod p.
+	 * Updates to u and v are delayed: they are accumulated in "update
+	 * factors" (f0, g0, f1 and g1) and applied every 30 iterations.
+	 * Moreover, the update factors are themselves held by pairs: we
+	 * can do 15 iterations with f0 and g0 in the same 32-bit variable
+	 * (and the same for f1 and g1).
+	 */
+	__m256i ya = yden;
+	__m256i yb = yp;
+	__m256i yu = ynum;
+	__m256i yv = _mm256_setzero_si256();
+	__m256i yaf0 = _mm256_setzero_si256();
+	__m256i yag0 = _mm256_setzero_si256();
+	__m256i yaf1 = _mm256_setzero_si256();
+	__m256i yag1 = _mm256_setzero_si256();
+	__m256i yc = _mm256_set1_epi32(0x7FFF7FFF);
+	__m256i y1 = _mm256_set1_epi32(1);
+	__m256i y15 = _mm256_set1_epi32(0x7FFF);
+	__m256i y16 = _mm256_set1_epi32(0xFFFF);
+	for (int i = 0; i < 4; i ++) {
+		__m256i yfg0 = y1;
+		__m256i yfg1 = _mm256_set1_epi32(1 << 16);
+		for (int j = 0; j < 15; j ++) {
+			__m256i yaodd = _mm256_sub_epi32(
+				_mm256_setzero_si256(),
+				_mm256_and_si256(ya, y1));
+			__m256i yswap = _mm256_and_si256(yaodd,
+				_mm256_cmpgt_epi32(yb, ya));
+			__m256i yna = _mm256_blendv_epi8(ya, yb, yswap);
+			yb = _mm256_blendv_epi8(yb, ya, yswap);
+			__m256i ynfg0 = _mm256_blendv_epi8(yfg0, yfg1, yswap);
+			yfg1 = _mm256_blendv_epi8(yfg1, yfg0, yswap);
+			ya = _mm256_sub_epi32(yna,
+				_mm256_and_si256(yaodd, yb));
+			yfg0 = _mm256_sub_epi32(ynfg0,
+				_mm256_and_si256(yaodd, yfg1));
+			ya = _mm256_srli_epi32(ya, 1);
+			yfg1 = _mm256_slli_epi32(yfg1, 1);
+		}
+
+		yfg0 = _mm256_add_epi32(yfg0, yc);
+		yfg1 = _mm256_add_epi32(yfg1, yc);
+		__m256i yf0 = _mm256_sub_epi32(
+			_mm256_and_si256(yfg0, y16), y15);
+		__m256i yg0 = _mm256_sub_epi32(
+			y15, _mm256_srli_epi32(yfg0, 16));
+		__m256i yf1 = _mm256_sub_epi32(
+			_mm256_and_si256(yfg1, y16), y15);
+		__m256i yg1 = _mm256_sub_epi32(
+			y15, _mm256_srli_epi32(yfg1, 16));
+
+		/*
+		 * We apply update factors only once every two outer
+		 * iterations. af0, ag0... are the factors from the
+		 * previous round. The aggregate factors (for two rounds)
+		 * are thus:
+		 *   bf0 = af0*f0 - af1*g0
+		 *   bg0 = ag0*f0 - ag1*g0
+		 *   bf1 = af0*f1 - af1*g1
+		 *   bg1 = ag0*f1 - ag1*g1
+		 * The aggregate update factors are, in fact, exactly the
+		 * value we would obtain from running 30 inner iterations,
+		 * so they follow the rules explained in:
+		 *    https://eprint.iacr.org/2020/972
+		 * In particular, these factors are at most 2^30 in
+		 * absolute value. Thus, they do not overflow and we can
+		 * use them with mp_lin_x8().
+		 */
+
+		if ((i & 1) == 0) {
+			yaf0 = yf0;
+			yag0 = yg0;
+			yaf1 = yf1;
+			yag1 = yg1;
+		} else {
+			__m256i ybf0 = _mm256_sub_epi32(
+				_mm256_mullo_epi32(yaf0, yf0),
+				_mm256_mullo_epi32(yaf1, yg0));
+			__m256i ybg0 = _mm256_sub_epi32(
+				_mm256_mullo_epi32(yag0, yf0),
+				_mm256_mullo_epi32(yag1, yg0));
+			__m256i ybf1 = _mm256_sub_epi32(
+				_mm256_mullo_epi32(yaf0, yf1),
+				_mm256_mullo_epi32(yaf1, yg1));
+			__m256i ybg1 = _mm256_sub_epi32(
+				_mm256_mullo_epi32(yag0, yf1),
+				_mm256_mullo_epi32(yag1, yg1));
+			__m256i ynu = mp_lin_x8(yu, yv, ybf0, ybg0, yp, yp0i);
+			__m256i ynv = mp_lin_x8(yu, yv, ybf1, ybg1, yp, yp0i);
+			yu = ynu;
+			yv = ynv;
+		}
+	}
+
+	/*
+	 * Each inner loop computed the update factors with an implicit
+	 * 2^15 factor; two inner iteration lead to a 2^30 factor, but
+	 * mp_lin_x8() divided by R = 2^32. In total, we divided by (2^2)^2,
+	 * which we compensate here.
+	 */
+	yv = mp_montymul_x8(yv, ym16, yp, yp0i);
+
+	/* GCD is in b; it is 1 if and only if y was invertible.
+	   Otherwise, the GCD is greater than 1. */
+	return _mm256_and_si256(yv,
+		_mm256_cmpgt_epi32(_mm256_set1_epi32(2), yb));
+}
+
+#else // HAWK_AVX2
+
+/*
+ * Return (u*f - v*g)/R mod p
+ * f and g are provided as uint32_t but there are signed value (at most
+ * 2^30 in absolute value).
+ */
+static inline uint32_t
+mp_lin(uint32_t u, uint32_t v, uint32_t f, uint32_t g, uint32_t p, uint32_t p0i)
+{
+	uint32_t sf = tbmask(f);
+	f = (f ^ sf) - sf;
+	uint32_t sg = tbmask(g);
+	g = (g ^ sg) - sg;
+
+	/* We can do a simpler conditional negation because Montgomery
+	   reduction works over values up to p*2^32, so we can allow
+	   intermediate values to go up to p. */
+	u += sf & (p - (u << 1));
+	v += sg & (p - (v << 1));
+	return mp_sub(mp_montymul(u, f, p, p0i), mp_montymul(v, g, p, p0i), p);
+}
+
+/*
+ * Division: return x/y mod p.
+ * Parameter m16 is the Montgomery representation of 16 (i.e. 16*R mod p).
+ */
+static uint32_t
+mp_div(uint32_t x, uint32_t y, uint32_t p, uint32_t p0i, uint32_t m16)
+{
+	/*
+	 * Binary GCD between y and p:
+	 * Init:
+	 *    a <- y
+	 *    b <- p
+	 *    u <- x
+	 *    v <- 0
+	 * Invariants:
+	 *    a*x = u*y mod p
+	 *    b*x = v*y mod p
+	 *    b is odd
+	 * Operations:
+	 *    if a is odd:
+	 *        if a > b:
+	 *            (a, b, u, v) <- (b, a, v, u)
+	 *        (a, u) <- (a - b, u - v mod p)
+	 *    (a, u) <- (a/2, u/2 mod p)
+	 * Since p < 2^31, we always reach b = 1 in at most 60 iterations
+	 * (unless y = 0, in which case b remains equal to p). Then, we
+	 * have v = x/y mod p.
+	 * Updates to u and v are delayed: they are accumulated in "update
+	 * factors" (f0, g0, f1 and g1) and applied every 30 iterations.
+	 * Moreover, the update factors are themselves held by pairs: we
+	 * can do 15 iterations with f0 and g0 in the same 32-bit variable
+	 * (and the same for f1 and g1).
+	 */
+	uint32_t a = y;
+	uint32_t b = p;
+	uint32_t u = x;
+	uint32_t v = 0;
+	uint32_t af0 = 0;
+	uint32_t ag0 = 0;
+	uint32_t af1 = 0;
+	uint32_t ag1 = 0;
+	for (int i = 0; i < 4; i ++) {
+		uint32_t fg0 = (uint32_t)1;
+		uint32_t fg1 = (uint32_t)1 << 16;
+		for (int j = 0; j < 15; j ++) {
+			uint32_t a_odd = -(a & 1);
+			uint32_t swap = tbmask(a - b) & a_odd;
+			uint32_t t1 = swap & (a ^ b);
+			a ^= t1;
+			b ^= t1;
+			uint32_t t2 = swap & (fg0 ^ fg1);
+			fg0 ^= t2;
+			fg1 ^= t2;
+			a -= a_odd & b;
+			fg0 -= a_odd & fg1;
+			a >>= 1;
+			fg1 <<= 1;
+		}
+		fg0 += 0x7FFF7FFF;
+		fg1 += 0x7FFF7FFF;
+		uint32_t f0 = (fg0 & 0xFFFF) - (uint32_t)0x7FFF;
+		uint32_t g0 = (uint32_t)0x7FFF - (fg0 >> 16);
+		uint32_t f1 = (fg1 & 0xFFFF) - (uint32_t)0x7FFF;
+		uint32_t g1 = (uint32_t)0x7FFF - (fg1 >> 16);
+
+		/*
+		 * We apply update factors only once every two outer
+		 * iterations. af0, ag0... are the factors from the
+		 * previous round. The aggregate factors (for two rounds)
+		 * are thus:
+		 *   bf0 = af0*f0 - af1*g0
+		 *   bg0 = ag0*f0 - ag1*g0
+		 *   bf1 = af0*f1 - af1*g1
+		 *   bg1 = ag0*f1 - ag1*g1
+		 * The aggregate update factors are, in fact, exactly the
+		 * value we would obtain from running 30 inner iterations,
+		 * so they follow the rules explained in:
+		 *    https://eprint.iacr.org/2020/972
+		 * In particular, these factors are at most 2^30 in
+		 * absolute value. Thus, they do not overflow and we can
+		 * use them with mp_lin().
+		 */
+
+		if ((i & 1) == 0) {
+			af0 = f0;
+			ag0 = g0;
+			af1 = f1;
+			ag1 = g1;
+		} else {
+			uint32_t bf0 = af0*f0 - af1*g0;
+			uint32_t bg0 = ag0*f0 - ag1*g0;
+			uint32_t bf1 = af0*f1 - af1*g1;
+			uint32_t bg1 = ag0*f1 - ag1*g1;
+			uint32_t nu = mp_lin(u, v, bf0, bg0, p, p0i);
+			uint32_t nv = mp_lin(u, v, bf1, bg1, p, p0i);
+			u = nu;
+			v = nv;
+		}
+	}
+
+	/*
+	 * Each inner loop computed the update factors with an implicit
+	 * 2^15 factor; two inner iteration lead to a 2^30 factor, but
+	 * mp_lin() divided by R = 2^32. In total, we divided by (2^2)^2,
+	 * which we must compensate here by multiplying by 16.
+	 */
+	v = mp_montymul(v, m16, p, p0i);
+
+	/* GCD is in b; it is 1 if and only if y was invertible.
+	   Otherwise, the GCD is greater than 1. */
+	return v & tbmask(b - 2);
+}
+#endif // HAWK_AVX2
+
+#if HAWK_AVX2
+TARGET_AVX2
+static inline __m256i
+mp_NTT8(__m256i ya, const uint32_t *restrict gm, size_t k,
+	__m256i yp, __m256i yp0i)
+{
+	__m256i yt1, yt2, ya0, ya1;
+
+	/* 0/4, 1/5, 2/6, 3/7 with gm[1] */
+	/* ya <- a0:a1:a4:a5:a2:a3:a6:a7 */
+	ya = _mm256_permute4x64_epi64(ya, 0xD8);
+	/* yt1 <- a0:a4:a1:a5:a2:a6:a3:a7 */
+	yt1 = _mm256_shuffle_epi32(ya, 0xD8);
+	/* yt2 <- a4:a0:a5:a1:a6:a2:a7:a3 */
+	yt2 = _mm256_shuffle_epi32(ya, 0x72);
+	/* yg0 <- g1:g1:g1:g1:g1:g1:g1:g1 */
+	__m256i yg0 = _mm256_set1_epi32(gm[k]);
+	yt2 = mp_montymul_x4(yt2, yg0, yp, yp0i);
+	ya0 = mp_add_x8(yt1, yt2, yp);
+	ya1 = mp_sub_x8(yt1, yt2, yp);
+
+	/* ya0 = a0:--:a1:--:a2:--:a3:--
+	   ya1 = a4:--:a5:--:a6:--:a7:-- */
+
+	/* 0/2, 1/3 with gm[2]; 4/6, 5/7 with gm[3] */
+	/* yt1 <- a0:--:a1:--:a4:--:a5:--
+	   yt2 <- a2:--:a3:--:a6:--:a7:-- */
+	yt1 = _mm256_permute2x128_si256(ya0, ya1, 0x20);
+	yt2 = _mm256_permute2x128_si256(ya0, ya1, 0x31);
+	__m256i yg1 = _mm256_setr_epi32(
+		gm[(k << 1) + 0], gm[(k << 1) + 0],
+		gm[(k << 1) + 0], gm[(k << 1) + 0],
+		gm[(k << 1) + 1], gm[(k << 1) + 1],
+		gm[(k << 1) + 1], gm[(k << 1) + 1]);
+	yt2 = mp_montymul_x4(yt2, yg1, yp, yp0i);
+	ya0 = mp_add_x8(yt1, yt2, yp);
+	ya1 = mp_sub_x8(yt1, yt2, yp);
+
+	/* ya0 = a0:--:a1:--:a4:--:a5:--
+	   ya1 = a2:--:a3:--:a6:--:a7:-- */
+
+	/* 0/1 with gm[4], 2/3 with gm[5], 4/5 with gm[6], 6/7 with gm[7] */
+	/* yt1 <- a0:--:a2:--:a4:--:a6:--
+	   yt2 <- a1:--:a3:--:a5:--:a7:-- */
+	yt1 = _mm256_unpacklo_epi64(ya0, ya1);
+	yt2 = _mm256_unpackhi_epi64(ya0, ya1);
+	__m256i yg2 = _mm256_setr_epi32(
+		gm[(k << 2) + 0], gm[(k << 2) + 0],
+		gm[(k << 2) + 1], gm[(k << 2) + 1],
+		gm[(k << 2) + 2], gm[(k << 2) + 2],
+		gm[(k << 2) + 3], gm[(k << 2) + 3]);
+	yt2 = mp_montymul_x4(yt2, yg2, yp, yp0i);
+	ya0 = mp_add_x8(yt1, yt2, yp);
+	ya1 = mp_sub_x8(yt1, yt2, yp);
+
+	/* ya0 = a0:--:a2:--:a4:--:a6:--
+	   ya1 = a1:--:a3:--:a5:--:a7:-- */
+	ya = _mm256_blend_epi32(ya0, _mm256_slli_epi64(ya1, 32), 0xAA);
+	return ya;
+}
+#endif // HAWK_AVX2
+
+/*
+ * Assumption: logn >= 3
+ */
+TARGET_AVX2
+static void
+mp_NTT(unsigned logn, uint32_t *restrict a,
+	const uint32_t *restrict gm, uint32_t p, uint32_t p0i)
+{
+#if HAWK_AVX2
+	__m256i yp = _mm256_set1_epi32(p);
+	__m256i yp0i = _mm256_set1_epi32(p0i);
+	size_t n = (size_t)1 << logn;
+	size_t t = n;
+	for (unsigned lm = 0; lm < (logn - 3); lm ++) {
+		size_t m = (size_t)1 << lm;
+		size_t ht = t >> 1;
+		size_t v0 = 0;
+		for (size_t u = 0; u < m; u ++) {
+			__m256i ys = _mm256_set1_epi32(gm[u + m]);
+			for (size_t v = 0; v < ht; v += 8) {
+				size_t k1 = v0 + v;
+				size_t k2 = k1 + ht;
+				__m256i *a1 = (__m256i *)(a + k1);
+				__m256i *a2 = (__m256i *)(a + k2);
+				__m256i y1 = _mm256_loadu_si256(a1);
+				__m256i y2 = _mm256_loadu_si256(a2);
+				y2 = mp_montymul_x8(y2, ys, yp, yp0i);
+				_mm256_storeu_si256(a1,
+					mp_add_x8(y1, y2, yp));
+				_mm256_storeu_si256(a2,
+					mp_sub_x8(y1, y2, yp));
+			}
+			v0 += t;
+		}
+		t = ht;
+	}
+	size_t m = n >> 3;
+	for (size_t u = 0; u < m; u ++) {
+		uint32_t *za = a + (u << 3);
+		__m256i ya = _mm256_loadu_si256((__m256i *)za);
+		ya = mp_NTT8(ya, gm, u + m, yp, yp0i);
+		_mm256_storeu_si256((__m256i *)za, ya);
+	}
+#else // HAWK_AVX2
+	size_t t = (size_t)1 << logn;
+	for (unsigned lm = 0; lm < logn; lm ++) {
+		size_t m = (size_t)1 << lm;
+		size_t ht = t >> 1;
+		size_t v0 = 0;
+		for (size_t u = 0; u < m; u ++) {
+			uint32_t s = gm[u + m];
+			for (size_t v = 0; v < ht; v ++) {
+				size_t k1 = v0 + v;
+				size_t k2 = k1 + ht;
+				uint32_t x1 = a[k1];
+				uint32_t x2 = mp_montymul(a[k2], s, p, p0i);
+				a[k1] = mp_add(x1, x2, p);
+				a[k2] = mp_sub(x1, x2, p);
+			}
+			v0 += t;
+		}
+		t = ht;
+	}
+#endif // HAWK_AVX2
+}
+
+/*
+ * This is a variant of mp_NTT, when the source polynomial is auto-adjoint.
+ * Only the first n/2 elements of a[] are accessed or modified; the upper
+ * n/2 elements are untouched (and need not exist at all in memory).
+ *
+ * Assumption: logn >= 3
+ */
+TARGET_AVX2
+static void
+mp_NTT_autoadj(unsigned logn, uint32_t *restrict a,
+	const uint32_t *restrict gm, uint32_t p, uint32_t p0i)
+{
+#if HAWK_AVX2
+	__m256i yp = _mm256_set1_epi32(p);
+	__m256i yp0i = _mm256_set1_epi32(p0i);
+	size_t hn = (size_t)1 << (logn - 1);
+
+	__m256i ys1 = _mm256_set1_epi32(gm[1]);
+	__m256i yrev = _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0);
+	size_t qn = hn >> 1;
+
+	__m256i yt1 = _mm256_loadu_si256((__m256i *)a);
+	__m256i yt2 = _mm256_loadu_si256((__m256i *)(a + (hn - 8)));
+	__m256i yrev7 = _mm256_setr_epi32(0, 7, 6, 5, 4, 3, 2, 1);
+	yt2 = _mm256_permutevar8x32_epi32(yt2, yrev7);
+	__m256i yu1 = mp_sub_x8(yt1, mp_montymul_x8(yt2, ys1, yp, yp0i), yp);
+	__m256i yu2 = mp_sub_x8(yt2, mp_montymul_x8(yt1, ys1, yp, yp0i), yp);
+	yu2 = _mm256_permutevar8x32_epi32(yu2, yrev7);
+	yu1 = _mm256_insert_epi32(yu1, a[0], 0);
+	yu2 = _mm256_insert_epi32(yu2, a[hn - 8], 0);
+	_mm256_storeu_si256((__m256i *)a, yu1);
+	_mm256_storeu_si256((__m256i *)(a + (hn - 8)), yu2);
+
+	for (size_t u = 8; u < qn; u += 8) {
+		__m256i yv1 = _mm256_loadu_si256((__m256i *)(a + u));
+		__m256i yv2 = _mm256_loadu_si256((__m256i *)(a + (hn - 7) - u));
+		yv2 = _mm256_permutevar8x32_epi32(yv2, yrev);
+		__m256i yw1 = mp_sub_x8(yv1,
+			mp_montymul_x8(yv2, ys1, yp, yp0i), yp);
+		__m256i yw2 = mp_sub_x8(yv2,
+			mp_montymul_x8(yv1, ys1, yp, yp0i), yp);
+		yw2 = _mm256_permutevar8x32_epi32(yw2, yrev);
+		_mm256_storeu_si256((__m256i *)(a + u), yw1);
+		_mm256_storeu_si256((__m256i *)(a + (hn - 7) - u), yw2);
+	}
+	a[qn] = mp_sub(a[qn], mp_montymul(a[qn], gm[1], p, p0i), p);
+
+	size_t t = hn;
+	for (unsigned lm = 1; lm < (logn - 3); lm ++) {
+		size_t m = (size_t)1 << lm;
+		size_t ht = t >> 1;
+		size_t v0 = 0;
+		for (size_t u = 0; u < (m >> 1); u ++) {
+			__m256i ys = _mm256_set1_epi32(gm[u + m]);
+			for (size_t v = 0; v < ht; v += 8) {
+				size_t k1 = v0 + v;
+				size_t k2 = k1 + ht;
+				__m256i *a1 = (__m256i *)(a + k1);
+				__m256i *a2 = (__m256i *)(a + k2);
+				__m256i y1 = _mm256_loadu_si256(a1);
+				__m256i y2 = _mm256_loadu_si256(a2);
+				y2 = mp_montymul_x8(y2, ys, yp, yp0i);
+				_mm256_storeu_si256(a1,
+					mp_add_x8(y1, y2, yp));
+				_mm256_storeu_si256(a2,
+					mp_sub_x8(y1, y2, yp));
+			}
+			v0 += t;
+		}
+		t = ht;
+	}
+	size_t m = hn >> 2;
+	for (size_t u = 0; u < (m >> 1); u ++) {
+		uint32_t *za = a + (u << 3);
+		__m256i ya = _mm256_loadu_si256((__m256i *)za);
+		ya = mp_NTT8(ya, gm, u + m, yp, yp0i);
+		_mm256_storeu_si256((__m256i *)za, ya);
+	}
+#else // HAWK_AVX2
+	size_t hn = (size_t)1 << (logn - 1);
+	uint32_t s1 = gm[1];
+	size_t qn = hn >> 1;
+	for (size_t u = 1; u < qn; u ++) {
+		uint32_t x1 = a[u];
+		uint32_t x2 = a[hn - u];
+		a[u] = mp_sub(x1, mp_montymul(x2, s1, p, p0i), p);
+		a[hn - u] = mp_sub(x2, mp_montymul(x1, s1, p, p0i), p);
+	}
+	a[qn] = mp_sub(a[qn], mp_montymul(a[qn], s1, p, p0i), p);
+
+	size_t t = (size_t)1 << (logn - 1);
+	for (unsigned lm = 1; lm < logn; lm ++) {
+		size_t m = (size_t)1 << lm;
+		size_t ht = t >> 1;
+		size_t v0 = 0;
+		for (size_t u = 0; u < (m >> 1); u ++) {
+			uint32_t s = gm[u + m];
+			for (size_t v = 0; v < ht; v ++) {
+				size_t k1 = v0 + v;
+				size_t k2 = k1 + ht;
+				uint32_t x1 = a[k1];
+				uint32_t x2 = mp_montymul(a[k2], s, p, p0i);
+				a[k1] = mp_add(x1, x2, p);
+				a[k2] = mp_sub(x1, x2, p);
+			}
+			v0 += t;
+		}
+		t = ht;
+	}
+#endif // HAWK_AVX2
+}
+
+/*
+ * For modulus p:
+ *   p0i = -1/p mod 2^32
+ *   R2 = 2^64 mod p
+ *   R3 = 2^96 mod p
+ *   m16 = 16*2^32 mod p
+ *   GM[i] = (2^32)*(g^rev(i)) mod p
+ * with g being primitive 2048-th root of 1 (i.e. g^1024 = -1 mod p) and
+ * rev() being the bit-reversal function over 10 bits.
+ * For g, we use s^((p-1)/2048) mod p, where s is the smallest positive
+ * integer which is not a quadratic residue modulo p.
+ */
+
+#define P1       2147473409
+#define P1_0i    2042615807
+#define P1_R2     419348484
+#define P1_R3    1819566170
+#define P1_m16       327648
+#define P2       2147389441
+#define P2_0i    1862176767
+#define P2_R2    1141604340
+#define P2_R3     976758995
+#define P2_m16      3014624
+
+static const uint32_t GM_p1[] = {
+	     20478,  276077475, 1688385698,  761932726, 1138340463, 1021445260,
+	 870248807,  829510770, 2071018143,   26820577, 1426496990,  384180207,
+	  13882587,  577122007, 1722769540, 1260619389,  904179219, 1933219019,
+	2017597084, 1980090712,  851314292, 1881833318, 1050710868,  292816539,
+	 889385875,  720685218, 1363110108,  294709449, 1507235110,  863691339,
+	1415779453,  577236769, 1356088707,  859867539,  495731206,  347731418,
+	2073233829, 2115445737,  254895402,  478229880, 1234534812,  897499182,
+	 768652460, 1665656027, 2064969281,  358946764,  613156407, 2057649013,
+	 865570366, 1603257950,   93784487, 1555405419,  430242169, 2016636767,
+	  15592939, 2063248902,  208260274,   40828442,  865639666, 1846671086,
+	 530045817, 1784636572,  434310431, 2034729522,   72005396, 1417019068,
+	 760717036, 1499147279, 1074727870, 1621768087,  901308486, 1903420619,
+	1374299204, 1561642259,  152372511,  253142775, 1885840122, 1195438051,
+	 349011077,  595407623,  937424114, 1675484834, 1009429298,  769141268,
+	 617989515,  362418520, 1507272315, 1697181709,  884880928, 1332962991,
+	2012484716, 1316703310,  385827688, 1979578328,  336092444, 1222074628,
+	2068318345, 1389478811,  614181936,  995572856, 1056612365, 1936377707,
+	2047801350,  726552192,  724676777, 1890296365,  634473887,  885891667,
+	1117786517,  606908348,  158269774,  349038201,  117167000,  707059113,
+	1022519749,  130182295,  572225325, 1832878810, 1546970796, 1401054322,
+	 604077445,  476025069,  801284700,  226545108, 1406213035,  361304820,
+	1840279680, 1572098519, 1166932694,  871641941,  355203126, 1527038490,
+	 920138677,  803717901, 1120014893, 1250819809,  411243131,  133719755,
+	1339419901, 1753499439, 1856740559, 1820059845,  627018263,  173517655,
+	 263148905, 1225387363, 1647095464, 1789037828, 1237357286,  424234403,
+	2022697741, 1993242204,  472346747,  736320822, 1422570303, 2034712410,
+	1521115092,   59511560,  233083739, 1071038359,  138044003,  585194459,
+	2063546001,   95336925,  122782184, 1241210668,  937341603,   23556949,
+	1744406767,  756806207,   27626758, 1670962005,  845043683, 1695011082,
+	 291079245, 1426975523,  177745285,  171730410,  894935878,  589703334,
+	 368695544, 2146021799, 1998710711, 1625208611, 1038019566, 1735822596,
+	1458959865,  536700716,   55127253,   33645590, 1293553947,  519357705,
+	1796389440,  532823571,  851537631,  439622598,  701386542,  226984529,
+	1900496338, 1359606048, 1300511823, 1878544316, 1917747700, 1219229593,
+	 716726745, 1881089376,   60741675, 1617103732, 1964359457, 2069854791,
+	 107674061,  686746896,  388509663,  244188980,  929151696,  622041584,
+	 414138988,  546275715, 1041130016, 1533321697, 1833810336, 1401193711,
+	1907983563, 1051788562, 1422084989,  214049450,   39017815, 2053123362,
+	2124366396,  533121484, 1959901195,  319344115, 1217983143, 2126710730,
+	 279479502, 2073299949,  146368726,  656585392, 1791775888, 2013070236,
+	1117897425, 1457198373,  815373156,  365463730, 1662227660,  507044354,
+	1157976583,  375250987, 1229900670, 1117341706,  212929125, 1419915198,
+	2132426927, 1671190125, 1711523818,  925978889, 1224002191,  458274263,
+	2083360144, 1779032320,  661970963,  453653695,  657293962, 1611752815,
+	1541857779, 1668004360, 1064388666, 1596405886, 1308837876, 1290842306,
+	 375999699, 1591206728,  688746554,  990123771, 1335220678, 2004995230,
+	1113023865,  587323686, 1053377545, 1930726543,  184237055, 1019351378,
+	2028012092, 1121557766, 1308820241, 1803419021, 1456239987,  596123337,
+	 123299236, 1213294379,  692834991,  577073536,  909039648, 1177323686,
+	  89776129,  346566096, 1021398434,   23598762, 1152552592, 1570217184,
+	1130049051,  923149110, 1226726700, 2022655059,  325622654,   45265626,
+	1041626893, 1936548398,  656976755, 1025242376, 1723768901,  610867220,
+	 230825811, 2083697236, 1210403043,   44503338,  864192739, 1564705317,
+	 110420908, 1234098582,  304390842,  818465205,  242446514,  350800442,
+	1577123608,  670710989,  416747844,  858828029, 1811181248, 2051206439,
+	1306395643,  981217938, 1943333004,  300209048, 1621161689,  696476799,
+	 915956451,   74087519, 1786333708,  769098576, 1366809333,  626344703,
+	1889035632, 1386530081,  866100449, 1862075713,  116818339,   16761336,
+	1942776517,  987244652, 2125889249,  619927687, 1285102690, 1928996642,
+	1341975713,  196341048, 1023580067,  449780147, 1712401383, 1291972102,
+	1311351410, 1751622449, 1810344957, 1427465309, 1998671058,  138319943,
+	1539053190,  681335526, 1914088724, 1937041613, 1461731252, 1549542054,
+	2057497941,  343265594, 1386029469, 1811074169,   73332584,  967162782,
+	1520294965,  528910843, 1614858633,  767170944, 1178765994, 1689360220,
+	 570009527,  907400393, 1898713577,  261714461,  321145051, 1990755768,
+	1218300312, 1688993883, 1078585155,  595378406, 2144101180, 1883408998,
+	  90674327, 1775658489, 1916101296, 1411305669,  355104385,  732871353,
+	 906656083, 1852436312, 1679378757, 1671113936, 2042504253,  536576504,
+	 497903458, 2117576585, 1525891773,  930467717, 1924508057, 1010032240,
+	 858312866,  682057827,  803778033, 1887005062, 1885367618,  825388599,
+	1047898330, 1270636295,   40098665, 1052606830, 1574791372, 1296306877,
+	1775024299,  704958127, 1901229227,  452149879, 1341693007,  914634230,
+	1281018795, 1227927428,    7400504,   78117101, 1400807206, 1572094690,
+	1200720226, 1300577167, 1130830146, 1011652166, 1459553519, 1806401605,
+	1126824389, 1302351063, 1992416865,   38163005,  503996986,   46983263,
+	 930159775, 2120264001,  249287608,  544554228, 1651136098,  278811392,
+	  43827035, 2094598565,  136767950,  277580829, 1880905914, 2033552566,
+	  86730434,  921540201,  549224603, 1178659383, 1332559691, 1826157172,
+	1150267634,  557997259, 1989989449,  909540133, 1819970661, 1540392471,
+	2004269739, 2083711935,  279801116,  167487064,  271736710,  828609888,
+	 299997693, 1743107346,  892846676,  645477138,  965401638,  870339971,
+	1015711103,  723089724,  271389756,  146539013,  675250822, 1693722626,
+	 515143301, 2094035615,   19427832, 1502606781,  632939895,  420826764,
+	2103115595, 1039935916, 1293980839,  526163207, 1224609315,  849364322,
+	 989463231, 1604020817,  780616727, 2036431109, 1509796635, 1159482103,
+	 413580819, 1559843409, 1790111537, 1202797834,  117002251, 1144670472,
+	1326133417, 1996180806, 1981249647,  614023950, 1675369087,  589303603,
+	 909185230, 1439901675,  146330143, 1901025173,   82363662,  624688200,
+	1353844204, 1250338764,  388319277, 1504288281,  310072049,  751188341,
+	 869587863,   31844112, 1458505756,  811359588,  547528896, 1662753828,
+	1247125052, 1769077842, 1243652758, 1192584586, 1843692173, 1368486872,
+	1375047202, 1793821540, 1244243154, 2129557563,  319487216, 1845939190,
+	1042735899, 1889455632,  667575758, 2062422674, 1847502619,  814568122,
+	 909158693,  948432950, 1025400363, 1216549935, 1665672950,  437595396,
+	1500648391,  949854479, 1078985562,  721114950, 1454099563,  759799383,
+	1046138978,  783748234, 1988568187, 1124904793, 1228513796,  609005725,
+	 158401014, 1496088724,  618177510, 2132735259,  970837697, 1584111699,
+	1255665028, 1043660987,  178631984, 1126476118, 1766607753, 1506934718,
+	 945005486,  518080758, 1920653973,  832808904,  886796275,  298393013,
+	1417610475, 2119073318, 1457473075,  680312246, 2077118775,  568187962,
+	 632356090, 1153586587,  701570167,  174727464,  438214724,  909063096,
+	1083882068, 1929283240,  428330566, 1903048380, 1633931132, 1230298232,
+	 838507832, 1399829344, 1800728559, 1114070798,  739540028, 1509915010,
+	 291489353, 1487500610,  409385375,  702348742,  745431275, 1193114225,
+	1469053117, 1486427244,  141903292,  451606796,  633045416,  519265308,
+	1003128400, 1475481540,   84643593,  981144629, 1579759149, 1394930788,
+	1616905114, 1865443581,  547830165,  918185560,  589743914,   34076823,
+	 600033931, 1217547625,  250480931,  305001212,  498008436,   13087068,
+	 554030411, 1144562565,  744723016, 2053213690, 1976419571,  732447240,
+	1107697424,  200570586,  297822305,  329269848,  892983345, 1302896585,
+	1738952407,  914716774, 1016235154, 1042157012, 1100375860, 1030024078,
+	 838660551,   77707346, 1816437515,  112052915, 1533136761,  182333635,
+	 620250513,  944653135, 1929778347,   99604862, 1338501791,  497661432,
+	 430905130,  101626919,  717940774, 1620557324, 2034911560,  253059400,
+	1102076764,  471591502,  844121319, 1630162912, 1210073260, 1827915047,
+	1182481947, 1475112556, 1200248030,  416215933,  532885224,   74555155,
+	 138497203,  336347601, 1103666447,  798712930,  285734844, 1495879060,
+	1053133847,  653062581, 1395120389, 2110517746, 1958384415,   34101643,
+	 745928690,  725854416,   36380873,  865483096,  632740760, 1523020547,
+	 867794962,  164508864, 1837906739, 1742360155,  237693636, 1329399516,
+	1829813931,  131706268, 1106171501,  666344300, 1450762766, 1124426192,
+	1012033666,  599621172,  237050852,  606674748,  342655552,   53427652,
+	1891601465, 1042612558,  251151473,  860834475,  299567806, 1145685328,
+	 268929217, 1930129187, 1726208529, 1686663211,  739517200, 1978593422,
+	 877630023, 1963259234, 2024264059,  408759332, 1865619299, 1201317263,
+	 368359460,  129979470,  433898878,  663911333,  959850992, 1754304644,
+	1725784329, 1563253760, 1844571806,  735499159,  408009966, 1673369135,
+	  10151567,  557725073,  702278661, 2031867842, 1262006172,  401708610,
+	1498855589, 1377377603,  638953550,  325366876,  597812889,  681948734,
+	1650421621,  254734336, 1747384097, 1291543540, 1141026409, 1091610258,
+	 392629494, 1708472593, 2102397705, 2043582652, 1768934917,  481998512,
+	 988047421, 1723631610,  444279182, 1279863678,  635865792, 1931754710,
+	 746566889,  612530185, 1293170492, 1816391575, 1327065362, 1247963721,
+	 244167333,  604172859, 1715728363,  126466518, 1003155197,  724002282,
+	1497108684, 1259955686,  879208696, 1320207079, 1920501463, 1345760748,
+	2070919415, 2069595143, 1349313787,  969943473,  847987463, 1805123632,
+	1032472062, 2086424949, 1213527249, 1246853623,  837965912, 2067749933,
+	 625152138,   89146939, 1696589581, 1507715632, 1881560996,   13893713,
+	 549453266, 1276787275, 1976375304,  340460424,  591815512,  648284075,
+	 163257661,  864124470,  704782200,  510870296,  340373961, 1520676335,
+	1152412135, 1214083467,  475362314, 1664176279,  627414670,  424650704,
+	1716551107,  683659719, 2057746809, 1270950650, 1854042026, 1223757075,
+	1856750643,   97340173, 1199409222,  104918287, 1586722213, 1872074540,
+	 116978614,  830723457, 1797861576, 1906232791,  652518947,  887210304,
+	1706343321,  838773546, 1930389538, 1606048260, 1524272525, 1570448333,
+	 242493479,  448828658,  401422089, 1629405374,  898581305, 2011935011,
+	 153965010, 1093277618, 1419854975,  743542714,  673328623, 1257330191,
+	1125943259,   14645907,  166327009,  130151644,  467028838, 1553116262,
+	1293709204, 1485261058, 1773824461, 1323232915, 1586092112,  985286537,
+	1422602167,  756632308, 1098844473, 1946260961, 1809471626,  461664429,
+	  24238557, 1080828576,   14549292, 1808900612,  592028197,  597697600,
+	 844383305, 1346676285,  911563103, 1512308729,   78848577, 1123905571,
+	 413530260,  323580692, 1517449176, 1907562715,  388241574, 1612885131,
+	 630612412, 1211488241, 1352051177,  812504369, 2116849558, 1079404813,
+	 929499315, 2048302692, 1248909687, 1160035325,  709727367, 1076707193,
+	1808857706, 1239248699, 2005880575,  988036577,  510264390, 1555654950,
+	1784316547,  577021955, 1965524472, 1074915683,  856407927, 1154124024,
+	 944344338, 1455525416, 1219394988,   24190940, 1522390293, 1929278133,
+	 975753860,  492967930,  632306077,  957837901,  590670101, 1984280579,
+	1235150233, 1922371347, 2143499093, 1852491350, 1079172972, 1529986185,
+	 390383373,  533692964,  543191472, 1444442805, 1690223550,  130085257,
+	1504417556, 1934334110,  236195813,  108960254, 1371149771, 1947385885,
+	 403621755,  515851906,  799013576,  461039313,  668590434,  758076422,
+	1761115327, 1585803130,  325165043,  757198833,  639697617, 1864574289,
+	1038570065, 1955909152,  914185250,  868647112, 1395698776,  468502101,
+	1548478888,  799022362, 1508734297,  420027969, 2031373060,  318627896,
+	1244553510,  141809776, 2068042335, 1747535105, 1431944840, 1530932574,
+	 507373255, 1150760238, 1531282814,  902672318, 2110249730, 1036970955,
+	 288573611, 1523839811,  951860614,  341673792, 1425070922,  927375324,
+	 374705249,  698002991, 1883310823, 1806010362, 1680777670, 1724191875,
+	1047630063, 1458433532, 1161258992, 1361307344
+};
+
+static const uint32_t GM_p2[] = {
+	    188414,  800844398, 1836298316, 1316983235, 1780194382, 1668674867,
+	1876898365,  482608607, 1536539100,  240878826, 2072934951, 1507793182,
+	 388886950,  544012720,  623971434, 1537514969,  564156249, 2026329244,
+	1169238305, 2116218470, 1755677505,  516332411, 2134172303, 1274742716,
+	  10156516,  874196011, 1146980039,  700067736, 1559979991,  472958838,
+	1857163854,  430993288, 1554664514, 1127812475,  800590259, 1831017538,
+	2085269795, 1190374405,  917848286,  743950221, 1292001292, 1587294613,
+	1816925571,  366363604, 1082647155, 1185071803, 1240791685, 1408372324,
+	 617500310, 1082289886,   10922647,  153116756, 1920875162,  891503444,
+	1432236273,  992755667,  566356501, 2003658946,  498863850, 1423355926,
+	1915416518,   41125005,  627808605, 2031299840,  683054160,  644325914,
+	 499678143, 1336706381, 1155624913,  994857681, 2032239795, 1992489933,
+	 215226666,   34808368, 1909683994,  889515154, 1302078304, 1532709873,
+	1366016693, 1601718349, 2022301192, 1587290046, 1937620842,  289781565,
+	 222477407,  468803542,  518431148,  294767756, 1499640124,  137663468,
+	1614566563,  723325274,  346400867,  520169341, 1560118441, 1256348401,
+	 420255414, 2045145768,  445763958, 1957279431, 1089897312, 1823785559,
+	1335442474,  628920254,  279364806,  394295299,  964686317,  711507074,
+	1690049866, 1206461796,  453690225,  551309632, 1889707476,  181265196,
+	 831006161,    5390203, 1031157967,   62957762,  680099046, 1225125850,
+	1918370150, 1347691183, 1753907129, 1812471308,  227048573,  593194929,
+	 997952947,  583017538, 1440833249, 1325395522,  995751503,  758791449,
+	 188172367, 1595490957, 1092692519,  252120048, 1617750316, 1896464617,
+	1149396899, 1434711773, 1456425607,  556200145, 1196999469,  856846016,
+	1943632162, 1932016102, 1771529939, 2004809953, 1617760120,  312942871,
+	 222939007,  211277906,  591793121, 1418587678, 1202513332, 1213649888,
+	 484489363,   51967098, 1395106310,  900003303,  567084680, 1790395392,
+	2071175318,  780936991, 1752405816,  693637362,  740145789,  650706339,
+	 302883486,  148679578,  911686989,  557316307,  336374168, 1903912849,
+	 615626314,  490189735, 2032388954,  926592571,  734573505, 1483387521,
+	1357349883, 1083511859, 1267428840,  347329879, 1143121434, 1586578240,
+	 146861552, 1532495907, 2124987676, 1048532778, 1077266041, 1117086595,
+	 689949358, 1068135350,  431710628, 1074622917, 1288317211, 1480936222,
+	 180241586,  421417660,  140818127, 1915163045,  123893560, 1737204941,
+	 703582550, 1073121055,  378838939, 1926843686, 1047812800,  589279768,
+	1457757792, 1247020075, 1639303227,  349300191,  136285747, 1064141730,
+	 886451495,  281645262,   59919543,  155276342,   29026456,  325545549,
+	 393696585,  579668960,  210745972, 2024600725, 1507975016,  387340488,
+	 400206015, 1623761418, 1343456319, 2101574429,  466920733, 1958554927,
+	1228886862, 1058433729,  780360398, 1792450489,  900892394, 1258481715,
+	 426713499, 1474683970,  131085194, 1939138799, 1028695955,  242084002,
+	 140605507, 1877846335, 1100147871,  273096450, 2126409091, 1862069753,
+	2111692429, 1530579988,  568291714,  131985840, 1003496155, 1647402985,
+	 931560424, 1623282579, 1903389091, 1192494857, 2139778950,  523133012,
+	 578148045,  108054268, 1943839217,  779929261,  187567130, 1233240106,
+	1826914648,  330785322,  349157420,   11273568, 2088408633, 1245375279,
+	1642596731,  445181866,  640222483, 1380727962, 1485344879,  654678981,
+	1615857474, 1336203637,  584637099,  411295468, 1885626069, 1260964109,
+	 580493593, 1513525322, 1116263926, 1886720168, 2086527716, 1086446753,
+	1294031540, 1443688172, 1285013232,  901967352, 1203237258,  939546925,
+	1214016639,  409021468, 2043372921, 1058152627,  883390189,  493551388,
+	 389568489,  383565368,  397237151,  832432388,  820048426,  885774060,
+	 137663796, 2003585073, 1124260993,  783642587, 2088466763,  974201121,
+	1211724818,  827383675, 1254109769, 1914948942,  887861792, 2065626072,
+	1967830391, 1208676475, 1525588909,   94643456, 1848540735,  842250812,
+	1886819651,  876386489, 1748999278,  700527067, 1739374519, 1325228668,
+	 160938898, 1232859535, 1908829603,  574230485, 1889039614,  128889256,
+	 428580633, 1251143298,  586815366,  983979359,  103683529, 1439527107,
+	 975244445, 1618214863, 1045984294,    9203382, 1384966915, 1164922792,
+	1508271814, 1347592503,  918789642, 1953875311, 1105947184, 1714608389,
+	 990812625,  837908263, 1586123643, 1167580957, 1550318118,  355689451,
+	 271669567,  885586853,  309986321, 1878559263, 1030824142,  632062282,
+	1758793692, 1032447117,  964851845, 1342498378,  231445866,  342368698,
+	 792958440,   34959533,   89135815,  687244446, 2143475638, 1335953248,
+	 964487639, 1949598297,  257738458, 1658405004,   87101421,   33866132,
+	 516453656,  212513291, 2090783108, 1578188696,  230647921,   22649291,
+	1152425856, 1715344274, 1482811363, 1601353271, 1858075840, 2008441088,
+	1818203951,   10808313,  431597898,  129507061,   62255403, 1162130477,
+	 743767283, 2009648457,   74771973, 1077791439,  640908479,  238871041,
+	1369859944,  464263469, 1316249001, 1080490145,  460190380, 1044655799,
+	 199560577, 1138794546,  391199045, 1387626260, 2135864202, 1697628854,
+	  37177883,  826722236, 1386411384, 1795258277,  798222235, 1950488085,
+	1480461342, 1006402752,  263630500,   61953912, 1690738761, 1232533229,
+	1065869122, 1162915330,  146088019, 1038903044,  341780976, 2069324444,
+	 826050042, 1958502644,  467918430, 1237314953,   36727052,  949489545,
+	1665222534,  593996630,  246658931,  902073858, 1582107096,  308217284,
+	1071533715,  322297018,  841593029, 1868529972, 1802998967,  670439288,
+	1466706083,  453901974, 1596792613,   33614118, 1916038714,  349087247,
+	 150500802,  344188537,  752940833,  154697538, 1888514636,  438111364,
+	1608203920,  121404176, 2106310373, 1601602748, 2118303190,  683722437,
+	1297050626,  176431468, 1707213588,  569841682,  343812661, 1973539602,
+	1678694504,  326476685, 1522490240,  394625844, 1937771236, 1945857990,
+	1201451834,  421367979,  838341318,  146363541, 1848507395,  145178700,
+	1540754890, 1333467727,  720369117,  442317435,  801974167, 1194629469,
+	 678922120, 1271864261,  612833486, 1970668877,  643827611, 1343246788,
+	 365322845,  129722976,  677655126,  298282875, 1395418115,  284009530,
+	 300258726,  399026718,  382510332, 1803732898,  427796945, 1544854479,
+	1273559369,  464935884,  258371998,  725494215, 1761446815, 1201466935,
+	 181959172, 1672237273,   26156780, 1328760851, 1881115117,  576106112,
+	1139428990, 2080590732,  822307267, 1467953565,  110078146,  496358325,
+	  19247995, 1542178163,  301142961,  577750067,  203294405,  287043649,
+	   6092627,  622602577, 1121594775, 1138213710,  331589734, 1263367198,
+	1793090743,  742730233, 1276953771, 1829469791, 1418916595, 1177321925,
+	1898312775, 1633701024,  100067957,  519747549,  265226334,  103281107,
+	 106997942, 1258408982,  262900075, 1518495250,  250149089, 1763685360,
+	 900791668,  753191878, 1679228019, 1812149299,  498846008,  367840055,
+	 152352108, 1786131540, 1384348595,  380734937, 1756045385, 2107159067,
+	 289895897,  549186096,  451440053, 2100164659,  459526429, 1158297688,
+	 640821771,  747017328,  995933539,  544914700, 1540248482, 1884098989,
+	 311179263, 1100588606, 1982674278, 1882755543,  673777687,  271104605,
+	2112996656, 1564459071, 1518775480, 1975080836, 1332656241, 1952295225,
+	 211938949, 1888873510,  706765073, 1456655195,  423910784, 1236384873,
+	1654948437,  379578112,   75094808,  698853585, 1980367894, 1103671188,
+	 795916535,  961046890, 1361891519, 2013925057, 1671660768,  326777440,
+	1473832834, 1007127031, 1610029129, 1188008450,  189894838, 1067709058,
+	1766124618, 1884400828, 1830906126, 2131178216, 1918083139, 2111855665,
+	 696549428, 1725967527,  423407733,  867627404,  953654908, 1193355515,
+	1270957094,   91737545, 2086247014,  249407662, 1088256985, 1413586645,
+	1901949274, 1931883585,  706155338,  832639723,   15933097, 1106425007,
+	 553766683, 1407756856, 1944121174, 1331469270,  374840113,   25376524,
+	1950864447,  657949688,  184584107,  338328461, 1551695475, 1420045117,
+	 174566115,  501036435,  457465168, 1031951008,  818194854, 1547192269,
+	 947572700,  455728514,  540786346,  884003438,  178693036, 1837084279,
+	 869814370,  990185548, 1055549196, 1342116841,  713892792, 1611719604,
+	1732566795,  521875044,  965776340,  969170609, 1100900800,  811753482,
+	 418934975, 1722802985,   60576544,  433493384, 1935577880,  463071507,
+	1041887457, 1099929728, 1415134313, 1497002403, 2015400126, 1837915900,
+	1301106554,  490356522, 1310382347,   42962209, 1037764812, 1412492680,
+	1153009459, 1321777575,  321401105,  181238904,  439657024,  724065395,
+	1062573413, 1406096188, 1692623910, 1496432987,  399504174, 1296871934,
+	 947264425, 1731647556, 1689493122,  987552344, 1673420326, 2131557045,
+	1033969938,  380648742, 1925832319, 1972106680,  306289979, 2076977586,
+	2143619481,  397994526,   82971832, 1339218763, 1209446579,  823539991,
+	1095743319,   31300264,  311568081,  308338715, 1973135524, 1738313289,
+	  10618949, 1729293984,  656884918,  963056129, 1831720766, 1724329394,
+	1473929688,  957670019, 2134652762,  855801569,  337911392, 1384522064,
+	 470709300, 1928726766,  681632576, 1515281359,  290264462,  598218076,
+	2075693619,  235431420,  313932494,    5935565, 1900122774, 1839972787,
+	1031482136, 1584403832,  458068360, 1294765291,  199321555,  958074738,
+	 329505910,  390634596,  772851744,  997372950, 1902392406,  911824306,
+	 207470906, 1833988394, 1164634858, 2136949384, 1500489452,  867256506,
+	1043024124,  273804599,  657638509,  519536539, 1551186589,  736063170,
+	1835049310,  274043354,  866615405, 1619589366, 1435976123,  574529072,
+	1988345751, 1513180857, 1589623570, 1950816712, 1786479022, 2073192949,
+	 498899764,   85596569,  926341728,  643726946, 1723803954, 1595790889,
+	1952281981,   15343692, 1963115356,  134007049, 2076842566, 1723273426,
+	 737980662, 1914022676,  179895009,   77496865, 1412978837,  884789007,
+	1508799456, 1575331823,  173928358,  683913421, 1736847062,  684795030,
+	 363617080,  316846537, 1986734606, 1948597245,  463943878,  372293218,
+	 633547358,  874650389,  329434973, 2114012866,  555202488, 1839426393,
+	 407553052, 1953865482, 1125904531, 1399327136,  831584792, 1266486934,
+	 300993160, 1765740623,  438357036,   64650456, 1935125408, 1580266195,
+	1634347599, 1372825220, 1618889826, 1640156223,  425697703, 1885010806,
+	 617651665,  923323310, 2078252329, 1561764248, 1663639491, 1076965730,
+	1582773684,  174737898,  729244234, 1369862069,  658178623,  817132520,
+	1013284789, 1446258571, 1548642213,  577140726,  202288442, 1302550874,
+	   5699694,  143694176, 1957130183, 1011681553,  896045799, 1588814079,
+	 174307020, 2010405410, 1387048505,  188015792,  672940168, 1924982686,
+	1436988104, 1023518611,  982656561,  469334233, 1596517097,  364399787,
+	2039377366, 1076699698, 1417164222, 1759039756, 1140061520, 1634104237,
+	 438437910,   57231296,  716467918,   36877713,  206526271,  765993995,
+	2013631225, 1977195771,  260055940,  179739511,  322258584, 1267265520,
+	 787402577, 2046224021,  620754217, 1563997662,  643098641,  355623087,
+	2022695761, 1286791925,  860977712,  943516276, 1100071272, 1113174801,
+	 653645145, 1660035253, 1245199973,  752358570, 1222551862, 1775430823,
+	  86612216, 2062571833,  322802261, 1235127002, 1817000159, 1400529963,
+	1916686064,  355444581,  764934655, 1112803231, 1521974213, 1015175466,
+	1482152159, 1146712531, 1230387615,  490930367,  519816765,  463925423,
+	 344985544, 1424643552,  880938040, 2135122316, 1756992366, 1926237424,
+	 436148971,  656681218,  652432843, 1811661106,  819600806, 1214027004,
+	 858146988, 1382533297, 1021218996,   22088975,  998028820,   41559673,
+	1792125582, 1089731404, 1219168775, 1276100632,  283839947,  564350328,
+	1218090329,  924017223,  539409057, 1619620887, 1398513793, 1054571004,
+	 895299788,  316520048,   58914203,  577335965,  874424904,  698714656,
+	 591662930, 1071437952,   32201227, 1661530915,  379351956, 1905483518,
+	 193218503, 1996955278, 1271856609,  267890848, 1128126656, 1732468852,
+	1156885260,  496459157,  935069431,  606130843,  828712842, 2140229490,
+	 724670039, 1373305728, 1414778007, 1658679940, 1889912627,  167277719,
+	1462531801, 1401659402, 1929713629, 1738080194, 1705888413, 1201691419,
+	1090717985,   88981489,  209446514,   46869354,  789334152, 1582208828,
+	 964368475, 1539039116,   68376751,  122421185
+};
+
+/*
+ * Convert a polynomial (signed, 16-bit) modulo p, then apply the NTT on it.
+ * The source polynomial should either not overlap with the destination,
+ * or be located in exactly the upper half of the destination array.
+ *
+ * Assumption: logn >= 3
+ */
+TARGET_AVX2
+static inline void
+mp_poly_to_NTT(unsigned logn, uint32_t *d, const int16_t *a,
+	uint32_t p, uint32_t p0i, const uint32_t *gm)
+{
+	size_t n = (size_t)1 << logn;
+#if HAWK_AVX2
+	__m256i yp = _mm256_set1_epi32(p);
+	for (size_t u = 0; u < n; u += 8) {
+		__m128i xa = _mm_loadu_si128((const __m128i *)(a + u));
+		__m256i ya = _mm256_cvtepi16_epi32(xa);
+		ya = _mm256_add_epi32(ya, _mm256_and_si256(yp,
+			_mm256_srai_epi16(ya, 15)));
+		_mm256_storeu_si256((__m256i *)(d + u), ya);
+	}
+#else // HAWK_AVX2
+	for (size_t u = 0; u < n; u ++) {
+		uint32_t x = a[u];
+		d[u] = x + (p & tbmask(x));
+	}
+#endif // HAWK_AVX2
+	mp_NTT(logn, d, gm, p, p0i);
+}
+
+/*
+ * Similar to mp_poly_to_NTT(), but for an auto-adjoint polynomial.
+ * Only the first n/2 elements are written in d[].
+ * The source polynomial should either not overlap with the destination,
+ * or be located in exactly the upper half of the destination array.
+ *
+ * Assumption: logn >= 3
+ */
+TARGET_AVX2
+static inline void
+mp_poly_to_NTT_autoadj(unsigned logn, uint32_t *d, const int16_t *a,
+	uint32_t p, uint32_t p0i, const uint32_t *gm)
+{
+	size_t hn = (size_t)1 << (logn - 1);
+#if HAWK_AVX2
+	__m256i yp = _mm256_set1_epi32(p);
+	for (size_t u = 0; u < hn; u += 8) {
+		__m128i xa = _mm_loadu_si128((const __m128i *)(a + u));
+		__m256i ya = _mm256_cvtepi16_epi32(xa);
+		ya = _mm256_add_epi32(ya, _mm256_and_si256(yp,
+			_mm256_srai_epi16(ya, 15)));
+		_mm256_storeu_si256((__m256i *)(d + u), ya);
+	}
+#else // HAWK_AVX2
+	for (size_t u = 0; u < hn; u ++) {
+		uint32_t x = a[u];
+		d[u] = x + (p & tbmask(x));
+	}
+#endif // HAWK_AVX2
+	mp_NTT_autoadj(logn, d, gm, p, p0i);
+}
+
+/* ==================================================================== */
+/*
+ * Fixed-point FFT.
+ * We use a fixed-mobile point representation, in which values are
+ * represented over 32 bits (signed), and the position of the cutoff
+ * between integral and fractional bits varies along the FFT computation
+ * (but independently of the actual values).
+ */
+
+/*
+ * For k in range(0, 1024):
+ *    FX32_GM[2*k + 0] = round((2^31)*Re(zeta^rev(k)))
+ *    FX32_GM[2*k + 1] = round((2^31)*Im(zeta^rev(k)))
+ * with zeta = exp(2*i*pi/2048) (primitive 2048-th root of 1), and
+ * rev() is the bit-reversal function over 10 bits.
+ * Values for k = 0 and 1 are not used in practice. For all other values,
+ * elements strictly fit in a signed 32-bit representation.
+ */
+static const int32_t FX32_GM[] = {
+  -2147483648,           0,           0, -2147483648,  1518500250,  1518500250,
+  -1518500250,  1518500250,  1984016189,   821806413,  -821806413,  1984016189,
+    821806413,  1984016189, -1984016189,   821806413,  2106220352,   418953276,
+   -418953276,  2106220352,  1193077991,  1785567396, -1785567396,  1193077991,
+   1785567396,  1193077991, -1193077991,  1785567396,   418953276,  2106220352,
+  -2106220352,   418953276,  2137142927,   210490206,  -210490206,  2137142927,
+   1362349204,  1660027308, -1660027308,  1362349204,  1893911494,  1012316784,
+  -1012316784,  1893911494,   623381598,  2055013723, -2055013723,   623381598,
+   2055013723,   623381598,  -623381598,  2055013723,  1012316784,  1893911494,
+  -1893911494,  1012316784,  1660027308,  1362349204, -1362349204,  1660027308,
+    210490206,  2137142927, -2137142927,   210490206,  2144896910,   105372028,
+   -105372028,  2144896910,  1442161874,  1591180426, -1591180426,  1442161874,
+   1941302225,   918167572,  -918167572,  1941302225,   723465451,  2021950484,
+  -2021950484,   723465451,  2083126254,   521795963,  -521795963,  2083126254,
+   1104027237,  1841958164, -1841958164,  1104027237,  1724875040,  1279254516,
+  -1279254516,  1724875040,   315101295,  2124240380, -2124240380,   315101295,
+   2124240380,   315101295,  -315101295,  2124240380,  1279254516,  1724875040,
+  -1724875040,  1279254516,  1841958164,  1104027237, -1104027237,  1841958164,
+    521795963,  2083126254, -2083126254,   521795963,  2021950484,   723465451,
+   -723465451,  2021950484,   918167572,  1941302225, -1941302225,   918167572,
+   1591180426,  1442161874, -1442161874,  1591180426,   105372028,  2144896910,
+  -2144896910,   105372028,  2146836866,    52701887,   -52701887,  2146836866,
+   1480777044,  1555308768, -1555308768,  1480777044,  1963250501,   870249095,
+   -870249095,  1963250501,   772868706,  2003586779, -2003586779,   772868706,
+   2095304370,   470516330,  -470516330,  2095304370,  1148898640,  1814309216,
+  -1814309216,  1148898640,  1755750017,  1236538675, -1236538675,  1755750017,
+    367137861,  2115867626, -2115867626,   367137861,  2131333572,   262874923,
+   -262874923,  2131333572,  1321199781,  1692961062, -1692961062,  1321199781,
+   1868497586,  1058490808, -1058490808,  1868497586,   572761285,  2069693342,
+  -2069693342,   572761285,  2039096241,   673626408,  -673626408,  2039096241,
+    965532978,  1918184581, -1918184581,   965532978,  1626093616,  1402678000,
+  -1402678000,  1626093616,   157978697,  2141664948, -2141664948,   157978697,
+   2141664948,   157978697,  -157978697,  2141664948,  1402678000,  1626093616,
+  -1626093616,  1402678000,  1918184581,   965532978,  -965532978,  1918184581,
+    673626408,  2039096241, -2039096241,   673626408,  2069693342,   572761285,
+   -572761285,  2069693342,  1058490808,  1868497586, -1868497586,  1058490808,
+   1692961062,  1321199781, -1321199781,  1692961062,   262874923,  2131333572,
+  -2131333572,   262874923,  2115867626,   367137861,  -367137861,  2115867626,
+   1236538675,  1755750017, -1755750017,  1236538675,  1814309216,  1148898640,
+  -1148898640,  1814309216,   470516330,  2095304370, -2095304370,   470516330,
+   2003586779,   772868706,  -772868706,  2003586779,   870249095,  1963250501,
+  -1963250501,   870249095,  1555308768,  1480777044, -1480777044,  1555308768,
+     52701887,  2146836866, -2146836866,    52701887,  2147321946,    26352928,
+    -26352928,  2147321946,  1499751576,  1537020244, -1537020244,  1499751576,
+   1973781967,   846091463,  -846091463,  1973781967,   797397602,  1993951625,
+  -1993951625,   797397602,  2100920556,   444768294,  -444768294,  2100920556,
+   1171076495,  1800073849, -1800073849,  1171076495,  1770792044,  1214899813,
+  -1214899813,  1770792044,   393075166,  2111202959, -2111202959,   393075166,
+   2134398966,   236700388,  -236700388,  2134398966,  1341875533,  1676620432,
+  -1676620432,  1341875533,  1881346202,  1035481766, -1035481766,  1881346202,
+    598116479,  2062508835, -2062508835,   598116479,  2047209133,   648552838,
+   -648552838,  2047209133,   988999351,  1906191570, -1906191570,   988999351,
+   1643184191,  1382617710, -1382617710,  1643184191,   184248325,  2139565043,
+  -2139565043,   184248325,  2143442326,   131685278,  -131685278,  2143442326,
+   1422527051,  1608758157, -1608758157,  1422527051,  1929888720,   941921200,
+   -941921200,  1929888720,   698598533,  2030676269, -2030676269,   698598533,
+   2076566160,   547319836,  -547319836,  2076566160,  1081340445,  1855367581,
+  -1855367581,  1081340445,  1709046739,  1300325060, -1300325060,  1709046739,
+    289009871,  2127947206, -2127947206,   289009871,  2120213651,   341145265,
+   -341145265,  2120213651,  1257991320,  1740443581, -1740443581,  1257991320,
+   1828271356,  1126547765, -1126547765,  1828271356,   496193509,  2089372638,
+  -2089372638,   496193509,  2012920201,   748223418,  -748223418,  2012920201,
+    894275671,  1952423377, -1952423377,   894275671,  1573363068,  1461579514,
+  -1461579514,  1573363068,    79042909,  2146028480, -2146028480,    79042909,
+   2146028480,    79042909,   -79042909,  2146028480,  1461579514,  1573363068,
+  -1573363068,  1461579514,  1952423377,   894275671,  -894275671,  1952423377,
+    748223418,  2012920201, -2012920201,   748223418,  2089372638,   496193509,
+   -496193509,  2089372638,  1126547765,  1828271356, -1828271356,  1126547765,
+   1740443581,  1257991320, -1257991320,  1740443581,   341145265,  2120213651,
+  -2120213651,   341145265,  2127947206,   289009871,  -289009871,  2127947206,
+   1300325060,  1709046739, -1709046739,  1300325060,  1855367581,  1081340445,
+  -1081340445,  1855367581,   547319836,  2076566160, -2076566160,   547319836,
+   2030676269,   698598533,  -698598533,  2030676269,   941921200,  1929888720,
+  -1929888720,   941921200,  1608758157,  1422527051, -1422527051,  1608758157,
+    131685278,  2143442326, -2143442326,   131685278,  2139565043,   184248325,
+   -184248325,  2139565043,  1382617710,  1643184191, -1643184191,  1382617710,
+   1906191570,   988999351,  -988999351,  1906191570,   648552838,  2047209133,
+  -2047209133,   648552838,  2062508835,   598116479,  -598116479,  2062508835,
+   1035481766,  1881346202, -1881346202,  1035481766,  1676620432,  1341875533,
+  -1341875533,  1676620432,   236700388,  2134398966, -2134398966,   236700388,
+   2111202959,   393075166,  -393075166,  2111202959,  1214899813,  1770792044,
+  -1770792044,  1214899813,  1800073849,  1171076495, -1171076495,  1800073849,
+    444768294,  2100920556, -2100920556,   444768294,  1993951625,   797397602,
+   -797397602,  1993951625,   846091463,  1973781967, -1973781967,   846091463,
+   1537020244,  1499751576, -1499751576,  1537020244,    26352928,  2147321946,
+  -2147321946,    26352928,  2147443222,    13176712,   -13176712,  2147443222,
+   1509154322,  1527789007, -1527789007,  1509154322,  1978936331,   833964638,
+   -833964638,  1978936331,   809617249,  1989021350, -1989021350,   809617249,
+   2103610054,   431868915,  -431868915,  2103610054,  1182099496,  1792854372,
+  -1792854372,  1182099496,  1778213194,  1204011567, -1204011567,  1778213194,
+    406021865,  2108751352, -2108751352,   406021865,  2135811153,   223599506,
+   -223599506,  2135811153,  1352137822,  1668355276, -1668355276,  1352137822,
+   1887664383,  1023918550, -1023918550,  1887664383,   610760536,  2058800036,
+  -2058800036,   610760536,  2051150040,   635979190,  -635979190,  2051150040,
+   1000676905,  1900087301, -1900087301,  1000676905,  1651636841,  1372509294,
+  -1372509294,  1651636841,   197372981,  2138394240, -2138394240,   197372981,
+   2144209982,   118530885,  -118530885,  2144209982,  1432371426,  1599999411,
+  -1599999411,  1432371426,  1935631910,   930061894,  -930061894,  1935631910,
+    711045377,  2026351522, -2026351522,   711045377,  2079885360,   534567963,
+   -534567963,  2079885360,  1092704411,  1848697674, -1848697674,  1092704411,
+   1716993211,  1289814068, -1289814068,  1716993211,   302061269,  2126133817,
+  -2126133817,   302061269,  2122266967,   328129457,  -328129457,  2122266967,
+   1268646800,  1732691928, -1732691928,  1268646800,  1835149306,  1115308496,
+  -1115308496,  1835149306,   509004318,  2086288720, -2086288720,   509004318,
+   2017473321,   735858287,  -735858287,  2017473321,   906238681,  1946899451,
+  -1946899451,   906238681,  1582301533,  1451898025, -1451898025,  1582301533,
+     92209205,  2145503083, -2145503083,    92209205,  2146473080,    65873638,
+    -65873638,  2146473080,  1471205974,  1564365367, -1564365367,  1471205974,
+   1957873796,   882278992,  -882278992,  1957873796,   760560380,  2008291295,
+  -2008291295,   760560380,  2092377892,   483364019,  -483364019,  2092377892,
+   1137744621,  1821324572, -1821324572,  1137744621,  1748129707,  1247288478,
+  -1247288478,  1748129707,   354148230,  2118080511, -2118080511,   354148230,
+   2129680480,   275947592,  -275947592,  2129680480,  1310787095,  1701035922,
+  -1701035922,  1310787095,  1861967634,  1069935768, -1069935768,  1861967634,
+    560051104,  2073168777, -2073168777,   560051104,  2034924562,   686125387,
+   -686125387,  2034924562,   953745043,  1924072871, -1924072871,   953745043,
+   1617456335,  1412629117, -1412629117,  1617456335,   144834714,  2142593971,
+  -2142593971,   144834714,  2140655293,   171116733,  -171116733,  2140655293,
+   1392674072,  1634669676, -1634669676,  1392674072,  1912224073,   977284562,
+   -977284562,  1912224073,   661102068,  2043191150, -2043191150,   661102068,
+   2066139983,   585449903,  -585449903,  2066139983,  1047005996,  1874957189,
+  -1874957189,  1047005996,  1684822463,  1331562723, -1331562723,  1684822463,
+    249792358,  2132906420, -2132906420,   249792358,  2113575080,   380113669,
+   -380113669,  2113575080,  1225742318,  1763304224, -1763304224,  1225742318,
+   1807225553,  1160009405, -1160009405,  1807225553,   457650927,  2098151960,
+  -2098151960,   457650927,  1998806829,   785147934,  -785147934,  1998806829,
+    858186435,  1968553292, -1968553292,   858186435,  1546193612,  1490292364,
+  -1490292364,  1546193612,    39528151,  2147119825, -2147119825,    39528151,
+   2147119825,    39528151,   -39528151,  2147119825,  1490292364,  1546193612,
+  -1546193612,  1490292364,  1968553292,   858186435,  -858186435,  1968553292,
+    785147934,  1998806829, -1998806829,   785147934,  2098151960,   457650927,
+   -457650927,  2098151960,  1160009405,  1807225553, -1807225553,  1160009405,
+   1763304224,  1225742318, -1225742318,  1763304224,   380113669,  2113575080,
+  -2113575080,   380113669,  2132906420,   249792358,  -249792358,  2132906420,
+   1331562723,  1684822463, -1684822463,  1331562723,  1874957189,  1047005996,
+  -1047005996,  1874957189,   585449903,  2066139983, -2066139983,   585449903,
+   2043191150,   661102068,  -661102068,  2043191150,   977284562,  1912224073,
+  -1912224073,   977284562,  1634669676,  1392674072, -1392674072,  1634669676,
+    171116733,  2140655293, -2140655293,   171116733,  2142593971,   144834714,
+   -144834714,  2142593971,  1412629117,  1617456335, -1617456335,  1412629117,
+   1924072871,   953745043,  -953745043,  1924072871,   686125387,  2034924562,
+  -2034924562,   686125387,  2073168777,   560051104,  -560051104,  2073168777,
+   1069935768,  1861967634, -1861967634,  1069935768,  1701035922,  1310787095,
+  -1310787095,  1701035922,   275947592,  2129680480, -2129680480,   275947592,
+   2118080511,   354148230,  -354148230,  2118080511,  1247288478,  1748129707,
+  -1748129707,  1247288478,  1821324572,  1137744621, -1137744621,  1821324572,
+    483364019,  2092377892, -2092377892,   483364019,  2008291295,   760560380,
+   -760560380,  2008291295,   882278992,  1957873796, -1957873796,   882278992,
+   1564365367,  1471205974, -1471205974,  1564365367,    65873638,  2146473080,
+  -2146473080,    65873638,  2145503083,    92209205,   -92209205,  2145503083,
+   1451898025,  1582301533, -1582301533,  1451898025,  1946899451,   906238681,
+   -906238681,  1946899451,   735858287,  2017473321, -2017473321,   735858287,
+   2086288720,   509004318,  -509004318,  2086288720,  1115308496,  1835149306,
+  -1835149306,  1115308496,  1732691928,  1268646800, -1268646800,  1732691928,
+    328129457,  2122266967, -2122266967,   328129457,  2126133817,   302061269,
+   -302061269,  2126133817,  1289814068,  1716993211, -1716993211,  1289814068,
+   1848697674,  1092704411, -1092704411,  1848697674,   534567963,  2079885360,
+  -2079885360,   534567963,  2026351522,   711045377,  -711045377,  2026351522,
+    930061894,  1935631910, -1935631910,   930061894,  1599999411,  1432371426,
+  -1432371426,  1599999411,   118530885,  2144209982, -2144209982,   118530885,
+   2138394240,   197372981,  -197372981,  2138394240,  1372509294,  1651636841,
+  -1651636841,  1372509294,  1900087301,  1000676905, -1000676905,  1900087301,
+    635979190,  2051150040, -2051150040,   635979190,  2058800036,   610760536,
+   -610760536,  2058800036,  1023918550,  1887664383, -1887664383,  1023918550,
+   1668355276,  1352137822, -1352137822,  1668355276,   223599506,  2135811153,
+  -2135811153,   223599506,  2108751352,   406021865,  -406021865,  2108751352,
+   1204011567,  1778213194, -1778213194,  1204011567,  1792854372,  1182099496,
+  -1182099496,  1792854372,   431868915,  2103610054, -2103610054,   431868915,
+   1989021350,   809617249,  -809617249,  1989021350,   833964638,  1978936331,
+  -1978936331,   833964638,  1527789007,  1509154322, -1509154322,  1527789007,
+     13176712,  2147443222, -2147443222,    13176712,  2147473542,     6588387,
+     -6588387,  2147473542,  1513834411,  1523151797, -1523151797,  1513834411,
+   1981485585,   827889422,  -827889422,  1981485585,   815715670,  1986528118,
+  -1986528118,   815715670,  2104925109,   425413098,  -425413098,  2104925109,
+   1187594332,  1789219305, -1789219305,  1187594332,  1781898681,  1198550419,
+  -1198550419,  1781898681,   412489512,  2107495770, -2107495770,   412489512,
+   2136487095,   217045878,  -217045878,  2136487095,  1357249901,  1664199124,
+  -1664199124,  1357249901,  1890796837,  1018122458, -1018122458,  1890796837,
+    617073971,  2056916560, -2056916560,   617073971,  2053091544,   629683357,
+   -629683357,  2053091544,  1006501581,  1897008325, -1897008325,  1006501581,
+   1655839867,  1367435685, -1367435685,  1655839867,   203932553,  2137778644,
+  -2137778644,   203932553,  2144563539,   111951983,  -111951983,  2144563539,
+   1437273414,  1595597428, -1595597428,  1437273414,  1938476190,   924119082,
+   -924119082,  1938476190,   717258790,  2024160529, -2024160529,   717258790,
+   2081515603,   528184449,  -528184449,  2081515603,  1098370993,  1845336604,
+  -1845336604,  1098370993,  1720942225,  1284540337, -1284540337,  1720942225,
+    308582734,  2125197100, -2125197100,   308582734,  2123263666,   321616889,
+   -321616889,  2123263666,  1273956653,  1728791620, -1728791620,  1273956653,
+   1838562388,  1109673089, -1109673089,  1838562388,   515402566,  2084717298,
+  -2084717298,   515402566,  2019721407,   729665303,  -729665303,  2019721407,
+    912207419,  1944109987, -1944109987,   912207419,  1586748447,  1447036760,
+  -1447036760,  1586748447,    98791081,  2145210092, -2145210092,    98791081,
+   2146665076,    59288042,   -59288042,  2146665076,  1475998456,  1559844408,
+  -1559844408,  1475998456,  1960571375,   876268167,  -876268167,  1960571375,
+    766718151,  2005948478, -2005948478,   766718151,  2093850985,   476942419,
+   -476942419,  2093850985,  1143327011,  1817825449, -1817825449,  1143327011,
+   1751948107,  1241919421, -1241919421,  1751948107,   360644742,  2116984031,
+  -2116984031,   360644742,  2130517052,   269412525,  -269412525,  2130517052,
+   1315999631,  1697006479, -1697006479,  1315999631,  1865241388,  1064218296,
+  -1064218296,  1865241388,   566408860,  2071440808, -2071440808,   566408860,
+   2037019988,   679879097,  -679879097,  2037019988,   959643527,  1921137767,
+  -1921137767,   959643527,  1621782608,  1407660183, -1407660183,  1621782608,
+    151407418,  2142139541, -2142139541,   151407418,  2141170197,   164548489,
+   -164548489,  2141170197,  1397682613,  1630389319, -1630389319,  1397682613,
+   1915213340,   971413342,  -971413342,  1915213340,   667367379,  2041153301,
+  -2041153301,   667367379,  2067926394,   579108320,  -579108320,  2067926394,
+   1052753357,  1871736196, -1871736196,  1052753357,  1688899711,  1326387494,
+  -1326387494,  1688899711,   256334847,  2132130030, -2132130030,   256334847,
+   2114731305,   373627523,  -373627523,  2114731305,  1231146291,  1759535401,
+  -1759535401,  1231146291,  1810775906,  1154459456, -1154459456,  1810775906,
+    464085813,  2096738032, -2096738032,   464085813,  2001206222,   779011986,
+   -779011986,  2001206222,   864221832,  1965911148, -1965911148,   864221832,
+   1550758488,  1485541696, -1485541696,  1550758488,    46115236,  2146988450,
+  -2146988450,    46115236,  2147230991,    32940695,   -32940695,  2147230991,
+   1495029006,  1541614183, -1541614183,  1495029006,  1971176906,   852142959,
+   -852142959,  1971176906,   791276492,  1996388622, -1996388622,   791276492,
+   2099546139,   451211734,  -451211734,  2099546139,  1165548435,  1803658189,
+  -1803658189,  1165548435,  1767056450,  1220326809, -1220326809,  1767056450,
+    386596237,  2112398960, -2112398960,   386596237,  2133662734,   243247518,
+   -243247518,  2133662734,  1336725419,  1680729357, -1680729357,  1336725419,
+   1878160535,  1041248781, -1041248781,  1878160535,   591785976,  2064334124,
+  -2064334124,   591785976,  2045209767,   654830535,  -654830535,  2045209767,
+    983146583,  1909216806, -1909216806,   983146583,  1638934646,  1387652422,
+  -1387652422,  1638934646,   177683365,  2140120240, -2140120240,   177683365,
+   2143028234,   138260647,  -138260647,  2143028234,  1417584755,  1613114838,
+  -1613114838,  1417584755,  1926989864,   947837582,  -947837582,  1926989864,
+    692365218,  2032809982, -2032809982,   692365218,  2074877233,   553688076,
+   -553688076,  2074877233,  1075643169,  1858676355, -1858676355,  1075643169,
+   1705049355,  1305562222, -1305562222,  1705049355,   282480061,  2128823862,
+  -2128823862,   282480061,  2119157054,   347648383,  -347648383,  2119157054,
+   1252645794,  1744294853, -1744294853,  1252645794,  1824806552,  1132151521,
+  -1132151521,  1824806552,   489781069,  2090885105, -2090885105,   489781069,
+   2010615210,   754395449,  -754395449,  2010615210,   888281512,  1955157788,
+  -1955157788,   888281512,  1568871601,  1466399645, -1466399645,  1568871601,
+     72458615,  2146260881, -2146260881,    72458615,  2145775880,    85626460,
+    -85626460,  2145775880,  1456745625,  1577839726, -1577839726,  1456745625,
+   1949670589,   900261413,  -900261413,  1949670589,   742044345,  2015206245,
+  -2015206245,   742044345,  2087840505,   502601279,  -502601279,  2087840505,
+   1120933406,  1831718951, -1831718951,  1120933406,  1736575927,  1263325005,
+  -1263325005,  1736575927,   334638936,  2121250292, -2121250292,   334638936,
+   2127050522,   295536961,  -295536961,  2127050522,  1295075659,  1713028037,
+  -1713028037,  1295075659,  1852041343,  1087027544, -1087027544,  1852041343,
+    540946445,  2078235540, -2078235540,   540946445,  2028523442,   704825272,
+   -704825272,  2028523442,   935995952,  1932769411, -1932769411,   935995952,
+   1604386335,  1427455956, -1427455956,  1604386335,   125108670,  2143836244,
+  -2143836244,   125108670,  2138989708,   190811551,  -190811551,  2138989708,
+   1377569986,  1647418269, -1647418269,  1377569986,  1903148392,   994842810,
+   -994842810,  1903148392,   642269036,  2049189231, -2049189231,   642269036,
+   2060664133,   604441352,  -604441352,  2060664133,  1029705004,  1884514161,
+  -1884514161,  1029705004,  1672495725,  1347013017, -1347013017,  1672495725,
+    230151030,  2135115107, -2135115107,   230151030,  2109987085,   399550396,
+   -399550396,  2109987085,  1209461382,  1774510970, -1774510970,  1209461382,
+   1796472565,  1176593533, -1176593533,  1796472565,   438320667,  2102275199,
+  -2102275199,   438320667,  1991495860,   803511207,  -803511207,  1991495860,
+    840032004,  1976368450, -1976368450,   840032004,  1532411837,  1504460029,
+  -1504460029,  1532411837,    19764913,  2147392690, -2147392690,    19764913,
+   2147392690,    19764913,   -19764913,  2147392690,  1504460029,  1532411837,
+  -1532411837,  1504460029,  1976368450,   840032004,  -840032004,  1976368450,
+    803511207,  1991495860, -1991495860,   803511207,  2102275199,   438320667,
+   -438320667,  2102275199,  1176593533,  1796472565, -1796472565,  1176593533,
+   1774510970,  1209461382, -1209461382,  1774510970,   399550396,  2109987085,
+  -2109987085,   399550396,  2135115107,   230151030,  -230151030,  2135115107,
+   1347013017,  1672495725, -1672495725,  1347013017,  1884514161,  1029705004,
+  -1029705004,  1884514161,   604441352,  2060664133, -2060664133,   604441352,
+   2049189231,   642269036,  -642269036,  2049189231,   994842810,  1903148392,
+  -1903148392,   994842810,  1647418269,  1377569986, -1377569986,  1647418269,
+    190811551,  2138989708, -2138989708,   190811551,  2143836244,   125108670,
+   -125108670,  2143836244,  1427455956,  1604386335, -1604386335,  1427455956,
+   1932769411,   935995952,  -935995952,  1932769411,   704825272,  2028523442,
+  -2028523442,   704825272,  2078235540,   540946445,  -540946445,  2078235540,
+   1087027544,  1852041343, -1852041343,  1087027544,  1713028037,  1295075659,
+  -1295075659,  1713028037,   295536961,  2127050522, -2127050522,   295536961,
+   2121250292,   334638936,  -334638936,  2121250292,  1263325005,  1736575927,
+  -1736575927,  1263325005,  1831718951,  1120933406, -1120933406,  1831718951,
+    502601279,  2087840505, -2087840505,   502601279,  2015206245,   742044345,
+   -742044345,  2015206245,   900261413,  1949670589, -1949670589,   900261413,
+   1577839726,  1456745625, -1456745625,  1577839726,    85626460,  2145775880,
+  -2145775880,    85626460,  2146260881,    72458615,   -72458615,  2146260881,
+   1466399645,  1568871601, -1568871601,  1466399645,  1955157788,   888281512,
+   -888281512,  1955157788,   754395449,  2010615210, -2010615210,   754395449,
+   2090885105,   489781069,  -489781069,  2090885105,  1132151521,  1824806552,
+  -1824806552,  1132151521,  1744294853,  1252645794, -1252645794,  1744294853,
+    347648383,  2119157054, -2119157054,   347648383,  2128823862,   282480061,
+   -282480061,  2128823862,  1305562222,  1705049355, -1705049355,  1305562222,
+   1858676355,  1075643169, -1075643169,  1858676355,   553688076,  2074877233,
+  -2074877233,   553688076,  2032809982,   692365218,  -692365218,  2032809982,
+    947837582,  1926989864, -1926989864,   947837582,  1613114838,  1417584755,
+  -1417584755,  1613114838,   138260647,  2143028234, -2143028234,   138260647,
+   2140120240,   177683365,  -177683365,  2140120240,  1387652422,  1638934646,
+  -1638934646,  1387652422,  1909216806,   983146583,  -983146583,  1909216806,
+    654830535,  2045209767, -2045209767,   654830535,  2064334124,   591785976,
+   -591785976,  2064334124,  1041248781,  1878160535, -1878160535,  1041248781,
+   1680729357,  1336725419, -1336725419,  1680729357,   243247518,  2133662734,
+  -2133662734,   243247518,  2112398960,   386596237,  -386596237,  2112398960,
+   1220326809,  1767056450, -1767056450,  1220326809,  1803658189,  1165548435,
+  -1165548435,  1803658189,   451211734,  2099546139, -2099546139,   451211734,
+   1996388622,   791276492,  -791276492,  1996388622,   852142959,  1971176906,
+  -1971176906,   852142959,  1541614183,  1495029006, -1495029006,  1541614183,
+     32940695,  2147230991, -2147230991,    32940695,  2146988450,    46115236,
+    -46115236,  2146988450,  1485541696,  1550758488, -1550758488,  1485541696,
+   1965911148,   864221832,  -864221832,  1965911148,   779011986,  2001206222,
+  -2001206222,   779011986,  2096738032,   464085813,  -464085813,  2096738032,
+   1154459456,  1810775906, -1810775906,  1154459456,  1759535401,  1231146291,
+  -1231146291,  1759535401,   373627523,  2114731305, -2114731305,   373627523,
+   2132130030,   256334847,  -256334847,  2132130030,  1326387494,  1688899711,
+  -1688899711,  1326387494,  1871736196,  1052753357, -1052753357,  1871736196,
+    579108320,  2067926394, -2067926394,   579108320,  2041153301,   667367379,
+   -667367379,  2041153301,   971413342,  1915213340, -1915213340,   971413342,
+   1630389319,  1397682613, -1397682613,  1630389319,   164548489,  2141170197,
+  -2141170197,   164548489,  2142139541,   151407418,  -151407418,  2142139541,
+   1407660183,  1621782608, -1621782608,  1407660183,  1921137767,   959643527,
+   -959643527,  1921137767,   679879097,  2037019988, -2037019988,   679879097,
+   2071440808,   566408860,  -566408860,  2071440808,  1064218296,  1865241388,
+  -1865241388,  1064218296,  1697006479,  1315999631, -1315999631,  1697006479,
+    269412525,  2130517052, -2130517052,   269412525,  2116984031,   360644742,
+   -360644742,  2116984031,  1241919421,  1751948107, -1751948107,  1241919421,
+   1817825449,  1143327011, -1143327011,  1817825449,   476942419,  2093850985,
+  -2093850985,   476942419,  2005948478,   766718151,  -766718151,  2005948478,
+    876268167,  1960571375, -1960571375,   876268167,  1559844408,  1475998456,
+  -1475998456,  1559844408,    59288042,  2146665076, -2146665076,    59288042,
+   2145210092,    98791081,   -98791081,  2145210092,  1447036760,  1586748447,
+  -1586748447,  1447036760,  1944109987,   912207419,  -912207419,  1944109987,
+    729665303,  2019721407, -2019721407,   729665303,  2084717298,   515402566,
+   -515402566,  2084717298,  1109673089,  1838562388, -1838562388,  1109673089,
+   1728791620,  1273956653, -1273956653,  1728791620,   321616889,  2123263666,
+  -2123263666,   321616889,  2125197100,   308582734,  -308582734,  2125197100,
+   1284540337,  1720942225, -1720942225,  1284540337,  1845336604,  1098370993,
+  -1098370993,  1845336604,   528184449,  2081515603, -2081515603,   528184449,
+   2024160529,   717258790,  -717258790,  2024160529,   924119082,  1938476190,
+  -1938476190,   924119082,  1595597428,  1437273414, -1437273414,  1595597428,
+    111951983,  2144563539, -2144563539,   111951983,  2137778644,   203932553,
+   -203932553,  2137778644,  1367435685,  1655839867, -1655839867,  1367435685,
+   1897008325,  1006501581, -1006501581,  1897008325,   629683357,  2053091544,
+  -2053091544,   629683357,  2056916560,   617073971,  -617073971,  2056916560,
+   1018122458,  1890796837, -1890796837,  1018122458,  1664199124,  1357249901,
+  -1357249901,  1664199124,   217045878,  2136487095, -2136487095,   217045878,
+   2107495770,   412489512,  -412489512,  2107495770,  1198550419,  1781898681,
+  -1781898681,  1198550419,  1789219305,  1187594332, -1187594332,  1789219305,
+    425413098,  2104925109, -2104925109,   425413098,  1986528118,   815715670,
+   -815715670,  1986528118,   827889422,  1981485585, -1981485585,   827889422,
+   1523151797,  1513834411, -1513834411,  1523151797,     6588387,  2147473542,
+  -2147473542,     6588387
+};
+
+#if HAWK_AVX2
+#else // HAWK_AVX2
+static inline uint32_t
+fx32_of(int32_t a, unsigned sh)
+{
+	return *(uint32_t *)&a << sh;
+}
+
+static inline int32_t
+fx32_rint(uint32_t a, unsigned sh)
+{
+	a += (uint32_t)1 << (sh - 1);
+	return *(int32_t *)&a >> sh;
+}
+#endif // HAWK_AVX2
+
+/*
+ * FFT algorithm in bit-reveral order formally works as follows:
+ *
+ *   t = n
+ *   for m = 1; m < n; m *= 2:
+ *       ht = t/2
+ *       for i1 = 0; i1 < m; i1 ++:
+ *           j1 = i1 * t
+ *           s = GM[m + i1]
+ *           for j = j1; j < (j1 + ht); j ++:
+ *               x = f[j]
+ *               y = s * f[j + ht]
+ *               f[j] = x + y
+ *               f[j + ht] = x - y
+ *       t = ht
+ *
+ * GM[k] contains (exp(i*pi/n))^rev(k), with rev() = bit-reversal function
+ * over log(n) bits. All values are complex numbers.
+ *
+ * We work over real polynomials; on input (non-FFT), the imaginary parts
+ * are zero (and implicit). Moreover, GM[1] = exp(i*(pi/n)*(n/2)) = i.
+ * Thus, the first outer iteration (for m = 1) really is:
+ *    for j = 0; j < n/2; j ++:
+ *       x = f[j]
+ *       y = i * f[j + n/2]
+ *       f[j] = x + y
+ *       f[j + ht] = x - y
+ * For a real polynomial f, the FFT representation f' is such that
+ * f'[i + n/2] = conj(f'[i]); thus, we can omit half of the coefficients.
+ * We have n/2 FFT coefficients to store, each being a complex number;
+ * we separate the real and imaginary parts, so that the real part of f[i]
+ * is stored at array index i, while the imaginary part is at index i + n/2.
+ * With these conventions, the first FFT iteration (m = 1) becomes a complete
+ * no-op, we can simply skip it altogether.
+ *
+ * Auto-adjoint polynomials
+ * ------------------------
+ *
+ * An auto-adjoint polynomial is f such that f = adj(f). This implies that:
+ *    for all i > 0, f[i] = -f[n - i]
+ *    FFT(f) contains only real coefficients
+ * For such polynomials, we can use only n/2 words in RAM (encoding the real
+ * parts of the first n/2 FFT coefficients). Intermediate values, however,
+ * are not real, so the FFT itself must still use a full-size array.
+ *
+ * Scaling
+ * -------
+ *
+ * To get better precision, values are upscaled (left-shifted) so that we
+ * get as many fractional bits as possible, without risking an overflow.
+ * At each iteration of the algorithm, the scaling is lowered (i.e. values
+ * are halved) to guarantee against overflows, so that the result is
+ * downscaled by logn-1 (on top of the upscaling). We take that upscaling
+ * into account when performing divisions.
+ */
+
+#if HAWK_AVX2
+/*
+ * Input: yt1_re, yt1_im, yt2_re, yt2_im already aligned (low 32-bit words
+ * of each 64-bit component); yg_re and yg_im contain the gm[] factors.
+ * Output: ya1_re, ya1_im, ya2_re, ya2_im (same alignment).
+ *
+ * Variables yt1_re, yt1_im, yt2_re, yt2_im, ya1_re, ya1_im, ya2_re and ya2_im
+ * are caller-declared.
+ */
+#define FFT_STEP_x8(yg_re, yg_im)   do { \
+		__m256i ytb = _mm256_set1_epi64x((uint64_t)1 << 63); \
+		__m256i yu_re = _mm256_sub_epi64( \
+			_mm256_mul_epi32(yt2_re, yg_re), \
+			_mm256_mul_epi32(yt2_im, yg_im)); \
+		__m256i yu_im = _mm256_add_epi64( \
+			_mm256_mul_epi32(yt2_re, yg_im), \
+			_mm256_mul_epi32(yt2_im, yg_re)); \
+		__m256i yv_re = _mm256_or_si256( \
+			_mm256_andnot_si256(ytb, \
+				_mm256_slli_epi64(yt1_re, 31)), \
+			_mm256_and_si256(ytb, \
+				_mm256_slli_epi64(yt1_re, 32))); \
+		__m256i yv_im = _mm256_or_si256( \
+			_mm256_andnot_si256(ytb, \
+				_mm256_slli_epi64(yt1_im, 31)), \
+			_mm256_and_si256(ytb, \
+				_mm256_slli_epi64(yt1_im, 32))); \
+		ya1_re = _mm256_srli_epi64( \
+			_mm256_add_epi64(yv_re, yu_re), 32); \
+		ya1_im = _mm256_srli_epi64( \
+			_mm256_add_epi64(yv_im, yu_im), 32); \
+		ya2_re = _mm256_srli_epi64( \
+			_mm256_sub_epi64(yv_re, yu_re), 32); \
+		ya2_im = _mm256_srli_epi64( \
+			_mm256_sub_epi64(yv_im, yu_im), 32); \
+	} while (0)
+
+TARGET_AVX2
+static inline void
+fx32_FFT8(__m256i *ya_re, __m256i *ya_im, size_t k)
+{
+	__m256i ya1_re, ya1_im, ya2_re, ya2_im;
+	__m256i yt1_re, yt1_im, yt2_re, yt2_im;
+
+	/* 0/4, 1/5, 2/6, 3/7 with gm[1] */
+	__m256i ypp = _mm256_setr_epi32(0, 4, 1, 5, 2, 6, 3, 7);
+	yt1_re = _mm256_permutevar8x32_epi32(*ya_re, ypp);
+	yt2_re = _mm256_srli_epi64(yt1_re, 32);
+	yt1_im = _mm256_permutevar8x32_epi32(*ya_im, ypp);
+	yt2_im = _mm256_srli_epi64(yt1_im, 32);
+
+	__m256i yg0_re = _mm256_set1_epi64x(((uint64_t *)FX32_GM)[k]);
+	__m256i yg0_im = _mm256_srli_epi64(yg0_re, 32);
+	FFT_STEP_x8(yg0_re, yg0_im);
+
+	/* ya1: 0:-:1:-:2:-:3:-
+	   ya2: 4:-:5:-:6:-:7:- */
+
+	/* 0/2, 1/3 with gm[2]; 4/6, 5/7 with gm[3] */
+	yt1_re = _mm256_permute2x128_si256(ya1_re, ya2_re, 0x20);
+	yt1_im = _mm256_permute2x128_si256(ya1_im, ya2_im, 0x20);
+	yt2_re = _mm256_permute2x128_si256(ya1_re, ya2_re, 0x31);
+	yt2_im = _mm256_permute2x128_si256(ya1_im, ya2_im, 0x31);
+
+	uint64_t g1_0 = ((uint64_t *)FX32_GM)[(k << 1) + 0];
+	uint64_t g1_1 = ((uint64_t *)FX32_GM)[(k << 1) + 1];
+	__m256i yg1_re = _mm256_setr_epi64x(g1_0, g1_0, g1_1, g1_1);
+	__m256i yg1_im = _mm256_srli_epi64(yg1_re, 32);
+	FFT_STEP_x8(yg1_re, yg1_im);
+
+	/* ya1: 0:-:1:-:4:-:5:-
+	   ya2: 2:-:3:-:6:-:7:- */
+
+	/* 0/1, 2/3, 4/5, 6/7 with gm[4..7] */
+	yt1_re = _mm256_unpacklo_epi64(ya1_re, ya2_re);
+	yt1_im = _mm256_unpacklo_epi64(ya1_im, ya2_im);
+	yt2_re = _mm256_unpackhi_epi64(ya1_re, ya2_re);
+	yt2_im = _mm256_unpackhi_epi64(ya1_im, ya2_im);
+
+	__m256i yg2_re = _mm256_loadu_si256(
+		(const __m256i *)((const uint64_t *)FX32_GM + (k << 2)));
+	__m256i yg2_im = _mm256_srli_epi64(yg2_re, 32);
+	FFT_STEP_x8(yg2_re, yg2_im);
+
+	/* ya1: 0:-:2:-:4:-:6:-
+	   ya2: 1:-:3:-:5:-:7:- */
+
+	*ya_re = _mm256_blend_epi32(ya1_re,
+		_mm256_slli_epi64(ya2_re, 32), 0xAA);
+	*ya_im = _mm256_blend_epi32(ya1_im,
+		_mm256_slli_epi64(ya2_im, 32), 0xAA);
+}
+
+/*
+ * Input: yt1_re, yt1_im, yt2_re, yt2_im already aligned (low 32-bit words
+ * of each 64-bit component); yg_re and yg_im contain the gm[] factors.
+ * Output: ya1_re, ya1_im, ya2_re, ya2_im (same alignment).
+ *
+ * Variables yt1_re, yt1_im, yt2_re, yt2_im, ya1_re, ya1_im, ya2_re and ya2_im
+ * are caller-declared.
+ */
+#define iFFT_STEP_x8(yg_re, yg_im)   do { \
+		ya1_re = _mm256_srai_epi32( \
+			_mm256_add_epi32(yt1_re, yt2_re), 1); \
+		ya1_im = _mm256_srai_epi32( \
+			_mm256_add_epi32(yt1_im, yt2_im), 1); \
+		yt1_re = _mm256_sub_epi32(yt1_re, yt2_re); \
+		yt1_im = _mm256_sub_epi32(yt1_im, yt2_im); \
+		__m256i yr0 = _mm256_mul_epi32(yt1_re, yg_re); \
+		__m256i yr1 = _mm256_mul_epi32(yt1_im, yg_im); \
+		__m256i yr2 = _mm256_mul_epi32(yt1_re, yg_im); \
+		__m256i yr3 = _mm256_mul_epi32(yt1_im, yg_re); \
+		ya2_re = _mm256_srli_epi64(_mm256_sub_epi64(yr0, yr1), 32); \
+		ya2_im = _mm256_srli_epi64(_mm256_add_epi64(yr2, yr3), 32); \
+	} while (0)
+
+TARGET_AVX2
+static inline void
+fx32_iFFT8(__m256i *ya_re, __m256i *ya_im, size_t k)
+{
+	__m256i ya1_re, ya1_im, ya2_re, ya2_im;
+	__m256i yt1_re, yt1_im, yt2_re, yt2_im;
+
+	/* 0/1, 2/3, 4/5, 6/7 with gm[4..7] */
+	yt1_re = *ya_re;
+	yt1_im = *ya_im;
+	yt2_re = _mm256_srli_epi64(yt1_re, 32);
+	yt2_im = _mm256_srli_epi64(yt1_im, 32);
+
+	__m256i yg2_re = _mm256_loadu_si256(
+		(const __m256i *)((const uint64_t *)FX32_GM + (k << 2)));
+	__m256i yg2_im = _mm256_sub_epi32(
+		_mm256_setzero_si256(), _mm256_srli_epi64(yg2_re, 32));
+	iFFT_STEP_x8(yg2_re, yg2_im);
+
+	/* ya1: 0:-:2:-:4:-:6:-
+	   ya2: 1:-:3:-:5:-:7:- */
+
+	/* 0/2, 1/3 with gm[2]; 4/6, 5/7 with gm[3] */
+	yt1_re = _mm256_unpacklo_epi64(ya1_re, ya2_re);
+	yt1_im = _mm256_unpacklo_epi64(ya1_im, ya2_im);
+	yt2_re = _mm256_unpackhi_epi64(ya1_re, ya2_re);
+	yt2_im = _mm256_unpackhi_epi64(ya1_im, ya2_im);
+
+	uint64_t g1_0 = ((uint64_t *)FX32_GM)[(k << 1) + 0];
+	uint64_t g1_1 = ((uint64_t *)FX32_GM)[(k << 1) + 1];
+	__m256i yg1_re = _mm256_setr_epi64x(g1_0, g1_0, g1_1, g1_1);
+	__m256i yg1_im = _mm256_sub_epi32(
+		_mm256_setzero_si256(), _mm256_srli_epi64(yg1_re, 32));
+	iFFT_STEP_x8(yg1_re, yg1_im);
+
+	/* ya1: 0:-:1:-:4:-:5:-
+	   ya2: 2:-:3:-:6:-:7:- */
+
+	/* 0/4, 1/5, 2/6, 3/7 with gm[1] */
+	yt1_re = _mm256_permute2x128_si256(ya1_re, ya2_re, 0x20);
+	yt1_im = _mm256_permute2x128_si256(ya1_im, ya2_im, 0x20);
+	yt2_re = _mm256_permute2x128_si256(ya1_re, ya2_re, 0x31);
+	yt2_im = _mm256_permute2x128_si256(ya1_im, ya2_im, 0x31);
+
+	__m256i yg0_re = _mm256_set1_epi64x(((uint64_t *)FX32_GM)[k]);
+	__m256i yg0_im = _mm256_sub_epi32(
+		_mm256_setzero_si256(), _mm256_srli_epi64(yg0_re, 32));
+	iFFT_STEP_x8(yg0_re, yg0_im);
+
+	/* ya1: 0:-:1:-:2:-:3:-
+	   ya2: 4:-:5:-:6:-:7:- */
+
+	__m256i yipp = _mm256_setr_epi32(0, 2, 4, 6, 1, 3, 5, 7);
+	yt1_re = _mm256_blend_epi32(ya1_re,
+		_mm256_slli_epi64(ya2_re, 32), 0xAA);
+	yt1_im = _mm256_blend_epi32(ya1_im,
+		_mm256_slli_epi64(ya2_im, 32), 0xAA);
+	*ya_re = _mm256_permutevar8x32_epi32(yt1_re, yipp);
+	*ya_im = _mm256_permutevar8x32_epi32(yt1_im, yipp);
+}
+#endif // HAWK_AVX2
+
+/*
+ * Apply the FFT on a given real polynomial.
+ *
+ * Assumption: logn >= 3
+ */
+TARGET_AVX2
+static void
+fx32_FFT(unsigned logn, uint32_t *a)
+{
+#if HAWK_AVX2
+	size_t hn = (size_t)1 << (logn - 1);
+	size_t t = hn;
+	__m256i ypc = _mm256_setr_epi32(0, 2, 4, 6, 1, 3, 5, 7);
+	for (unsigned lm = 1; lm < (logn - 3); lm ++) {
+		size_t m = (size_t)1 << lm;
+		size_t ht = t >> 1;
+		size_t j0 = 0;
+		size_t hm = m >> 1;
+		for (size_t i = 0; i < hm; i ++) {
+			__m256i ys_re = _mm256_set1_epi64x(
+				((const uint64_t *)FX32_GM)[i + m]);
+			__m256i ys_im = _mm256_srli_epi64(ys_re, 32);
+			for (size_t j = j0; j < j0 + ht; j += 4) {
+				__m128i xa1_re = _mm_loadu_si128(
+					(__m128i *)(a + j));
+				__m128i xa1_im = _mm_loadu_si128(
+					(__m128i *)(a + j + hn));
+				__m128i xa2_re = _mm_loadu_si128(
+					(__m128i *)(a + j + ht));
+				__m128i xa2_im = _mm_loadu_si128(
+					(__m128i *)(a + j + ht + hn));
+				__m256i yt1_re = _mm256_cvtepi32_epi64(xa1_re);
+				__m256i yt1_im = _mm256_cvtepi32_epi64(xa1_im);
+				__m256i yt2_re = _mm256_cvtepi32_epi64(xa2_re);
+				__m256i yt2_im = _mm256_cvtepi32_epi64(xa2_im);
+				__m256i ya1_re, ya1_im, ya2_re, ya2_im;
+				FFT_STEP_x8(ys_re, ys_im);
+				ya1_re = _mm256_permutevar8x32_epi32(
+					ya1_re, ypc);
+				ya1_im = _mm256_permutevar8x32_epi32(
+					ya1_im, ypc);
+				ya2_re = _mm256_permutevar8x32_epi32(
+					ya2_re, ypc);
+				ya2_im = _mm256_permutevar8x32_epi32(
+					ya2_im, ypc);
+				xa1_re = _mm256_castsi256_si128(ya1_re);
+				xa1_im = _mm256_castsi256_si128(ya1_im);
+				xa2_re = _mm256_castsi256_si128(ya2_re);
+				xa2_im = _mm256_castsi256_si128(ya2_im);
+				_mm_storeu_si128((__m128i *)(a + j),
+					xa1_re);
+				_mm_storeu_si128((__m128i *)(a + j + hn),
+					xa1_im);
+				_mm_storeu_si128((__m128i *)(a + j + ht),
+					xa2_re);
+				_mm_storeu_si128((__m128i *)(a + j + ht + hn),
+					xa2_im);
+			}
+			j0 += t;
+		}
+		t = ht;
+	}
+	size_t m = (size_t)1 << (logn - 3);
+	size_t hm = m >> 1;
+	for (size_t u = 0; u < hm; u ++) {
+		uint32_t *za_re = a + (u << 3);
+		uint32_t *za_im = za_re + hn;
+		__m256i yre = _mm256_loadu_si256((__m256i *)za_re);
+		__m256i yim = _mm256_loadu_si256((__m256i *)za_im);
+		fx32_FFT8(&yre, &yim, u + m);
+		_mm256_storeu_si256((__m256i *)za_re, yre);
+		_mm256_storeu_si256((__m256i *)za_im, yim);
+	}
+#else // HAWK_AVX2
+	size_t hn = (size_t)1 << (logn - 1);
+	size_t t = hn;
+	for (unsigned lm = 1; lm < logn; lm ++) {
+		size_t m = (size_t)1 << lm;
+		size_t ht = t >> 1;
+		size_t j0 = 0;
+		size_t hm = m >> 1;
+		for (size_t i = 0; i < hm; i ++) {
+			uint32_t s_re = (uint32_t)FX32_GM[((i + m) << 1) + 0];
+			uint32_t s_im = (uint32_t)FX32_GM[((i + m) << 1) + 1];
+			for (size_t j = j0; j < j0 + ht; j ++) {
+				uint32_t x1_re = a[j];
+				uint32_t x1_im = a[j + hn];
+				uint32_t x2_re = a[j + ht];
+				uint32_t x2_im = a[j + ht + hn];
+
+#define M(c, d)   ((uint64_t)((int64_t)*(int32_t *)&(c) \
+                   * (int64_t)*(int32_t *)&(d)))
+#define SSX(c)    ((uint64_t)*(int32_t *)&(c) << 31)
+				/*
+				 * t <- s*x2
+				 * (x1, x2) <- ((x1 + t)/2, (x2 - t)/2)
+				 */
+				uint64_t t_re = M(x2_re, s_re) - M(x2_im, s_im);
+				uint64_t t_im = M(x2_re, s_im) + M(x2_im, s_re);
+				a[j]           = (SSX(x1_re) + t_re) >> 32;
+				a[j + hn]      = (SSX(x1_im) + t_im) >> 32;
+				a[j + ht]      = (SSX(x1_re) - t_re) >> 32;
+				a[j + ht + hn] = (SSX(x1_im) - t_im) >> 32;
+#undef M
+#undef SSX
+			}
+			j0 += t;
+		}
+		t = ht;
+	}
+#endif // HAWK_AVX2
+}
+
+/*
+ * Apply the inverse FFT on a given real polynomial.
+ *
+ * Assumption: logn >= 3
+ */
+TARGET_AVX2
+static void
+fx32_iFFT(unsigned logn, uint32_t *a)
+{
+#if HAWK_AVX2
+	size_t hn = (size_t)1 << (logn - 1);
+	size_t m = (size_t)1 << (logn - 3);
+	size_t hm = m >> 1;
+	for (size_t u = 0; u < hm; u ++) {
+		uint32_t *za_re = a + (u << 3);
+		uint32_t *za_im = za_re + hn;
+		__m256i yre = _mm256_loadu_si256((__m256i *)za_re);
+		__m256i yim = _mm256_loadu_si256((__m256i *)za_im);
+		fx32_iFFT8(&yre, &yim, u + m);
+		_mm256_storeu_si256((__m256i *)za_re, yre);
+		_mm256_storeu_si256((__m256i *)za_im, yim);
+	}
+
+	size_t ht = 8;
+	__m256i ypc = _mm256_setr_epi32(0, 2, 4, 6, 1, 3, 5, 7);
+	for (unsigned lm = logn - 4; lm > 0; lm --) {
+		m = (size_t)1 << lm;
+		size_t t = ht << 1;
+		size_t j0 = 0;
+		hm = m >> 1;
+		for (size_t i = 0; i < hm; i ++) {
+			__m256i ys_re = _mm256_set1_epi64x(
+				((const uint64_t *)FX32_GM)[i + m]);
+			__m256i ys_im = _mm256_sub_epi32(_mm256_setzero_si256(),
+				_mm256_srli_epi64(ys_re, 32));
+			for (size_t j = j0; j < j0 + ht; j += 4) {
+				__m128i xa1_re = _mm_loadu_si128(
+					(__m128i *)(a + j));
+				__m128i xa1_im = _mm_loadu_si128(
+					(__m128i *)(a + j + hn));
+				__m128i xa2_re = _mm_loadu_si128(
+					(__m128i *)(a + j + ht));
+				__m128i xa2_im = _mm_loadu_si128(
+					(__m128i *)(a + j + ht + hn));
+				__m256i yt1_re = _mm256_cvtepi32_epi64(xa1_re);
+				__m256i yt1_im = _mm256_cvtepi32_epi64(xa1_im);
+				__m256i yt2_re = _mm256_cvtepi32_epi64(xa2_re);
+				__m256i yt2_im = _mm256_cvtepi32_epi64(xa2_im);
+				__m256i ya1_re, ya1_im, ya2_re, ya2_im;
+				iFFT_STEP_x8(ys_re, ys_im);
+				ya1_re = _mm256_permutevar8x32_epi32(
+					ya1_re, ypc);
+				ya1_im = _mm256_permutevar8x32_epi32(
+					ya1_im, ypc);
+				ya2_re = _mm256_permutevar8x32_epi32(
+					ya2_re, ypc);
+				ya2_im = _mm256_permutevar8x32_epi32(
+					ya2_im, ypc);
+				xa1_re = _mm256_castsi256_si128(ya1_re);
+				xa1_im = _mm256_castsi256_si128(ya1_im);
+				xa2_re = _mm256_castsi256_si128(ya2_re);
+				xa2_im = _mm256_castsi256_si128(ya2_im);
+				_mm_storeu_si128((__m128i *)(a + j),
+					xa1_re);
+				_mm_storeu_si128((__m128i *)(a + j + hn),
+					xa1_im);
+				_mm_storeu_si128((__m128i *)(a + j + ht),
+					xa2_re);
+				_mm_storeu_si128((__m128i *)(a + j + ht + hn),
+					xa2_im);
+			}
+			j0 += t;
+		}
+		ht = t;
+	}
+#else // HAWK_AVX2
+	size_t hn = (size_t)1 << (logn - 1);
+	size_t ht = 1;
+	for (unsigned lm = logn - 1; lm > 0; lm --) {
+		size_t m = (size_t)1 << lm;
+		size_t t = ht << 1;
+		size_t j0 = 0;
+		size_t hm = m >> 1;
+		for (size_t i = 0; i < hm; i ++) {
+			uint32_t s_re = (uint32_t)FX32_GM[((i + m) << 1) + 0];
+			uint32_t s_im = -(uint32_t)FX32_GM[((i + m) << 1) + 1];
+			for (size_t j = j0; j < j0 + ht; j ++) {
+				uint32_t x1_re = a[j];
+				uint32_t x1_im = a[j + hn];
+				uint32_t x2_re = a[j + ht];
+				uint32_t x2_im = a[j + ht + hn];
+
+#define M(c, d)   ((uint64_t)((int64_t)*(int32_t *)&(c) \
+                   * (int64_t)*(int32_t *)&(d)))
+#define H(c)       ((uint32_t)(*(int32_t *)&(c) >> 1))
+				/*
+				 * t1 <- x1 + x2
+				 * t2 <- s*(x1 - x2)
+				 * (x1, x2) <- (t1/2, t2/2)
+				 */
+				uint32_t t1_re = x1_re + x2_re;
+				uint32_t t1_im = x1_im + x2_im;
+				uint32_t t2_re = x1_re - x2_re;
+				uint32_t t2_im = x1_im - x2_im;
+
+				a[j]           = H(t1_re);
+				a[j + hn]      = H(t1_im);
+				a[j + ht]      = (M(t2_re, s_re)
+				                 - M(t2_im, s_im)) >> 32;
+				a[j + ht + hn] = (M(t2_re, s_im)
+				                 + M(t2_im, s_re)) >> 32;
+#undef M
+#undef H
+			}
+			j0 += t;
+		}
+		ht = t;
+	}
+#endif // HAWK_AVX2
+}
+
+/* ==================================================================== */
+
+/*
+ * Limits for q00, q01 and s1 (maximum bit size of the absolute value of
+ * a coefficient, excluding q00[0] and q11[0]).
+ */
+static const int8_t bits_lim00[11] = {
+	0, 0, 0, 0, 0, 0, 0, 0,  9,  9, 10
+};
+static const int8_t bits_lim01[11] = {
+	0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 14
+};
+static const int8_t bits_lims0[11] = {
+	0, 0, 0, 0, 0, 0, 0, 0, 12, 13, 14
+};
+static const int8_t bits_lims1[11] = {
+	0, 0, 0, 0, 0, 0, 0, 0,  9,  9, 10
+};
+
+#if HAWK_AVX2
+
+/*
+ * Decode the variable-sized part of a Golomb-Rice encoded polynomial.
+ * Exactly n = 2^logn values are decoded. Output is d[]. buf/buf_len
+ * designates that variable part. Values written in d[] are scaled by 2^low
+ * (i.e. left-shifted by low bits); if any value is greater than or equal
+ * to 2^lim_bits, then an error is reported.
+ *
+ * Returned value is the actual encoded length (in bytes), with the number of
+ * ignored bits in the last byte written into *num_ignored. On error (value
+ * is out of range, or input buffer is too short), 0 is returned.
+ *
+ * Assumption: low < lim_bits <= low + 4
+ */
+TARGET_AVX2
+static size_t
+decode_gr_vpart(unsigned logn, int16_t *d, const uint8_t *buf, size_t buf_len,
+	int low, int lim_bits, int *num_ignored)
+{
+	size_t n = (size_t)1 << logn;
+	size_t voff = 0;
+
+	/*
+	 * Invariants:
+	 *
+	 *   - acc may contain acc_len bits, in its bottommost indexes. The
+	 *     remaining 64 - acc_len bits are all zero.
+	 *   - acc_len < 8
+	 *   - buffered bits end at a byte boundary
+	 *   - voff points to the next unbuffered byte.
+	 */
+	uint64_t acc = 0;
+	unsigned acc_len = 0;
+	size_t du;
+	__m128i xlim = _mm_set1_epi16((1 << (lim_bits - low)) - 1);
+	__m128i xlow = _mm_cvtsi32_si128(low);
+	for (du = 0; du < n; du += 8) {
+		if (voff + 8 > buf_len) {
+			break;
+		}
+
+#if defined __x86_64__ || defined _M_X64
+		/*
+		 * Ensure that we have at least 56 bits.
+		 */
+		uint64_t x = *(uint64_t *)(buf + voff);
+		acc |= x << acc_len;
+		if (acc == 0) {
+			return 0;
+		}
+
+		/*
+		 * Fast path: assume that the next 8 values will fit in
+		 * 48+acc_len bits. We put 8 guard bits to ensure that
+		 * all shift counts will be lower than 64.
+		 */
+		x = acc | (uint64_t)0xFF00000000000000;
+		unsigned k0 = _tzcnt_u64(x);
+		x >>= k0 + 1;
+		unsigned k1 = _tzcnt_u64(x);
+		x >>= k1 + 1;
+		unsigned k2 = _tzcnt_u64(x);
+		x >>= k2 + 1;
+		unsigned k3 = _tzcnt_u64(x);
+		x >>= k3 + 1;
+		unsigned k4 = _tzcnt_u64(x);
+		x >>= k4 + 1;
+		unsigned k5 = _tzcnt_u64(x);
+		x >>= k5 + 1;
+		unsigned k6 = _tzcnt_u64(x);
+		x >>= k6 + 1;
+		unsigned k7 = _tzcnt_u64(x);
+#else
+		/*
+		 * The 32-bit version only has access to _tzcnt_u32().
+		 */
+		uint32_t x0 = *(uint32_t *)(buf + voff);
+		uint32_t x1 = *(uint32_t *)(buf + voff + 4);
+		uint64_t x = ((uint64_t)x1 << 32) | (uint64_t)x0;
+		acc |= x << acc_len;
+		if (acc == 0) {
+			return 0;
+		}
+
+		x = acc;
+
+#define GR_STEP(kk)   do { \
+		uint32_t x_lo = (uint32_t)x; \
+		if (x_lo == 0) { \
+			return 0; \
+		} \
+		(kk) = _tzcnt_u32(x_lo); \
+		x >>= (kk) + 1; \
+	} while (0)
+
+		unsigned k0, k1, k2, k3, k4, k5, k6, k7;
+		GR_STEP(k0);
+		GR_STEP(k1);
+		GR_STEP(k2);
+		GR_STEP(k3);
+		GR_STEP(k4);
+		GR_STEP(k5);
+		GR_STEP(k6);
+		GR_STEP(k7);
+
+#undef GR_STEP
+
+#endif
+
+		/*
+		 * Compute the number of actually consumed bits.
+		 */
+		unsigned lgb = 8 + k0 + k1 + k2 + k3 + k4 + k5 + k6 + k7;
+		if (lgb <= 48 + acc_len) {
+			/*
+			 * We consumed exactly lgb bits from acc. We
+			 * update acc_len and voff accordingly, keeping
+			 * at most 7 bits. Since acc_len < 8, and we
+			 * consumed at least 8 bits, we know that
+			 * lgb > acc_len. We need to reach the next byte
+			 * boundary.
+			 */
+			acc >>= lgb;
+			unsigned lgn = (lgb - acc_len + 7) & ~7u;
+			voff += lgn >> 3;
+			acc_len += lgn - lgb;
+			acc &= ~((uint64_t)-1 << acc_len);
+
+			/*
+			 * Assemble the 8 values; also check that they are
+			 * all in the allowed range.
+			 */
+			__m128i xd = _mm_setr_epi16(
+				k0, k1, k2, k3, k4, k5, k6, k7);
+			if (_mm_movemask_epi8(_mm_cmpgt_epi16(xd, xlim)) != 0) {
+				return 0;
+			}
+			xd = _mm_sll_epi16(xd, xlow);
+			_mm_storeu_si128((__m128i *)(d + du), xd);
+			continue;
+		}
+
+		/*
+		 * Slow path: we process values and bytes one
+		 * by one. This branch is rarely taken.
+		 */
+		acc &= ~((uint64_t)-1 << acc_len);
+		for (size_t v = 0; v < 8; v ++) {
+			while (acc == 0) {
+				if (acc_len > 15) {
+					return 0;
+				}
+				if (voff >= buf_len) {
+					return 0;
+				}
+				acc |= (uint64_t)buf[voff ++] << acc_len;
+				acc_len += 8;
+			}
+#if defined __x86_64__ || defined _M_X64
+			unsigned k = _tzcnt_u64(acc);
+#else
+			unsigned k = _tzcnt_u32((uint32_t)acc);
+#endif
+			if (k > 15) {
+				return 0;
+			}
+			acc >>= k + 1;
+			acc_len -= k + 1;
+			d[du + v] = k << low;
+		}
+	}
+
+	/*
+	 * Final values: we do not have enough source bytes to keep reading
+	 * with 8 bytes of look-ahead, so we handle bytes one by one.
+	 */
+	while (du < n) {
+		while (acc == 0) {
+			if (acc_len > 15) {
+				return 0;
+			}
+			if (voff >= buf_len) {
+				return 0;
+			}
+			acc |= (uint64_t)buf[voff ++] << acc_len;
+			acc_len += 8;
+		}
+#if defined __x86_64__ || defined _M_X64
+		unsigned k = _tzcnt_u64(acc);
+#else
+		unsigned k = _tzcnt_u32((uint32_t)acc);
+#endif
+		if (k > 15) {
+			return 0;
+		}
+		acc >>= k + 1;
+		acc_len -= k + 1;
+		d[du ++] = k << low;
+	}
+
+	if (num_ignored != NULL) {
+		*num_ignored = (int)acc_len;
+	}
+	return voff;
+}
+
+TARGET_AVX2
+static size_t
+decode_gr_5_9(unsigned logn, int16_t *d, const uint8_t *buf, size_t buf_len,
+	int *num_ignored)
+{
+	const int low = 5;
+	const int lim_bits = 9;
+	size_t n = (size_t)1 << logn;
+
+	/*
+	 * At least low+2 bits per input.
+	 */
+	if (buf_len < (size_t)(low + 2) << (logn - 3)) {
+		return 0;
+	}
+
+	/*
+	 * Decode variable parts first.
+	 */
+	size_t voff = (size_t)(low + 1) << (logn - 3);
+	size_t vlen = decode_gr_vpart(logn, d, buf + voff, buf_len - voff,
+		low, lim_bits, num_ignored);
+	if (vlen == 0) {
+		return 0;
+	}
+	voff += vlen;
+
+	/*
+	 * Fixed-size elements.
+	 */
+	size_t loff = (size_t)1 << (logn - 3);
+	__m256i ybs = _mm256_setr_epi32(15, 13, 11, 9, 7, 5, 3, 1);
+	__m128i xsf1 = _mm_setr_epi8(
+		0, 1, 2, 3, 4, -1, -1, -1,
+		5, 6, 7, 8, 9, -1, -1, -1);
+	__m128i xm1 = _mm_setr_epi8(
+		0x00, 0x00, 0xF0, 0xFF, 0xFF, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0xF0, 0xFF, 0xFF, 0x00, 0x00, 0x00);
+	__m128i xm2 = _mm_setr_epi8(
+		0x00, 0xFC, 0x0F, 0x00, 0x00, 0xFC, 0x0F, 0x00,
+		0x00, 0xFC, 0x0F, 0x00, 0x00, 0xFC, 0x0F, 0x00);
+	__m128i xm3 = _mm_setr_epi8(
+		0xE0, 0x03, 0xE0, 0x03, 0xE0, 0x03, 0xE0, 0x03,
+		0xE0, 0x03, 0xE0, 0x03, 0xE0, 0x03, 0xE0, 0x03);
+	for (size_t u = 0; u < n; u += 16) {
+		uint32_t wbb = *(uint16_t *)(buf + (u >> 3));
+		wbb |= (wbb >> 1) << 16;
+		__m256i ybb = _mm256_sllv_epi32(_mm256_set1_epi32(wbb), ybs);
+
+		__m128i xlp = _mm_loadu_si128((__m128i *)(buf + loff));
+		loff += 10;
+		xlp = _mm_shuffle_epi8(xlp, xsf1);
+		xlp = _mm_or_si128(
+			_mm_andnot_si128(xm1, xlp),
+			_mm_slli_epi64(_mm_and_si128(xm1, xlp), 12));
+		xlp = _mm_or_si128(
+			_mm_andnot_si128(xm2, xlp),
+			_mm_slli_epi64(_mm_and_si128(xm2, xlp), 6));
+		xlp = _mm_or_si128(
+			_mm_andnot_si128(xm3, xlp),
+			_mm_slli_epi64(_mm_and_si128(xm3, xlp), 3));
+		__m256i ylp = _mm256_cvtepu8_epi16(xlp);
+
+		ylp = _mm256_xor_si256(_mm256_srai_epi16(ybb, 15), ylp);
+		__m256i yd = _mm256_loadu_si256((__m256i *)(d + u));
+		yd = _mm256_xor_si256(yd, ylp);
+		_mm256_storeu_si256((__m256i *)(d + u), yd);
+	}
+
+	return voff;
+}
+
+TARGET_AVX2
+static size_t
+decode_gr_6_10(unsigned logn, int16_t *d, const uint8_t *buf, size_t buf_len,
+	int *num_ignored)
+{
+	const int low = 6;
+	const int lim_bits = 10;
+	size_t n = (size_t)1 << logn;
+
+	/*
+	 * At least low+2 bits per input.
+	 */
+	if (buf_len < (size_t)(low + 2) << (logn - 3)) {
+		return 0;
+	}
+
+	/*
+	 * Decode variable parts first.
+	 */
+	size_t voff = (size_t)(low + 1) << (logn - 3);
+	size_t vlen = decode_gr_vpart(logn, d, buf + voff, buf_len - voff,
+		low, lim_bits, num_ignored);
+	if (vlen == 0) {
+		return 0;
+	}
+	voff += vlen;
+
+	/*
+	 * Fixed-size elements.
+	 */
+	size_t loff = (size_t)1 << (logn - 3);
+	__m256i ybs = _mm256_setr_epi32(15, 13, 11, 9, 7, 5, 3, 1);
+	__m128i xsf1 = _mm_setr_epi8(
+		0, 1, 2, -1, 3, 4, 5, -1,
+		6, 7, 8, -1, 9, 10, 11, -1);
+	__m128i xm1 = _mm_setr_epi8(
+		0x00, 0xF0, 0xFF, 0x00, 0x00, 0xF0, 0xFF, 0x00,
+		0x00, 0xF0, 0xFF, 0x00, 0x00, 0xF0, 0xFF, 0x00);
+	__m128i xm2 = _mm_setr_epi8(
+		0xC0, 0x0F, 0xC0, 0x0F, 0xC0, 0x0F, 0xC0, 0x0F,
+		0xC0, 0x0F, 0xC0, 0x0F, 0xC0, 0x0F, 0xC0, 0x0F);
+	for (size_t u = 0; u < n; u += 16) {
+		uint32_t wbb = *(uint16_t *)(buf + (u >> 3));
+		wbb |= (wbb >> 1) << 16;
+		__m256i ybb = _mm256_sllv_epi32(_mm256_set1_epi32(wbb), ybs);
+
+		__m128i xlp = _mm_loadu_si128((__m128i *)(buf + loff));
+		loff += 12;
+		xlp = _mm_shuffle_epi8(xlp, xsf1);
+		xlp = _mm_or_si128(
+			_mm_andnot_si128(xm1, xlp),
+			_mm_slli_epi64(_mm_and_si128(xm1, xlp), 4));
+		xlp = _mm_or_si128(
+			_mm_andnot_si128(xm2, xlp),
+			_mm_slli_epi64(_mm_and_si128(xm2, xlp), 2));
+		__m256i ylp = _mm256_cvtepu8_epi16(xlp);
+
+		ylp = _mm256_xor_si256(_mm256_srai_epi16(ybb, 15), ylp);
+		__m256i yd = _mm256_loadu_si256((__m256i *)(d + u));
+		yd = _mm256_xor_si256(yd, ylp);
+		_mm256_storeu_si256((__m256i *)(d + u), yd);
+	}
+
+	return voff;
+}
+
+TARGET_AVX2
+static size_t
+decode_gr_8_11(unsigned logn, int16_t *d, const uint8_t *buf, size_t buf_len,
+	int *num_ignored)
+{
+	const int low = 8;
+	const int lim_bits = 11;
+	size_t n = (size_t)1 << logn;
+
+	/*
+	 * At least low+2 bits per input.
+	 */
+	if (buf_len < (size_t)(low + 2) << (logn - 3)) {
+		return 0;
+	}
+
+	/*
+	 * Decode variable parts first.
+	 */
+	size_t voff = (size_t)(low + 1) << (logn - 3);
+	size_t vlen = decode_gr_vpart(logn, d, buf + voff, buf_len - voff,
+		low, lim_bits, num_ignored);
+	if (vlen == 0) {
+		return 0;
+	}
+	voff += vlen;
+
+	/*
+	 * Fixed-size elements.
+	 */
+	size_t loff = (size_t)1 << (logn - 3);
+	__m256i ybs = _mm256_setr_epi32(15, 13, 11, 9, 7, 5, 3, 1);
+	for (size_t u = 0; u < n; u += 16) {
+		uint32_t wbb = *(uint16_t *)(buf + (u >> 3));
+		wbb |= (wbb >> 1) << 16;
+		__m256i ybb = _mm256_sllv_epi32(_mm256_set1_epi32(wbb), ybs);
+
+		__m128i xlp = _mm_loadu_si128((__m128i *)(buf + loff + u));
+		__m256i ylp = _mm256_cvtepu8_epi16(xlp);
+
+		ylp = _mm256_xor_si256(_mm256_srai_epi16(ybb, 15), ylp);
+		__m256i yd = _mm256_loadu_si256((__m256i *)(d + u));
+		yd = _mm256_xor_si256(yd, ylp);
+		_mm256_storeu_si256((__m256i *)(d + u), yd);
+	}
+
+	return voff;
+}
+
+TARGET_AVX2
+static size_t
+decode_gr_9_12(unsigned logn, int16_t *d, const uint8_t *buf, size_t buf_len,
+	int *num_ignored)
+{
+	const int low = 9;
+	const int lim_bits = 12;
+	size_t n = (size_t)1 << logn;
+
+	/*
+	 * At least low+2 bits per input.
+	 */
+	if (buf_len < (size_t)(low + 2) << (logn - 3)) {
+		return 0;
+	}
+
+	/*
+	 * Decode variable parts first.
+	 */
+	size_t voff = (size_t)(low + 1) << (logn - 3);
+	size_t vlen = decode_gr_vpart(logn, d, buf + voff, buf_len - voff,
+		low, lim_bits, num_ignored);
+	if (vlen == 0) {
+		return 0;
+	}
+	voff += vlen;
+
+	/*
+	 * Fixed-size elements.
+	 */
+	size_t loff = (size_t)1 << (logn - 3);
+	__m256i ybs = _mm256_setr_epi32(15, 13, 11, 9, 7, 5, 3, 1);
+	__m256i ysf1 = _mm256_setr_epi8(
+		0, 1, 2, 3, 4, -1, -1, -1,
+		4, 5, 6, 7, 8, -1, -1, -1,
+		0, 1, 2, 3, 4, -1, -1, -1,
+		4, 5, 6, 7, 8, -1, -1, -1);
+	__m256i ysh4h = _mm256_setr_epi64x(0, 4, 0, 4);
+	__m256i ym0 = _mm256_setr_epi8(
+		0xFF, 0xFF, 0xFF, 0xFF, 0x0F, 0x00, 0x00, 0x00,
+		0xFF, 0xFF, 0xFF, 0xFF, 0x0F, 0x00, 0x00, 0x00,
+		0xFF, 0xFF, 0xFF, 0xFF, 0x0F, 0x00, 0x00, 0x00,
+		0xFF, 0xFF, 0xFF, 0xFF, 0x0F, 0x00, 0x00, 0x00);
+	__m256i ym1 = _mm256_setr_epi8(
+		0x00, 0x00, 0xFC, 0xFF, 0x0F, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0xFC, 0xFF, 0x0F, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0xFC, 0xFF, 0x0F, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0xFC, 0xFF, 0x0F, 0x00, 0x00, 0x00);
+	__m256i ym2 = _mm256_setr_epi8(
+		0x00, 0xFE, 0x03, 0x00, 0x00, 0xFE, 0x03, 0x00,
+		0x00, 0xFE, 0x03, 0x00, 0x00, 0xFE, 0x03, 0x00,
+		0x00, 0xFE, 0x03, 0x00, 0x00, 0xFE, 0x03, 0x00,
+		0x00, 0xFE, 0x03, 0x00, 0x00, 0xFE, 0x03, 0x00);
+	for (size_t u = 0; u < n; u += 16) {
+		uint32_t wbb = *(uint16_t *)(buf + (u >> 3));
+		wbb |= (wbb >> 1) << 16;
+		__m256i ybb = _mm256_sllv_epi32(_mm256_set1_epi32(wbb), ybs);
+
+		__m128i xlp0 = _mm_loadu_si128((__m128i *)(buf + loff));
+		__m128i xlp1 = _mm_loadu_si128((__m128i *)(buf + loff + 9));
+		loff += 18;
+		__m256i ylp = _mm256_setr_m128i(xlp0, xlp1);
+		ylp = _mm256_shuffle_epi8(ylp, ysf1);
+		ylp = _mm256_srlv_epi64(ylp, ysh4h);
+
+		ylp = _mm256_and_si256(ylp, ym0);
+		ylp = _mm256_or_si256(
+			_mm256_andnot_si256(ym1, ylp),
+			_mm256_slli_epi64(_mm256_and_si256(ym1, ylp), 14));
+		ylp = _mm256_or_si256(
+			_mm256_andnot_si256(ym2, ylp),
+			_mm256_slli_epi64(_mm256_and_si256(ym2, ylp), 7));
+
+		ylp = _mm256_xor_si256(_mm256_srai_epi16(ybb, 15), ylp);
+		__m256i yd = _mm256_loadu_si256((__m256i *)(d + u));
+		yd = _mm256_xor_si256(yd, ylp);
+		_mm256_storeu_si256((__m256i *)(d + u), yd);
+	}
+
+	return voff;
+}
+
+TARGET_AVX2
+static size_t
+decode_gr_10_14(unsigned logn, int16_t *d, const uint8_t *buf, size_t buf_len,
+	int *num_ignored)
+{
+	const int low = 10;
+	const int lim_bits = 14;
+	size_t n = (size_t)1 << logn;
+
+	/*
+	 * At least low+2 bits per input.
+	 */
+	if (buf_len < (size_t)(low + 2) << (logn - 3)) {
+		return 0;
+	}
+
+	/*
+	 * Decode variable parts first.
+	 */
+	size_t voff = (size_t)(low + 1) << (logn - 3);
+	size_t vlen = decode_gr_vpart(logn, d, buf + voff, buf_len - voff,
+		low, lim_bits, num_ignored);
+	if (vlen == 0) {
+		return 0;
+	}
+	voff += vlen;
+
+	/*
+	 * Fixed-size elements.
+	 */
+	size_t loff = (size_t)1 << (logn - 3);
+	__m256i ybs = _mm256_setr_epi32(15, 13, 11, 9, 7, 5, 3, 1);
+
+	__m256i ysf1 = _mm256_setr_epi8(
+		0, 1, 2, 3, 4, -1, -1, -1,
+		5, 6, 7, 8, 9, -1, -1, -1,
+		0, 1, 2, 3, 4, -1, -1, -1,
+		5, 6, 7, 8, 9, -1, -1, -1);
+	__m256i ym1 = _mm256_setr_epi8(
+		0x00, 0x00, 0xF0, 0xFF, 0xFF, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0xF0, 0xFF, 0xFF, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0xF0, 0xFF, 0xFF, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0xF0, 0xFF, 0xFF, 0x00, 0x00, 0x00);
+	__m256i ym2 = _mm256_setr_epi8(
+		0x00, 0xFC, 0x0F, 0x00, 0x00, 0xFC, 0x0F, 0x00,
+		0x00, 0xFC, 0x0F, 0x00, 0x00, 0xFC, 0x0F, 0x00,
+		0x00, 0xFC, 0x0F, 0x00, 0x00, 0xFC, 0x0F, 0x00,
+		0x00, 0xFC, 0x0F, 0x00, 0x00, 0xFC, 0x0F, 0x00);
+
+	for (size_t u = 0; u < n; u += 16) {
+		uint32_t wbb = *(uint16_t *)(buf + (u >> 3));
+		wbb |= (wbb >> 1) << 16;
+		__m256i ybb = _mm256_sllv_epi32(_mm256_set1_epi32(wbb), ybs);
+
+		__m128i xlp0 = _mm_loadu_si128((__m128i *)(buf + loff));
+		__m128i xlp1 = _mm_loadu_si128((__m128i *)(buf + loff + 10));
+		loff += 20;
+		__m256i ylp = _mm256_set_m128i(xlp1, xlp0);
+		ylp = _mm256_shuffle_epi8(ylp, ysf1);
+		ylp = _mm256_or_si256(
+			_mm256_andnot_si256(ym1, ylp),
+			_mm256_slli_epi64(_mm256_and_si256(ym1, ylp), 12));
+		ylp = _mm256_or_si256(
+			_mm256_andnot_si256(ym2, ylp),
+			_mm256_slli_epi64(_mm256_and_si256(ym2, ylp), 6));
+
+		ylp = _mm256_xor_si256(_mm256_srai_epi16(ybb, 15), ylp);
+		__m256i yd = _mm256_loadu_si256((__m256i *)(d + u));
+		yd = _mm256_xor_si256(yd, ylp);
+		_mm256_storeu_si256((__m256i *)(d + u), yd);
+	}
+
+	return voff;
+}
+
+#else // HAWK_AVX2
+
+/*
+ * Decode n = 2^logn values into d from a provided buffer, using
+ * Golomb-Rice coding. The sign, fixed part ('low' bits) and
+ * variable part (high bits) are segregated.
+ *
+ * Values are verified to be such that -2^lim_bits <= d[i] < +2^lim_bits.
+ * The number of ignored bits in the last byte is written into *num_ignored
+ * (unless num_ignored == NULL). Total size (in bytes) is returned.
+ *
+ * ASSUMPTION: lim_bits <= low + 4
+ */
+static size_t
+decode_gr(unsigned logn, int16_t *d, const uint8_t *buf, size_t buf_len,
+	int low, int lim_bits, int *num_ignored)
+{
+	size_t n = (size_t)1 << logn;
+	if (buf_len < ((uint32_t)(low + 1) << (logn - 3))) {
+		return 0;
+	}
+	size_t voff = (size_t)(low + 1) << (logn - 3);
+
+	/*
+	 * Precomputed table for the number of trailing zeros in an
+	 * 8-bit value.
+	 */
+	static const uint8_t ntz[256] = {
+		8, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		5, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		6, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		5, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		7, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		5, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		6, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		5, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
+		4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0
+	};
+
+	/*
+	 * Variable part.
+	 */
+	uint32_t acc = 0;
+	int acc_off = 0;
+	int lim_hi = 1 << (lim_bits - low);
+	for (size_t u = 0; u < n; u ++) {
+		while (acc == 0) {
+			if (acc_off >= lim_hi) {
+				return 0;
+			}
+			if (voff >= buf_len) {
+				return 0;
+			}
+			acc |= (uint32_t)buf[voff ++] << acc_off;
+			acc_off += 8;
+		}
+		int k = ntz[acc & 0xFF];
+		if (k == 8) {
+			k += ntz[(acc >> 8) & 0xFF];
+			if (k >= lim_hi) {
+				return 0;
+			}
+		}
+		d[u] = k << low;
+		acc >>= k + 1;
+		acc_off -= k + 1;
+	}
+	if (num_ignored != NULL) {
+		*num_ignored = acc_off;
+	}
+
+	/*
+	 * Sign bits and fixed-width parts.
+	 */
+	size_t loff = (size_t)1 << (logn - 3);
+	uint32_t lmask = ((uint32_t)1 << low) - 1;
+	if (low <= 8) {
+		for (size_t u = 0; u < n; u += 8) {
+			uint32_t sbb = buf[u >> 3];
+			uint64_t lpp = 0;
+			for (int j = 0; j < (low << 3); j += 8) {
+				lpp |= (uint64_t)buf[loff ++] << j;
+			}
+			for (int i = 0, j = 0; i < 8; i ++, j += low) {
+				uint32_t lp = (uint32_t)(lpp >> j) & lmask;
+				uint32_t sm = -((sbb >> i) & 1);
+				((uint16_t *)d)[u + (size_t)i] ^= sm ^ lp;
+			}
+		}
+	} else {
+		for (size_t u = 0; u < n; u += 8) {
+			uint32_t sbb = buf[u >> 3];
+			uint32_t lpp0 = dec32le(buf + loff);
+			loff += 4;
+			uint64_t lpp1 = 0;
+			for (int j = 4, k = 0; j < low; j ++, k += 8) {
+				lpp1 |= (uint64_t)buf[loff ++] << k;
+			}
+			for (int i = 0, j = 0; i < 3; i ++, j += low) {
+				uint32_t lp = (lpp0 >> j) & lmask;
+				uint32_t sm = -((sbb >> i) & 1);
+				((uint16_t *)d)[u + (size_t)i] ^= sm ^ lp;
+			}
+			lpp1 = (lpp1 << (32 - 3 * low))
+				| (uint64_t)(lpp0 >> (3 * low));
+			for (int i = 3, j = 0; i < 8; i ++, j += low) {
+				uint32_t lp = (uint32_t)(lpp1 >> j) & lmask;
+				uint32_t sm = -((sbb >> i) & 1);
+				((uint16_t *)d)[u + (size_t)i] ^= sm ^ lp;
+			}
+		}
+	}
+
+	return voff;
+}
+
+#define decode_gr_5_9(logn, d, buf, buf_len, pni) \
+	decode_gr(logn, d, buf, buf_len, 5, 9, pni)
+#define decode_gr_6_10(logn, d, buf, buf_len, pni) \
+	decode_gr(logn, d, buf, buf_len, 6, 10, pni)
+#define decode_gr_8_11(logn, d, buf, buf_len, pni) \
+	decode_gr(logn, d, buf, buf_len, 8, 11, pni)
+#define decode_gr_9_12(logn, d, buf, buf_len, pni) \
+	decode_gr(logn, d, buf, buf_len, 9, 12, pni)
+#define decode_gr_10_14(logn, d, buf, buf_len, pni) \
+	decode_gr(logn, d, buf, buf_len, 10, 14, pni)
+
+#endif // HAWK_AVX2
+
+/*
+ * Decode q00 from the source public key. The provided 'buf' must point to
+ * the first byte of the encoding of q00. Output values are written into d[].
+ *
+ * Note: q00 is auto-adjoint so only 2^(logn-1) values are decoded and
+ * returned.
+ *
+ * Returned value is the encoded length of q00, in bytes.
+ */
+static size_t
+decode_q00(unsigned logn, int16_t *q00, const uint8_t *buf, size_t buf_len)
+{
+	int lim_len;
+	int ni;
+	size_t len;
+	switch (logn) {
+	case 8:
+		lim_len = 9;
+		len = decode_gr_5_9(logn - 1, q00, buf, buf_len, &ni);
+		break;
+	case 9:
+		lim_len = 9;
+		len = decode_gr_5_9(logn - 1, q00, buf, buf_len, &ni);
+		break;
+	default:
+		lim_len = 10;
+		len = decode_gr_6_10(logn - 1, q00, buf, buf_len, &ni);
+		break;
+	}
+	if (len == 0) {
+		return 0;
+	}
+
+	/*
+	 * q00[0] has a special encoding: decoded value was scaled down,
+	 * and must be scaled up again.
+	 */
+	int eb00_len = 16 - lim_len;
+	int32_t eb00;
+	unsigned last;
+	if (eb00_len <= ni) {
+		last = (unsigned)buf[len - 1] >> (8 - ni);
+		eb00 = last;
+		last >>= eb00_len;
+	} else {
+		if (len >= buf_len) {
+			return 0;
+		}
+		eb00 = (unsigned)buf[len - 1] >> (8 - ni);
+		last = buf[len ++];
+		eb00 |= last << ni;
+		last >>= eb00_len - ni;
+	}
+	if (last != 0) {
+		return 0;
+	}
+	q00[0] = (q00[0] << eb00_len) | eb00;
+	return len;
+}
+
+/*
+ * Like decode_q00(), but for q01. n values are produced. This function
+ * expects to receive the current buf / buf_len values. It reports an
+ * error if the last unused bits of the last byte are not zero.
+ *
+ * Returned value: actual encoded length (in bytes), or 0 on error.
+ */
+static size_t
+decode_q01(unsigned logn, int16_t *q01, const uint8_t *buf, size_t buf_len)
+{
+	int ni;
+	size_t len;
+	switch (logn) {
+	case 8:
+		len = decode_gr_8_11(logn, q01, buf, buf_len, &ni);
+		break;
+	case 9:
+		len = decode_gr_9_12(logn, q01, buf, buf_len, &ni);
+		break;
+	default:
+		len = decode_gr_10_14(logn, q01, buf, buf_len, &ni);
+		break;
+	}
+	if (len > 0 && ((unsigned)buf[len - 1] >> (8 - ni)) != 0) {
+		return 0;
+	}
+	return len;
+}
+
+/*
+ * Check padding. Return value is 1 if the padding is correct, 0 otherwise.
+ */
+static int
+check_padding(const uint8_t *buf, size_t len)
+{
+	while (len -- > 0) {
+		if (*buf ++ != 0) {
+			return 0;
+		}
+	}
+	return 1;
+}
+
+/*
+ * Decode s1. Returned value is 0 on error, or the actual encoded length
+ * of s1 otherwise. This function checks that the unused bits in the last
+ * used byte are zero.
+ */
+static size_t
+decode_s1(unsigned logn, int16_t *s1, const uint8_t *buf, size_t buf_len)
+{
+	int ni;
+	size_t len;
+	switch (logn) {
+	case 8:
+	case 9:
+		len = decode_gr_5_9(logn, s1, buf, buf_len, &ni);
+		break;
+	default:
+		len = decode_gr_6_10(logn, s1, buf, buf_len, &ni);
+		break;
+	}
+	if (len > 0 && ((unsigned)buf[len - 1] >> (8 - ni)) != 0) {
+		return 0;
+	}
+	return len;
+}
+
+/*
+ * Decode the signature (salt and s1). This function returns an error if
+ * there is trailing garbage (non-zero bits in the padding, including the
+ * unused bits in the last used byte).
+ *
+ * salt can be NULL, in which case the salt is skipped (salt_len must still
+ * contain the correct salt length).
+ *
+ * Returned value: 1 on success, 0 on error.
+ */
+static int
+decode_sig_inner(unsigned logn, uint8_t *salt, size_t salt_len, int16_t *s1,
+	const uint8_t *buf, size_t buf_len)
+{
+	if (buf_len < salt_len) {
+		return 0;
+	}
+	if (salt != NULL) {
+		memcpy(salt, buf, salt_len);
+	}
+	buf += salt_len;
+	buf_len -= salt_len;
+	size_t s1_len = decode_s1(logn, s1, buf, buf_len);
+	if (s1_len == 0) {
+		return 0;
+	}
+	return check_padding(buf + s1_len, buf_len - s1_len);
+}
+
+/* see hawk.h */
+int
+Zh(decode_public_key)(unsigned logn, int16_t *q00_q01_hpk,
+	const void *restrict pub, size_t pub_len)
+{
+	if (logn < 8 || logn > 10) {
+		return 0;
+	}
+	if (pub_len != HAWK_PUBKEY_SIZE(logn)) {
+		return 0;
+	}
+	int16_t *q00 = q00_q01_hpk;
+	int16_t *q01 = q00 + ((size_t)1 << (logn - 1));
+	void *hpk = q01 + ((size_t)1 << logn);
+	size_t hpub_len = (size_t)1 << (logn - 4);
+	const uint8_t *buf = (const uint8_t *)pub;
+	size_t buf_len = pub_len;
+
+	/* Decode q00 */
+	size_t len00 = decode_q00(logn, q00, buf, buf_len);
+	if (len00 == 0) {
+		return 0;
+	}
+	buf += len00;
+	buf_len -= len00;
+
+	/* Decode q01 */
+	size_t len01 = decode_q01(logn, q01, buf, buf_len);
+	if (len01 == 0) {
+		return 0;
+	}
+
+	/* Check that the padding is correct */
+	if (!check_padding(buf + len01, buf_len - len01)) {
+		return 0;
+	}
+
+	/* Recompute SHAKE256(pub) with the appropriate length */
+	shake_context sc;
+	shake_init(&sc, 256);
+	shake_inject(&sc, pub, pub_len);
+	shake_flip(&sc);
+	shake_extract(&sc, hpk, hpub_len);
+
+	return 1;
+}
+
+/* see hawk.h */
+int
+Zh(decode_signature)(unsigned logn, int16_t *s1_and_salt,
+	const void *restrict sig, size_t sig_len)
+{
+	size_t salt_len;
+	switch (logn) {
+	case 8:   salt_len = 14; break;
+	case 9:   salt_len = 24; break;
+	case 10:  salt_len = 40; break;
+	default:
+		return 0;
+	}
+	if (sig_len != HAWK_SIG_SIZE(logn)) {
+		return 0;
+	}
+	int16_t *s1 = s1_and_salt;
+	uint8_t *salt = (uint8_t *)(s1 + ((size_t)1 << logn));
+	return decode_sig_inner(logn, salt, salt_len, s1, sig, sig_len);
+}
+
+/*
+ * Decode s1 and recompute t1, given h1.
+ */
+TARGET_AVX2
+static inline void
+make_t1(unsigned logn, int16_t *d, const uint8_t *h1)
+{
+	size_t n = (size_t)1 << logn;
+#if HAWK_AVX2
+	__m256i ys = _mm256_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14);
+	__m256i y1 = _mm256_set1_epi16(1);
+	for (size_t u = 0; u < n; u += 16) {
+		uint32_t w = (uint32_t)h1[(u >> 3) + 0]
+			| ((uint32_t)h1[(u >> 3) + 1] << 8);
+		w |= (w & 0xFFFE) << 15;
+		__m256i yh = _mm256_set1_epi32(w);
+		yh = _mm256_and_si256(_mm256_srlv_epi32(yh, ys), y1);
+		__m256i yd = _mm256_loadu_si256((const __m256i *)(d + u));
+		yd = _mm256_sub_epi16(yh, _mm256_add_epi16(yd, yd));
+		_mm256_storeu_si256((__m256i *)(d + u), yd);
+	}
+#else // HAWK_AVX2
+	for (size_t u = 0; u < n; u += 8) {
+		uint32_t h1b = h1[u >> 3];
+		for (size_t v = 0; v < 8; v ++, h1b >>= 1) {
+			uint32_t x = d[u + v];
+			x = (h1b & 1) - (x << 1);
+			d[u + v] = (int16_t)*(int32_t *)&x;
+		}
+	}
+#endif // HAWK_AVX2
+}
+
+/*
+ * Inner verification function; non-static so that it may be invoked
+ * from the test code. If ss != NULL, then it should point to a buffer
+ * of size 4*n + 8 bytes, which receives, in that order: s0, s1, tnorm1
+ * and tnorm2 (s0 and s1 are int16_t[n] each, tnorm1 and tnorm2 are
+ * uint32_t).
+ */
+TARGET_AVX2_ONLY
+int
+Zh(verify_inner)(unsigned logn,
+        const void *restrict sig, size_t sig_len,
+        const shake_context *restrict sc_data,
+        const void *restrict pub, size_t pub_len,
+        void *restrict tmp, size_t tmp_len, void *restrict ss);
+int
+Zh(verify_inner)(unsigned logn,
+	const void *restrict sig, size_t sig_len,
+	const shake_context *restrict sc_data,
+	const void *restrict pub, size_t pub_len,
+	void *restrict tmp, size_t tmp_len, void *restrict ss)
+{
+	/*
+	 * Check degree and align temporary buffer.
+	 */
+	if (logn < 8 || logn > 10) {
+		return 0;
+	}
+	if (tmp_len < 7) {
+		return 0;
+	}
+	uintptr_t utmp1 = (uintptr_t)tmp;
+	uintptr_t utmp2 = (utmp1 + 7) & ~(uintptr_t)7;
+	tmp_len -= (size_t)(utmp2 - utmp1);
+	uint32_t *tt32 = (void *)utmp2;
+	if (tmp_len < ((size_t)10 << logn)) {
+		return 0;
+	}
+
+	size_t salt_len;
+	uint32_t max_tnorm;  /* floor((sigma_ver^2)*8*n */
+	switch (logn) {
+	case 8:
+		salt_len = 14;
+		max_tnorm = 2223;
+		break;
+	case 9:
+		salt_len = 24;
+		max_tnorm = 8317;
+		break;
+	case 10:
+		salt_len = 40;
+		max_tnorm = 20218;
+		break;
+	default:
+		return 0;
+	}
+
+	size_t n = (size_t)1 << logn;
+	size_t hn = n >> 1;
+
+	/*
+	 * Decode the signature, resulting into the salt and s1 polynomial.
+	 */
+	int16_t *s1;
+	uint8_t *salt;
+	const uint8_t *s1buf = NULL;
+	size_t s1buf_len = 0;
+	if (sig_len == (size_t)-1) {
+		s1 = (int16_t *)sig;
+		salt = (uint8_t *)(s1 + n);
+	} else {
+		s1 = (int16_t *)tt32;
+		salt = (uint8_t *)(s1 + n);
+		if (sig_len != HAWK_SIG_SIZE(logn)) {
+			return 0;
+		}
+		if (!decode_sig_inner(logn, salt, salt_len, s1, sig, sig_len)) {
+			return 0;
+		}
+		s1buf = (const uint8_t *)sig + salt_len;
+		s1buf_len = sig_len - salt_len;
+	}
+
+	/* Copy s1 to the test buffer (if present). */
+	if (ss != NULL) {
+		memcpy((uint8_t *)ss + (2 * n), s1, 2 * n);
+	}
+
+#if HAWK_DEBUG
+	printf("#### Verify (n=%u):\n", 1u << logn);
+	print_blob("salt", salt, salt_len);
+	print_i16(logn, "s1", s1);
+#endif
+
+	/*
+	 * Compute:
+	 *    hm = SHAKE256(message || H(pub))
+	 *    h = SHAKE256(hm || salt)
+	 * H(pub) is SHAKE256 over the public key, with output length
+	 * 16, 32 or 64 bytes (for n = 256, 512 or 1024).
+	 * hm has length 64 bytes (always).
+	 *
+	 * We keep h into a stack buffer (at most 256 bytes, this is small
+	 * enough not to warrant storing in tmp).
+	 */
+	uint8_t h[256];
+	const void *hpub;
+	size_t hpub_len = (size_t)1 << (logn - 4);
+	shake_context scd;
+	if (pub_len == (size_t)-1) {
+		hpub = (const uint8_t *)pub + (3 * n);
+	} else {
+		shake_init(&scd, 256);
+		shake_inject(&scd, pub, pub_len);
+		shake_flip(&scd);
+		shake_extract(&scd, h, hpub_len);
+		hpub = h;
+	}
+	scd = *sc_data;
+	shake_inject(&scd, hpub, hpub_len);
+	shake_flip(&scd);
+	shake_extract(&scd, h, 64);
+#if HAWK_DEBUG
+	printf("# hm = SHAKE256(message || hpub) (64 bytes)\n");
+	print_blob("hm", h, 64);
+#endif
+	shake_init(&scd, 256);
+	shake_inject(&scd, h, 64);
+	shake_inject(&scd, salt, salt_len);
+	shake_flip(&scd);
+	shake_extract(&scd, h, n >> 2);
+
+#if HAWK_DEBUG
+	printf("# h = SHAKE256(hm || salt) (2*n bits)\n");
+	print_u1(logn, "h0", h);
+	print_u1(logn, "h1", h + (n >> 3));
+#endif
+
+	/*
+	 * Target memory layout, with polynomials in fx32 format:
+	 *   fq00    2*n bytes
+	 *   fq01    4*n bytes
+	 *   ft1     4*n bytes
+	 * Polynomials are scaled by some bits; shift counts are such that
+	 * we get maximum precision but no overflow.
+	 */
+	uint32_t *fq00 = tt32;
+	uint32_t *fq01 = fq00 + hn;
+	uint32_t *ft1 = fq01 + n;
+	int sh_q00 = 29 - bits_lim00[logn];
+	int sh_q01 = 29 - bits_lim01[logn];
+	int sh_t1 = 29 - (1 + bits_lims1[logn]);
+
+	/*
+	 * Compute t1 = h1 - 2*s1 and convert to fx32 format + FFT.
+	 * We also check that sym-break(h1 - 2*s1) is true.
+	 */
+#if HAWK_AVX2
+	__m128i xone = _mm_set1_epi16(1);
+	__m128i xsh_h = _mm_setr_epi32(0, 2, 4, 6);
+	__m256i ysh_t1 = _mm256_set1_epi32(sh_t1);
+	uint32_t rp = 0, rn = 0, csb = 0xFFFF;
+	for (size_t u = 0; u < n; u += 8) {
+		uint32_t w = (uint32_t)h[(n >> 3) + (u >> 3)];
+		w |= (w & 0xFFFE) << 15;
+		__m128i xh = _mm_set1_epi32(w);
+		xh = _mm_and_si128(xone, _mm_srlv_epi32(xh, xsh_h));
+		__m128i xs1 = _mm_loadu_si128((__m128i *)(s1 + u));
+		__m128i xt1 = _mm_sub_epi16(xh, _mm_add_epi16(xs1, xs1));
+		__m256i yt1 = _mm256_cvtepi16_epi32(xt1);
+		_mm256_storeu_si256((__m256i *)(ft1 + u),
+			_mm256_sllv_epi32(yt1, ysh_t1));
+
+		__m128i xp = _mm_cmpgt_epi16(xt1, _mm_setzero_si128());
+		__m128i xn = _mm_cmpgt_epi16(_mm_setzero_si128(), xt1);
+		uint32_t mp = _mm_movemask_epi8(xp);
+		uint32_t mn = _mm_movemask_epi8(xn);
+		rp |= csb & mp;
+		rn |= csb & mn;
+		csb &= ((rp | rn) - 1) >> 16;
+	}
+	/*
+	 * rp and rn contain the positive/negative masks for the lowest
+	 * group which contained a non-zero value.
+	 */
+#if defined _MSC_VER
+	unsigned long k;
+	(void)_BitScanForward(&k, rp | rn | 0x80000000);
+	if (((rp >> k) & 1) == 0) {
+		return 0;
+	}
+#else
+	if (((rp >> _bit_scan_forward(rp | rn | 0x80000000)) & 1) == 0) {
+		return 0;
+	}
+#endif
+#else // HAWK_AVX2
+	uint32_t csb = 0xFFFFFFFF;
+	for (size_t u = 0; u < n; u += 8) {
+		uint32_t hb = h[(n >> 3) + (u >> 3)];
+		for (size_t v = 0; v < 8; v ++, hb >>= 1) {
+			uint32_t w = (uint32_t)s1[u + v];
+			w = (hb & 1) - (w << 1);
+			ft1[u + v] = fx32_of(*(int32_t *)&w, sh_t1);
+			if ((csb & w) >> 31) {
+				/* First non-zero value is negative. */
+				return 0;
+			}
+			csb &= ~tbmask(-w);
+		}
+	}
+	if (csb) {
+		/* t1 is entirely 0, this is not valid */
+		return 0;
+	}
+#endif // HAWK_AVX2
+	fx32_FFT(logn, ft1);
+
+#if HAWK_DEBUG
+	printf("# ft1 = FFT(h1 - 2*s1)\n");
+	print_i32(logn, "ft1", (int32_t *)ft1);
+#endif
+
+	/*
+	 * Decode the first public key polynomial (q00). We put it
+	 * temporarily at offset 4*n (bytes) in the temporary array.
+	 */
+	int16_t *q00;
+	uint8_t *q00buf = NULL;
+	size_t q00buf_len = 0;
+	if (pub_len == (size_t)-1) {
+		q00 = (int16_t *)pub;
+	} else {
+		q00 = (int16_t *)fq01 + n;
+		if (pub_len != HAWK_PUBKEY_SIZE(logn)) {
+			return 0;
+		}
+		q00buf = (uint8_t *)pub;
+		q00buf_len = decode_q00(logn, q00, q00buf, pub_len - 1);
+		if (q00buf_len == 0) {
+			return 0;
+		}
+	}
+
+#if HAWK_DEBUG
+	printf("# first half of q00 only\n");
+	print_i16(logn - 1, "q00", q00);
+#endif
+
+	/*
+	 * q00 = f*adj(f) + g*adj(g) (mod X^n+1)
+	 * adj(f) = f_0 - \sum_{i=1}^{n-1} f_{n-i}*X^i
+	 * Thus, q00[0] = f_0^2 + g_0^2 + \sum_{i=1}^{n-1} (f_i^2 + g_i^2)
+	 * Therefore, it must be non-negative.
+	 */
+	if (q00[0] < 0) {
+		return 0;
+	}
+
+	/*
+	 * Convert q00 to fx32 and apply the FFT. Since q00 is auto-adjoint,
+	 * we must complete it for the FFT; however, afterwards, we can
+	 * ignore the last n/2 words (the imaginary parts of the FFT
+	 * coefficients should all be zero).
+	 *
+	 * The constant term of q00 can be large, so we clear it first, to
+	 * avoid overflow; it will become a corrective value later on.
+	 */
+	int32_t cstq00 = q00[0];
+#if HAWK_AVX2
+	__m128i xrevmv = _mm_setr_epi8(
+		-1, -1, 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3);
+	__m256i ysh_q00 = _mm256_set1_epi32(sh_q00);
+	__m128i xi1 = _mm_loadu_si128((__m128i *)q00);
+	__m128i xdelay = _mm_shuffle_epi8(xi1, xrevmv);
+	__m256i yi1 = _mm256_sllv_epi32(_mm256_cvtepi16_epi32(xi1), ysh_q00);
+	_mm256_storeu_si256((__m256i *)fq00, yi1);
+	for (size_t u = 8; u < hn; u += 8) {
+		__m128i x1 = _mm_loadu_si128((__m128i *)(q00 + u));
+		__m128i x2 = _mm_blend_epi16(x1, xdelay, 0xFE);
+		xdelay = _mm_shuffle_epi8(x1, xrevmv);
+		x2 = _mm_sub_epi16(_mm_setzero_si128(), x2);
+		__m256i y1 = _mm256_sllv_epi32(
+			_mm256_cvtepi16_epi32(x1), ysh_q00);
+		__m256i y2 = _mm256_sllv_epi32(
+			_mm256_cvtepi16_epi32(x2), ysh_q00);
+		_mm256_storeu_si256((__m256i *)(fq00 + u), y1);
+		_mm256_storeu_si256((__m256i *)(fq00 + n - u), y2);
+	}
+	__m128i xi2 = _mm_sub_epi16(_mm_setzero_si128(), xdelay);
+	__m256i yi2 = _mm256_sllv_epi32(_mm256_cvtepi16_epi32(xi2), ysh_q00);
+	_mm256_storeu_si256((__m256i *)(fq00 + hn), yi2);
+	fq00[0] = 0;
+#else // HAWK_AVX2
+	fq00[0] = 0;
+	fq00[hn] = 0;
+	for (size_t u = 1; u < hn; u ++) {
+		uint32_t z = fx32_of(q00[u], sh_q00);
+		fq00[u] = z;
+		fq00[n - u] = -z;
+	}
+#endif // HAWK_AVX2
+	fx32_FFT(logn, fq00);
+
+#if HAWK_DEBUG
+	printf("# fq00 = FFT(q00') (q00' = q00 except that q00'[0] = 0)\n");
+	print_i32(logn, "fq00", (int32_t *)fq00);
+#endif
+
+	/*
+	 * Decode q01 and convert it to fx32 + FFT. The conversion to fx32
+	 * is done in-place; to deal with the overlap, we ensure that q01
+	 * is written into the second half of fq01.
+	 */
+	int16_t *q01;
+	uint8_t *q01buf = NULL;
+	size_t q01buf_len = 0;
+	if (pub_len == (size_t)-1) {
+		q01 = (int16_t *)pub + hn;
+	} else {
+		q01 = (int16_t *)(fq01 + hn);
+		q01buf = q00buf + q00buf_len;
+		q01buf_len = pub_len - q00buf_len;
+		size_t len01 = decode_q01(logn, q01, q01buf, q01buf_len);
+		if (len01 == 0) {
+			return 0;
+		}
+		/* This is the first decoding of the public key; we must
+		   check the padding. */
+		if (!check_padding(q01buf + len01, q01buf_len - len01)) {
+			return 0;
+		}
+	}
+#if HAWK_DEBUG
+	print_i16(logn, "q01", q01);
+#endif
+
+#if HAWK_AVX2
+	__m256i ysh_q01 = _mm256_set1_epi32(sh_q01);
+	for (size_t u = 0; u < n; u += 8) {
+		__m128i x = _mm_loadu_si128((__m128i *)(q01 + u));
+		__m256i y = _mm256_cvtepi16_epi32(x);
+		y = _mm256_sllv_epi32(y, ysh_q01);
+		_mm256_storeu_si256((__m256i *)(fq01 + u), y);
+	}
+#else // HAWK_AVX2
+	for (size_t u = 0; u < n; u ++) {
+		fq01[u] = fx32_of(q01[u], sh_q01);
+	}
+#endif // HAWK_AVX2
+	fx32_FFT(logn, fq01);
+
+#if HAWK_DEBUG
+	printf("# fq01 = FFT(q01)\n");
+	print_i32(logn, "fq01", (int32_t *)fq01);
+#endif
+
+	/*
+	 * fq01 <- (q01*t1)/q00
+	 */
+#if HAWK_AVX2
+	/*
+	 * Potential replacement for the division step, using AVX2 with
+	 * floating-point instructions. It is slightly faster, but since
+	 * it does not compute things with the same precision as the
+	 * integer code, it may yield a result that rounds to the s0
+	 * value differently, and ultimately lead to a different
+	 * signature validation outcome (presumably, the signer would
+	 * have to make it so on purpose, it will not happen in practice
+	 * with an honest signer). This can break consensus protocols
+	 * that rely on all parties always agreeing on the validity of a
+	 * given pkey+msg+sig.
+	 */
+	/*
+	uint64_t cstup = (uint64_t)cstq00 << (sh_q00 - (logn - 1));
+	__m256d ycstup = _mm256_set1_pd((double)*(int64_t *)&cstup);
+	for (size_t u = 0; u < hn; u += 4) {
+		__m128i xq00 = _mm_loadu_si128((__m128i *)(fq00 + u));
+		__m128i xq01_re = _mm_loadu_si128((__m128i *)(fq01 + u));
+		__m128i xq01_im = _mm_loadu_si128((__m128i *)(fq01 + u + hn));
+		__m128i xt1_re = _mm_loadu_si128((__m128i *)(ft1 + u));
+		__m128i xt1_im = _mm_loadu_si128((__m128i *)(ft1 + u + hn));
+		__m256d yq00 = _mm256_add_pd(ycstup, _mm256_cvtepi32_pd(xq00));
+		__m256d yq01_re = _mm256_cvtepi32_pd(xq01_re);
+		__m256d yq01_im = _mm256_cvtepi32_pd(xq01_im);
+		__m256d yt1_re = _mm256_cvtepi32_pd(xt1_re);
+		__m256d yt1_im = _mm256_cvtepi32_pd(xt1_im);
+		__m256d y_re = _mm256_sub_pd(
+			_mm256_mul_pd(yq01_re, yt1_re),
+			_mm256_mul_pd(yq01_im, yt1_im));
+		__m256d y_im = _mm256_add_pd(
+			_mm256_mul_pd(yq01_re, yt1_im),
+			_mm256_mul_pd(yq01_im, yt1_re));
+		y_re = _mm256_div_pd(y_re, yq00);
+		y_im = _mm256_div_pd(y_im, yq00);
+		xq01_re = _mm256_cvtpd_epi32(y_re);
+		xq01_im = _mm256_cvtpd_epi32(y_im);
+		_mm_storeu_si128((__m128i *)(fq01 + u), xq01_re);
+		_mm_storeu_si128((__m128i *)(fq01 + u + hn), xq01_im);
+	}
+	*/
+#endif // HAWK_AVX2
+
+	/*
+	 * We inject back q00[0] here; in FFT representation, it must be
+	 * added to all elements. Note the following:
+	 *  - q00 = f*adj(f) + g*adj(g); in FFT representation, the
+	 *    coefficients of f*adj(f) are non-negative real numbers,
+	 *    so the same must hold for q00 (if the public key is valid).
+	 *  - Our fixed-point representation was adjusted such that the
+	 *    values in fq00[] are less than 2^29.
+	 *  - q00[0] is a 16-bit integer (necessarily non-negative) and
+	 *    is here shifted by at most 13 bits, which again yield an
+	 *    integer lower than 2^28.
+	 * Thus, in the loop below, all divisors should be positive integers
+	 * less than 2^30; if they are not, then the public key is invalid
+	 * and we should reject it.
+	 *
+	 * In each division, the dividend is the sum (or difference) of
+	 * two products of elements of fq01[] and ft1[]; these elements are
+	 * also bound by 2^29 (in absolute value), so the dividend cannot be
+	 * greater than 2^58 (in absolute value). To enforce reproducible
+	 * checks, we perform the following:
+	 *
+	 *   - Negative dividends are negated, so that we always divide a
+	 *     positive value by the positive divisor; we can do that with
+	 *     an unsigned division. The sign is applied afterwards to the
+	 *     result.
+	 *
+	 *   - We explicitly check that the upper word of the dividend is
+	 *     lower than the divisor; this guarantees that the quotient
+	 *     fits in an (unsigned) 32-bit word.
+	 *
+	 * In particular, it is possible to use an unsigned 64-by-32
+	 * division, such as the one-operand 'div' 32-bit opcode on x86
+	 * (dividend is in edx:eax, quotient is returned in eax and
+	 * remainder in edx); there will be no overflow, hence no CPU
+	 * exception. This cannot be expressed in C, since C division
+	 * operators (and the standard div(), ldiv() and lldiv()
+	 * functions) always use the same type for the two operands, and
+	 * the C compiler does not understand that thanks to our checks,
+	 * our results will always fit. When using GCC or Clang on x86,
+	 * we can use some inline assembly.
+	 */
+	uint32_t cstup = (uint32_t)cstq00 << (sh_q00 - (logn - 1));
+	for (size_t u = 0; u < hn; u ++) {
+#define M(c, d)   ((uint64_t)((int64_t)*(int32_t *)&(c) \
+                   * (int64_t)*(int32_t *)&(d)))
+
+		uint32_t q01_re = fq01[u];
+		uint32_t q01_im = fq01[u + hn];
+		uint32_t t1_re = ft1[u];
+		uint32_t t1_im = ft1[u + hn];
+		uint64_t x_re = M(q01_re, t1_re) - M(q01_im, t1_im);
+		uint64_t x_im = M(q01_re, t1_im) + M(q01_im, t1_re);
+
+		uint64_t sx_re = (uint64_t)(*(int64_t *)&x_re >> 63);
+		uint64_t sx_im = (uint64_t)(*(int64_t *)&x_im >> 63);
+		x_re = (x_re ^ sx_re) - sx_re;
+		x_im = (x_im ^ sx_im) - sx_im;
+
+		uint32_t x_re_hi = (uint32_t)(x_re >> 32);
+		uint32_t x_re_lo = (uint32_t)x_re;
+		uint32_t x_im_hi = (uint32_t)(x_im >> 32);
+		uint32_t x_im_lo = (uint32_t)x_im;
+		uint32_t w00 = cstup + fq00[u];
+		if ((w00 - 1) >= 0x3FFFFFFF
+			|| x_re_hi >= w00 || x_im_hi >= w00)
+		{
+			/* Division would overflow. This should never happen
+			   with a valid signature. */
+			return 0;
+		}
+#if HAWK_AVX2
+		/*
+		 * When using AVX2, the rest of the signature verification
+		 * is fast enough that it is worthwhile to optimize this
+		 * step with an inline assembly 'div' opcode. We can use
+		 * the 64/32 variant, since we know that the quotient will
+		 * fit in 32 bits.
+		 */
+#if defined __GNUC__ || defined __clang__
+		/*
+		 * x86 in 32-bit or 64-bit with GCC syntax.
+		 */
+		uint32_t y_re = x_re_lo;
+		__asm__ ("divl %2" : "=a" (y_re), "=d" (x_re_hi)
+		                   : "r" (w00), "0" (y_re), "1" (x_re_hi));
+		uint32_t y_im = x_im_lo;
+		__asm__ ("divl %2" : "=a" (y_im), "=d" (x_im_hi)
+		                   : "r" (w00), "0" (y_im), "1" (x_im_hi));
+#elif defined _MSC_VER && defined _M_IX86
+		/*
+		 * MSVC supports inline assembly on 32-bit x86 only.
+		 */
+		uint32_t y_re, y_im;
+		__asm {
+			mov ecx, w00
+			mov edx, x_re_hi
+			mov eax, x_re_lo
+			div ecx
+			mov y_re, eax
+			mov edx, x_im_hi
+			mov eax, x_im_lo
+			div ecx
+			mov y_im, eax
+		}
+#else
+		uint32_t y_re = (((uint64_t)x_re_hi << 32) | x_re_lo) / w00;
+		uint32_t y_im = (((uint64_t)x_im_hi << 32) | x_im_lo) / w00;
+#endif
+#else // HAWK_AVX2
+		uint32_t y_re = (((uint64_t)x_re_hi << 32) | x_re_lo) / w00;
+		uint32_t y_im = (((uint64_t)x_im_hi << 32) | x_im_lo) / w00;
+#endif // HAWK_AVX2
+
+		fq01[u]      = (y_re ^ (uint32_t)sx_re) - (uint32_t)sx_re;
+		fq01[u + hn] = (y_im ^ (uint32_t)sx_im) - (uint32_t)sx_im;
+
+#undef M
+	}
+	fx32_iFFT(logn, fq01);
+
+#if HAWK_DEBUG
+	printf("# fz = (q01*t1)/q00\n");
+	print_i32(logn, "fz", (int32_t *)fq01);
+#endif
+
+	/*
+	 * s0 <- round(h0/2 + (q01*t1)/q00)
+	 * t0 <- h0 - 2*s0
+	 * We also check that s0 is within the expected range; this is needed
+	 * to verify that we do not lose information when writing the
+	 * coefficients as int16_t, and also to ensure that the norm can be
+	 * computed with the two moduli p1 and p2.
+	 */
+	int16_t *t0 = (int16_t *)tt32;
+	int sh_s0 = sh_t1 + sh_q01 - sh_q00 - (logn - 1);
+	int32_t lims0 = (int32_t)1 << bits_lims0[logn];
+#if HAWK_AVX2
+	__m256i ysb = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);
+	__m256i ymb = _mm256_set1_epi32((uint32_t)1 << sh_s0);
+	__m256i yst = _mm256_set1_epi32(sh_s0 + 1);
+	__m256i ylimp = _mm256_set1_epi32(lims0);
+	__m256i ylimn = _mm256_set1_epi32(-lims0 - 1);
+	__m256i ylc = _mm256_set1_epi32(-1);
+	__m256i ypw = _mm256_setr_epi8(
+		0, 1, 4, 5, 8, 9, 12, 13, -1, -1, -1, -1, -1, -1, -1, -1,
+		0, 1, 4, 5, 8, 9, 12, 13, -1, -1, -1, -1, -1, -1, -1, -1);
+	for (size_t u = 0; u < n; u += 8) {
+		uint32_t w1 = (uint32_t)h[u >> 3];
+		uint32_t w2 = w1 | (w1 << 15);
+		__m128i xh = _mm_set1_epi32(w2);
+		xh = _mm_and_si128(xone, _mm_srlv_epi32(xh, xsh_h));
+		w1 <<= sh_s0;
+		__m256i yh = _mm256_and_si256(
+			_mm256_srlv_epi32(_mm256_set1_epi32(w1), ysb), ymb);
+		__m256i yz = _mm256_loadu_si256((__m256i *)(fq01 + u));
+		yz = _mm256_add_epi32(_mm256_add_epi32(yh, yz), ymb);
+		yz = _mm256_srav_epi32(yz, yst);
+		ylc = _mm256_and_si256(ylc, _mm256_and_si256(
+			_mm256_cmpgt_epi32(ylimp, yz),
+			_mm256_cmpgt_epi32(yz, ylimn)));
+		yz = _mm256_permute4x64_epi64(
+			_mm256_shuffle_epi8(yz, ypw), 0xD8);
+		__m128i xz = _mm256_castsi256_si128(yz);
+		if (ss != NULL) {
+			_mm_storeu_si128(
+				(__m128i *)((uint8_t *)ss + (u << 1)), xz);
+		}
+		xz = _mm_sub_epi16(xh, _mm_add_epi16(xz, xz));
+		_mm_storeu_si128((__m128i *)(t0 + u), xz);
+	}
+	if (_mm256_movemask_epi8(ylc) != -1) {
+		return 0;
+	}
+#else // HAWK_AVX2
+	for (size_t u = 0; u < n; u += 8) {
+		uint32_t h0b = h[u >> 3];
+		for (size_t v = 0; v < 8; v ++, h0b >>= 1) {
+			uint32_t w = fx32_of(h0b & 1, sh_s0) + fq01[u + v];
+			int32_t z = fx32_rint(w, sh_s0 + 1);
+			if (z < -lims0 || z >= lims0) {
+				return 0;
+			}
+			if (ss != NULL) {
+				int16_t zz = (int16_t)z;
+				memcpy((uint8_t *)ss + ((u + v) << 1), &zz, 2);
+			}
+			w = (h0b & 1) - ((uint32_t)z << 1);
+			t0[u + v] = (int16_t)*(int32_t *)&w;
+		}
+	}
+#endif // HAWK_AVX2
+
+#if HAWK_DEBUG
+	printf("# t0 = h0 - 2*s0\n");
+	print_i16(logn, "t0", t0);
+#endif
+
+	/*
+	 * At that point, we have t0 at the start of the temporary array.
+	 * If s1, q00 and/or q01 were not provided in already decoded format,
+	 * then they must be redecoded. t1 was also lost, so it is recomputed
+	 * as t1 = h1 - 2*s1.
+	 *
+	 * The squared norm in base Q is such that:
+	 *
+	 *   n*sqnorm_Q(t) = Tr(q00*t0*adj(t0) + adj(q01)*t0*adj(t1)
+	 *                      + q01*adj(t0)*t1 + q11*t1*adj(t1))
+	 *
+	 * The trace can be computed in NTT representation by simply
+	 * adding the coefficients together.
+	 *
+	 * To perform the operation with minimal RAM usage, we rewrite the
+	 * expression above into:
+	 *
+	 *   d = t1/q00
+	 *   e = t0 + q01*d
+	 *   n*sqnorm_Q(t) = Tr(q00*e*adj(e) + d*adj(t1))
+	 *
+	 * This works because q00*q11 = 1 + q01*adj(q01), and adj(q00) = q00.
+	 *
+	 * We must do the computation twice (modulo p1 and p2): thanks
+	 * to the limits enforced on the coefficients of q00, q01, q11
+	 * (at keygen), t0 and t1 (verified above), we know that the
+	 * norm over the integers cannot exceed 15*2^58; since p1*p2 is
+	 * larger than that bound, two moduli are enough.
+	 */
+
+	uint32_t *c1 = (uint32_t *)(t0 + n);
+	uint32_t *c2 = c1 + n;
+	int16_t *c1hi = (int16_t *)c1 + n;
+	int16_t *c2hi = (int16_t *)c2 + n;
+
+	uint32_t tnorm = 0;
+	for (int i = 0; i < 2; i ++) {
+		uint32_t p = (i == 0) ? P1 : P2;
+		uint32_t p0i = (i == 0) ? P1_0i : P2_0i;
+		uint32_t R3 = (i == 0) ? P1_R3 : P2_R3;
+		uint32_t m16 = (i == 0) ? P1_m16 : P2_m16;
+		const uint32_t *gm = (i == 0) ? GM_p1 : GM_p2;
+#if HAWK_AVX2
+		__m256i yp = _mm256_set1_epi32(p);
+		__m256i yp0i = _mm256_set1_epi32(p0i);
+		__m256i ym16 = _mm256_set1_epi32(m16);
+		__m256i yone = _mm256_set1_epi32(1);
+		__m256i yR3 = _mm256_set1_epi32(R3);
+#endif // HAWK_AVX2
+
+		/* c2 <- t1 */
+		if (sig_len == (size_t)-1) {
+			memcpy(c2hi, s1, n * sizeof(int16_t));
+		} else {
+			(void)decode_s1(logn, c2hi, s1buf, s1buf_len);
+		}
+		make_t1(logn, c2hi, h + (n >> 3));
+		mp_poly_to_NTT(logn, c2, c2hi, p, p0i, gm);
+
+		/* c1 <- q00 (auto-adjoint) */
+		if (pub_len != (size_t)-1) {
+			q00 = c1hi;
+			(void)decode_q00(logn, q00, q00buf, q00buf_len);
+		}
+		mp_poly_to_NTT_autoadj(logn, c1, q00, p, p0i, gm);
+
+		/*
+		 * c1 <- 1/c1
+		 * We use Montgomery's trick to invert the n/2 values.
+		 * For any x and y:
+		 *   1/x = y*(1/(x*y)) 
+		 *   1/y = x*(1/(x*y)) 
+		 * Applied recursively, this allows inverting m values at
+		 * the cost of a single true inversion, and 3*(m-1)
+		 * much cheaper multiplications.
+		 *
+		 * This requires some temporary storage, but the upper
+		 * half of c1[] is free at this point.
+		 */
+#if HAWK_AVX2
+		__m256i ybx = _mm256_loadu_si256((const __m256i *)c1);
+		_mm256_storeu_si256((__m256i *)(c1 + hn), ybx);
+		for (size_t u = 8; u < hn; u += 8) {
+			__m256i yc = _mm256_loadu_si256((__m256i *)(c1 + u));
+			ybx = mp_montymul_x8(ybx, yc, yp, yp0i);
+			_mm256_storeu_si256((__m256i *)(c1 + u + hn), ybx);
+		}
+		ybx = mp_div_x8(yone, ybx, yp, yp0i, ym16);
+		for (size_t u = hn - 8; u > 0; u -= 8) {
+			__m256i yi = _mm256_loadu_si256(
+				(const __m256i *)(c1 + u + hn - 8));
+			__m256i yj = _mm256_loadu_si256((__m256i *)(c1 + u));
+			yi = mp_montymul_x8(ybx, yi, yp, yp0i);
+			ybx = mp_montymul_x8(ybx, yj, yp, yp0i);
+			_mm256_storeu_si256((__m256i *)(c1 + u), yi);
+		}
+		_mm256_storeu_si256((__m256i *)c1, ybx);
+#else // HAWK_AVX2
+		uint32_t bx = c1[0];
+		c1[hn] = bx;
+		for (size_t u = 1; u < hn; u ++) {
+			bx = mp_montymul(bx, c1[u], p, p0i);
+			c1[u + hn] = bx;
+		}
+		bx = mp_div(1, bx, p, p0i, m16);
+		for (size_t u = hn - 1; u > 0; u --) {
+			uint32_t ix = mp_montymul(bx, c1[u + hn - 1], p, p0i);
+			bx = mp_montymul(bx, c1[u], p, p0i);
+			c1[u] = ix;
+		}
+		c1[0] = bx;
+#endif // HAWK_AVX2
+
+		/* c2 <- c2*c1 = t1/q00
+		   nnacc = Tr_p(t1*adj(t1)/q00) */
+#if HAWK_AVX2
+		__m256i yacc = _mm256_setzero_si256();
+		__m256i yrev = _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0);
+		for (size_t u = 0; u < hn; u += 8) {
+			__m256i y1 = _mm256_loadu_si256(
+				(const __m256i *)(c2 + u));
+			__m256i y2 = _mm256_loadu_si256(
+				(const __m256i *)(c2 + (n - 8) - u));
+			__m256i yq = _mm256_loadu_si256(
+				(const __m256i *)(c1 + u));
+			y2 = _mm256_permutevar8x32_epi32(y2, yrev);
+			y1 = mp_montymul_x8(y1, yq, yp, yp0i);
+			yacc = mp_add_x8(yacc,
+				mp_montymul_x8(y1, y2, yp, yp0i), yp);
+			y2 = mp_montymul_x8(y2, yq, yp, yp0i);
+			y2 = _mm256_permutevar8x32_epi32(y2, yrev);
+			_mm256_storeu_si256((__m256i *)(c2 + u), y1);
+			_mm256_storeu_si256((__m256i *)(c2 + (n - 8) - u), y2);
+		}
+		yacc = mp_add_x8(yacc, _mm256_srli_epi64(yacc, 32), yp);
+		yacc = mp_add_x8(yacc, _mm256_bsrli_epi128(yacc, 8), yp);
+		uint32_t nnacc = _mm256_cvtsi256_si32(yacc);
+		nnacc = mp_add(nnacc, _mm256_extract_epi32(yacc, 4), p);
+#else // HAWK_AVX2
+		uint32_t nnacc = 0;
+		for (size_t u = 0; u < hn; u ++) {
+			uint32_t x1 = c2[u];
+			uint32_t x2 = c2[(n - 1) - u];
+			uint32_t qx = c1[u];
+			x1 = mp_montymul(x1, qx, p, p0i);
+			nnacc = mp_add(nnacc, mp_montymul(x1, x2, p, p0i), p);
+			x2 = mp_montymul(x2, qx, p, p0i);
+			c2[u] = x1;
+			c2[(n - 1) - u] = x2;
+		}
+#endif // HAWK_AVX2
+		/* nnacc is in double-anti-Montgomery representation
+		   (i.e. divided by R = 2^64) */
+
+		/* c1 <- q01 */
+		if (pub_len != (size_t)-1) {
+			q01 = c1hi;
+			(void)decode_q01(logn, q01, q01buf, q01buf_len);
+		}
+		mp_poly_to_NTT(logn, c1, q01, p, p0i, gm);
+
+		/* c1 <- c1*c2 = q01*t1/q00 */
+#if HAWK_AVX2
+		for (size_t u = 0; u < n; u += 8) {
+			__m256i y1 = _mm256_loadu_si256((__m256i *)(c1 + u));
+			__m256i y2 = _mm256_loadu_si256((__m256i *)(c2 + u));
+			y1 = mp_montymul_x8(y1, y2, yp, yp0i);
+			y1 = mp_montymul_x8(y1, yR3, yp, yp0i);
+			_mm256_storeu_si256((__m256i *)(c1 + u), y1);
+		}
+#else // HAWK_AVX2
+		for (size_t u = 0; u < n; u ++) {
+			c1[u] = mp_montymul(
+				mp_montymul(c1[u], c2[u], p, p0i), R3, p, p0i);
+		}
+#endif // HAWK_AVX2
+
+		/* c2 <- t0 */
+		mp_poly_to_NTT(logn, c2, t0, p, p0i, gm);
+
+		/* c2 <- c2 + c1 = t0 + q01*t1/q00 = e */
+#if HAWK_AVX2
+		for (size_t u = 0; u < n; u += 8) {
+			__m256i y1 = _mm256_loadu_si256((__m256i *)(c1 + u));
+			__m256i y2 = _mm256_loadu_si256((__m256i *)(c2 + u));
+			y2 = mp_add_x8(y1, y2, yp);
+			_mm256_storeu_si256((__m256i *)(c2 + u), y2);
+		}
+#else // HAWK_AVX2
+		for (size_t u = 0; u < n; u ++) {
+			c2[u] = mp_add(c2[u], c1[u], p);
+		}
+#endif // HAWK_AVX2
+
+		/* c1 <- q00 (auto-adjoint) */
+		if (pub_len != (size_t)-1) {
+			q00 = c1hi;
+			(void)decode_q00(logn, q00, q00buf, q00buf_len);
+		}
+		mp_poly_to_NTT_autoadj(logn, c1, q00, p, p0i, gm);
+
+		/* nnacc <- nnacc + Tr_p(c1*c2*adj(c2) = q00*e*adj(e)) */
+#if HAWK_AVX2
+		yacc = _mm256_setzero_si256();
+		for (size_t u = 0; u < hn; u += 8) {
+			__m256i y1 = _mm256_loadu_si256(
+				(const __m256i *)(c2 + u));
+			__m256i y2 = _mm256_loadu_si256(
+				(const __m256i *)(c2 + (n - 8) - u));
+			__m256i yq = _mm256_loadu_si256(
+				(const __m256i *)(c1 + u));
+			y2 = _mm256_permutevar8x32_epi32(y2, yrev);
+			yacc = mp_add_x8(yacc,
+				mp_montymul_x8(yq,
+					mp_montymul_x8(y1, y2, yp, yp0i),
+					yp, yp0i), yp);
+		}
+		yacc = mp_add_x8(yacc, _mm256_srli_epi64(yacc, 32), yp);
+		yacc = mp_add_x8(yacc, _mm256_bsrli_epi128(yacc, 8), yp);
+		nnacc = mp_add(nnacc, _mm256_cvtsi256_si32(yacc), p);
+		nnacc = mp_add(nnacc, _mm256_extract_epi32(yacc, 4), p);
+#else // HAWK_AVX2
+		for (size_t u = 0; u < hn; u ++) {
+			uint32_t x1 = c2[u];
+			uint32_t x2 = c2[(n - 1) - u];
+			uint32_t qx = c1[u];
+			nnacc = mp_add(nnacc, mp_montymul(qx,
+				mp_montymul(x1, x2, p, p0i), p, p0i), p);
+		}
+#endif // HAWK_AVX2
+
+		/* nnacc is in double-anti-Montgomery representation;
+		   convert it to normal representation */
+		nnacc = mp_montymul(nnacc, R3, p, p0i);
+
+		if (ss != NULL) {
+			memcpy((uint8_t *)ss + 4 * n + 4 * i, &nnacc, 4);
+		}
+
+#if HAWK_DEBUG
+		printf("# Qnorm(t)^2 mod %lu\n", (unsigned long)p);
+		printf("qnt = %lu\n", (unsigned long)nnacc);
+#endif
+
+		if (i == 0) {
+			tnorm = nnacc;
+		} else {
+			if (tnorm != nnacc) {
+				/* Not the same value modulo p1 and p2;
+				   thus, the value is greater than p2,
+				   hence too large. */
+				return 0;
+			}
+		}
+	}
+
+	/*
+	 * We have (n*sqnorm_Q(t))/2 in tnorm (for each component of tnorm,
+	 * we only used half of the coefficients, since adj(t)*Q*t is
+	 * auto-adjoint). Signature is valid if:
+	 *   sqnorm_Q(h/2 - s) <= (sigma_ver^2)*2*n
+	 * since t = 2*(h/2 - s), we have sqnorm(t) = 4*sqnorm(h/2 - s).
+	 * We thus need:
+	 *   n*sqnorm_Q(t) <= (sigma_ver^2)*8*n^2
+	 * i.e.:
+	 *   tnorm <= (sigma_ver^2)*4*n^2
+	 *
+	 * Moreover, tnorm should be a multiple of 2^(n-1) at this point,
+	 * since sqnorm_Q(t) is integral. We check this property as well;
+	 * this allows expressing the bound as:
+	 *   (tnorm >> (logn-1)) <= (sigma_ver^2)*8*n
+	 */
+	return (tnorm & (hn - 1)) == 0 && (tnorm >> (logn - 1)) <= max_tnorm;
+}
+
+/* see hawk.h */
+int
+Zh(verify_finish)(unsigned logn,
+	const void *restrict sig, size_t sig_len,
+	const shake_context *restrict sc_data,
+	const void *restrict pub, size_t pub_len,
+	void *restrict tmp, size_t tmp_len)
+{
+	/*
+	 * Check degree and align temporary buffer.
+	 */
+	if (logn < 8 || logn > 10) {
+		return 0;
+	}
+	if (tmp_len < 7) {
+		return 0;
+	}
+	uintptr_t utmp1 = (uintptr_t)tmp;
+	uintptr_t utmp2 = (utmp1 + 7) & ~(uintptr_t)7;
+	tmp_len -= (size_t)(utmp2 - utmp1);
+	uint32_t *tt32 = (void *)utmp2;
+
+	/*
+	 * If the signature and/or public key is in encoded format,
+	 * try decoding it into the temporary buffer, if size allows.
+	 * We also round up sizes to the next multiple of 32 bytes, to
+	 * preserve 256-bit alignment, if any.
+	 */
+	size_t min_tmp_len = (size_t)10 << logn;
+	if (pub_len != (size_t)-1) {
+		size_t dpk_len = HAWK_DECODED_PUB_LENGTH(logn) << 1;
+		dpk_len = (dpk_len + 31) & ~(size_t)31;
+		if (tmp_len >= min_tmp_len + dpk_len) {
+			int16_t *q00_q01_hpk = (int16_t *)tt32;
+			tt32 = (uint32_t *)(q00_q01_hpk + (dpk_len >> 1));
+			tmp_len -= dpk_len;
+			if (Zh(decode_public_key)(logn, q00_q01_hpk,
+				pub, pub_len) == 0)
+			{
+				return 0;
+			}
+			pub = q00_q01_hpk;
+			pub_len = (size_t)-1;
+		}
+	}
+	if (sig_len != (size_t)-1) {
+		size_t dsig_len = HAWK_DECODED_SIG_LENGTH(logn) << 1;
+		dsig_len = (dsig_len + 31) & ~(size_t)31;
+		if (tmp_len >= min_tmp_len + dsig_len) {
+			int16_t *s1_and_salt = (int16_t *)tt32;
+			tt32 = (uint32_t *)(s1_and_salt + (dsig_len >> 1));
+			tmp_len -= dsig_len;
+			if (Zh(decode_signature)(logn, s1_and_salt,
+				sig, sig_len) == 0)
+			{
+				return 0;
+			}
+			sig = s1_and_salt;
+			sig_len = (size_t)-1;
+		}
+	}
+	return Zh(verify_inner)(logn, sig, sig_len, sc_data,
+		pub, pub_len, tt32, tmp_len, NULL);
+}
diff --git a/lib/dns/hawk/modq.h b/lib/dns/hawk/modq.h
new file mode 100644
index 0000000000..5227c33d65
--- /dev/null
+++ b/lib/dns/hawk/modq.h
@@ -0,0 +1,1358 @@
+/*
+ * This file is supposed to be included only with the Q macro already
+ * defined to the value of the modulus (as a literal constant).
+ * Moreover, Q is explicitly undefined at the end of this file.
+ *
+ * It is assumed that the file that includes this file already included
+ * "ng_inner.h".
+ */
+
+#ifndef Q
+#error Modulus was not defined!
+#endif
+
+/*
+ * Q0I = -1/q mod 2^32
+ * R = 2^32 mod q
+ * R2 = 2^64 mod q
+ * MSF = multiplier for mq_set()
+ */
+#if Q == 12289
+#define Q0I   4143984639
+#define R          10952
+#define R2          5664
+#define MSF           13
+#elif Q == 18433
+#define Q0I   3955247103
+#define R           4564
+#define R2           806
+#define MSF           19
+#else
+#error Unknown modulus
+#endif
+
+#define Q0Ilo   ((uint32_t)Q0I & 0xFFFF)
+#define Q0Ihi   ((uint32_t)Q0I >> 16)
+#define Zq(name)              Zq_(Q, name)
+#define Zq_(modulus, name)    Zq__(modulus, name)
+#define Zq__(modulus, name)   mq ## modulus ## _ ## name
+
+/*
+ * To avoid spurious warnings about unused functions, we tag them with
+ * a specific attribute.
+ */
+#if defined __GNUC__ || defined __clang__
+#define MQ_UNUSED   __attribute__ ((unused))
+#else
+#define MQ_UNUSED
+#endif
+
+/*
+ * IMPLEMENTATION NOTES:
+ * ---------------------
+ *
+ * Values are kept in the [1..q] range (i.e. 0 is represented as q, not as 0).
+ * Montgomery reduction of an integer x is:
+ *    x <- x * Q0I
+ *    x <- (x >> 16) * Q
+ *    x <- (x >> 16) + 1
+ * This works for all x in the [1..m] range, with:
+ *    m = 2^32 - (2^16 - 1)*(q - 1)
+ * The output is in the expected [1..q] range. Note that it does not work
+ * for x = 0 (it returns 1 in that case, which is wrong). For the supported
+ * values q (up to 18433), this implies that we can add together up to 9
+ * products of reduced values, and apply one mutualized Montgomery reduction.
+ *
+ * When using AVX2, we keep values over 16 bits each, and can perform 16
+ * Montgomery reductions in parallel.
+ */
+
+MQ_UNUSED
+static inline uint32_t
+Zq(add)(uint32_t x, uint32_t y)
+{
+	x = Q - (x + y);
+	x += Q & (x >> 16);
+	return Q - x;
+}
+
+#if HAWK_AVX2
+MQ_UNUSED TARGET_AVX2
+static inline __m256i
+Zq(add_x16)(__m256i a, __m256i b)
+{
+	__m256i Qx16 = _mm256_set1_epi16(Q);
+	a = _mm256_sub_epi16(Qx16, _mm256_add_epi16(a, b));
+	a = _mm256_add_epi16(a,
+		_mm256_and_si256(Qx16, _mm256_srai_epi16(a, 15)));
+	return _mm256_sub_epi16(Qx16, a);
+}
+#endif // HAWK_AVX2
+
+MQ_UNUSED
+static inline uint32_t
+Zq(sub)(uint32_t x, uint32_t y)
+{
+	y -= x;
+	y += Q & (y >> 16);
+	return Q - y;
+}
+
+#if HAWK_AVX2
+MQ_UNUSED TARGET_AVX2
+static inline __m256i
+Zq(sub_x16)(__m256i a, __m256i b)
+{
+	__m256i Qx16 = _mm256_set1_epi16(Q);
+	b = _mm256_sub_epi16(b, a);
+	b = _mm256_add_epi16(b,
+		_mm256_and_si256(Qx16, _mm256_srai_epi16(b, 15)));
+	return _mm256_sub_epi16(Qx16, b);
+}
+#endif // HAWK_AVX2
+
+MQ_UNUSED
+static inline uint32_t
+Zq(neg)(uint32_t x)
+{
+	return Zq(sub)(Q, x);
+}
+
+#if HAWK_AVX2
+MQ_UNUSED TARGET_AVX2
+static inline __m256i
+Zq(neg_x16)(__m256i a)
+{
+	return Zq(sub_x16)(_mm256_set1_epi16(Q), a);
+}
+#endif // HAWK_AVX2
+
+MQ_UNUSED
+static inline uint32_t
+Zq(half)(uint32_t x)
+{
+	return (x >> 1) + (((Q + 1) >> 1) & -(x & 1));
+}
+
+#if HAWK_AVX2
+MQ_UNUSED TARGET_AVX2
+static inline __m256i
+Zq(half_x16)(__m256i a)
+{
+	__m256i hx16 = _mm256_set1_epi16((Q + 1) >> 1);
+	__m256i y1 = _mm256_set1_epi16(1);
+	__m256i yodd = _mm256_sub_epi16(
+		_mm256_setzero_si256(), _mm256_and_si256(a, y1));
+	return _mm256_add_epi16(
+		_mm256_srli_epi16(a, 1),
+		_mm256_and_si256(yodd, hx16));
+}
+#endif // HAWK_AVX2
+
+MQ_UNUSED
+static inline uint32_t
+Zq(montyred)(uint32_t x)
+{
+	x *= Q0I;
+	x = (x >> 16) * Q;
+	return (x >> 16) + 1;
+}
+
+#if HAWK_AVX2
+MQ_UNUSED TARGET_AVX2
+static inline __m256i
+Zq(montyred_x16)(__m256i lo, __m256i hi)
+{
+	__m256i Qx16 = _mm256_set1_epi16(Q);
+	__m256i Q0Ilox16 = _mm256_set1_epi16(Q0Ilo);
+	__m256i Q0Ihix16 = _mm256_set1_epi16(Q0Ihi);
+
+	/* y <- (uint32_t)(x * q0i) >> 16 */
+	__m256i y = _mm256_add_epi16(
+		_mm256_add_epi16(
+			_mm256_mulhi_epu16(lo, Q0Ilox16),
+			_mm256_mullo_epi16(lo, Q0Ihix16)),
+		_mm256_mullo_epi16(hi, Q0Ilox16));
+
+	/* y <- (y * q) >> 16 */
+	y = _mm256_mulhi_epu16(y, Qx16);
+
+	/* return y + 1 */
+	return _mm256_add_epi16(y, _mm256_set1_epi16(1));
+}
+#endif // HAWK_AVX2
+
+MQ_UNUSED
+static inline uint32_t
+Zq(montymul)(uint32_t x, uint32_t y)
+{
+	return Zq(montyred)(x * y);
+}
+
+#if HAWK_AVX2
+MQ_UNUSED TARGET_AVX2
+static inline __m256i
+Zq(montymul_x16)(__m256i a, __m256i b)
+{
+	return Zq(montyred_x16)(
+		_mm256_mullo_epi16(a, b),
+		_mm256_mulhi_epu16(a, b));
+}
+#endif // HAWK_AVX2
+
+MQ_UNUSED
+static inline uint32_t
+Zq(montysqr)(uint32_t x)
+{
+	return Zq(montyred)(x * x);
+}
+
+#if HAWK_AVX2
+MQ_UNUSED TARGET_AVX2
+static inline __m256i
+Zq(montysqr_x16)(__m256i a)
+{
+	return Zq(montyred_x16)(
+		_mm256_mullo_epi16(a, a),
+		_mm256_mulhi_epu16(a, a));
+}
+#endif // HAWK_AVX2
+
+MQ_UNUSED
+static inline uint32_t
+Zq(tomonty)(uint32_t x)
+{
+	return Zq(montyred(x * R2));
+}
+
+#if HAWK_AVX2
+MQ_UNUSED TARGET_AVX2
+static inline __m256i
+Zq(tomonty_x16)(__m256i a)
+{
+	return Zq(montymul_x16)(a, _mm256_set1_epi16(R2));
+}
+#endif // HAWK_AVX2
+
+/*
+ * Reduce an integer x (signed) modulo q, into the [1..q] range.
+ * Maximum input range for |x| is:
+ *    q      max(|x|)
+ *   12289    158876
+ *   18433    326159
+ */
+MQ_UNUSED
+static inline uint32_t
+Zq(set)(int32_t x)
+{
+	return Zq(montyred)((uint32_t)(x + (int32_t)Q * MSF) * R);
+}
+
+/*
+ * Same as mq_set(), but for a small input guaranteed to be lower than
+ * q/2 (e.g. x is really an int8_t).
+ */
+MQ_UNUSED
+static inline uint32_t
+Zq(set_small)(int x)
+{
+	uint32_t y = (uint32_t)-x;
+	y += Q & (y >> 16);
+	return Q - y;
+}
+
+#if HAWK_AVX2
+MQ_UNUSED TARGET_AVX2
+static inline __m256i
+Zq(set_small_x16)(__m256i a)
+{
+	__m256i Qx16 = _mm256_set1_epi16(Q);
+	a = _mm256_sub_epi16(_mm256_setzero_si256(), a);
+	a = _mm256_add_epi16(a,
+		_mm256_and_si256(Qx16, _mm256_srai_epi16(a, 15)));
+	return _mm256_sub_epi16(Qx16, a);
+}
+#endif // HAWK_AVX2
+
+/*
+ * Convert back an integer from [1..q] representation to [0..q-1]
+ * representation.
+ */
+MQ_UNUSED
+static inline uint32_t
+Zq(unorm)(uint32_t x)
+{
+	return x & ((x - Q) >> 16);
+}
+
+#if HAWK_AVX2
+MQ_UNUSED TARGET_AVX2
+static inline __m256i
+Zq(unorm_x16)(__m256i a)
+{
+	__m256i Qx16 = _mm256_set1_epi16(Q);
+	return _mm256_and_si256(a, _mm256_srai_epi16(
+		_mm256_sub_epi16(a, Qx16), 15));
+}
+#endif // HAWK_AVX2
+
+/*
+ * Convert back an integer from [1..q] representation to a signed
+ * value in [-(q-1)/2..+(q-1)/2].
+ */
+MQ_UNUSED
+static inline int32_t
+Zq(snorm)(uint32_t x)
+{
+	x -= Q & (((Q >> 1) - x) >> 16);
+	return *(int32_t *)&x;
+}
+
+#if HAWK_AVX2
+MQ_UNUSED TARGET_AVX2
+static inline __m256i
+Zq(snorm_x16)(__m256i a)
+{
+	__m256i Qx16 = _mm256_set1_epi16(Q);
+	__m256i hQx16 = _mm256_set1_epi16(Q >> 1);
+	return _mm256_sub_epi16(a,
+		_mm256_and_si256(Qx16, _mm256_cmpgt_epi16(a, hQx16)));
+}
+#endif // HAWK_AVX2
+
+/*
+ * Division modulo q. If the divisor is not invertible (i.e. it is zero),
+ * then zero is returned (represented as q).
+ */
+MQ_UNUSED
+static uint32_t
+Zq(div)(uint32_t x, uint32_t y)
+{
+	/* TODO: try binary GCD */
+#if Q == 12289
+	uint32_t y1, y2, y3, y163, y323, yt;
+	y1 = Zq(tomonty(y));            /* y in Montgomery representation */
+	y2 = Zq(montysqr)(y1);          /* y^2 */
+	y3 = Zq(montymul)(y2, y1);      /* y^3 */
+	yt = Zq(montymul)(y3, y2);      /* y^5 */
+	yt = Zq(montysqr)(yt);          /* y^10 */
+	yt = Zq(montysqr)(yt);          /* y^20 */
+	yt = Zq(montysqr)(yt);          /* y^40 */
+	yt = Zq(montysqr)(yt);          /* y^80 */
+	yt = Zq(montysqr)(yt);          /* y^160 */
+	y163 = Zq(montymul)(yt, y3);    /* y^163 */
+	y323 = Zq(montymul)(y163, yt);  /* y^323 */
+	yt = Zq(montysqr)(y323);        /* y^646 */
+	yt = Zq(montysqr)(yt);          /* y^1292 */
+	yt = Zq(montymul)(yt, y163);    /* y^1455 */
+	yt = Zq(montysqr)(yt);          /* y^2910 */
+	yt = Zq(montysqr)(yt);          /* y^5820 */
+	yt = Zq(montymul)(yt, y323);    /* y^6143 */
+	yt = Zq(montysqr)(yt);          /* y^12286 */
+	yt = Zq(montymul)(yt, y1);      /* y^12287 = 1/y */
+	return Zq(montymul)(yt, x);
+#elif Q == 18433
+	uint32_t y1, y7, y28, y35, y63, yt;
+	y1 = Zq(tomonty(y));            /* y in Montgomery representation */
+	yt = Zq(montysqr)(y1);          /* y^2 */
+	yt = Zq(montymul)(yt, y1);      /* y^3 */
+	yt = Zq(montysqr)(yt);          /* y^6 */
+	y7 = Zq(montymul)(yt, y1);      /* y^7 */
+	yt = Zq(montysqr)(y7);          /* y^14 */
+	y28 = Zq(montysqr)(yt);         /* y^28 */
+	y35 = Zq(montymul)(y28, y7);    /* y^35 */
+	y63 = Zq(montymul)(y35, y28);   /* y^63 */
+	yt = Zq(montysqr)(y63);         /* y^126 */
+	yt = Zq(montysqr)(yt);          /* y^252 */
+	yt = Zq(montymul)(yt, y35);     /* y^287 */
+	yt = Zq(montysqr)(yt);          /* y^574 */
+	yt = Zq(montysqr)(yt);          /* y^1148 */
+	yt = Zq(montysqr)(yt);          /* y^2296 */
+	yt = Zq(montysqr)(yt);          /* y^4592 */
+	yt = Zq(montysqr)(yt);          /* y^9184 */
+	yt = Zq(montysqr)(yt);          /* y^18368 */
+	yt = Zq(montymul)(yt, y63);     /* y^18431 = 1/y */
+	return Zq(montymul)(yt, x);
+#else
+#error Unknown modulus
+#endif
+}
+
+/* TODO: div_x16() */
+
+/*
+ * mq_GM[x] = (2^32)*(g^rev(x)) mod q, with g a 2048-th root of 1 modulo q,
+ * and rev() is the bit-reversal function over 10 bits.
+ *     q      g
+ *   12289    7
+ *   18433   19
+ */
+ALIGNED_AVX2
+static const uint16_t Zq(GM)[] = {
+#if Q == 12289
+	 10952,  11183,  10651,   1669,  12036,   5517,  11593,   9397,
+	  7900,   2739,  10901,    589,    971,   1704,   4857,   5562,
+	  6241,  10889,   7260,   3046,   3102,   8228,    519,   6606,
+	 10000,   5956,   6332,  11479,    918,   6357,   7237,    196,
+	  8614,   3587,  11068,  11665,   3165,   1074,   8124,   3246,
+	  9490,  10617,    946,   1812,   2862,   6807,   6659,   7117,
+	  8726,   9985,     10,   9788,   4473,   8204,  11528,   7220,
+	   657,  11417,  10842,   1827,   2845,   7372,   8118,  12120,
+	 11262,   7386,    672,   1521,    734,   8135,   7848,   5913,
+	 12199,  10220,   8447,   4800,   6849,   8754,  12187,   3390,
+	 10989,   5616,   4584,   3792,    618,   7653,   2623,   3907,
+	  3775,   8270,   2759,  11676,   1514,   9681,    182,   1180,
+	  2453,   9557,   9954,    256,   6264,   1450,  11792,  10012,
+	   203,   6988,  12216,   9655,   5443,  11387,   9242,   8739,
+	  8394,   9453,    311,   7013,   7618,   1991,  11971,   3340,
+	  4457,   7290,   7841,   3977,   8601,  10525,   4232,   8262,
+	  9581,  11207,  11931,   1055,   6997,  11064,    208,  11882,
+	  5973,   1724,  10020,    954,   8750,  11356,  11685,   8508,
+	  4350,   5786,   5458,   1491,    768,   7005,   4930,   8196,
+	  9583,   8249,   1639,   9141,   4387,    219,  11680,   3614,
+	 12116,  10087,   5450,   1034,   4563,  10273,   3081,   2420,
+	  1684,   4031,  10170,    306,   2111,  11526,    270,   6207,
+	 10670,  10435,  11721,   4420,  11376,  10826,   3900,   7730,
+	  4465,   7747,   3540,  11743,  10450,   4012,    964,  12057,
+	  4262,    759,   3613,   2088,   5007,   4914,   4011,   3318,
+	  5112,   9376,   4397,  10007,   1767,   4164,    878,   4072,
+	   106,   2983,   7529,  10732,   9138,   2798,   5855,   4200,
+	  6782,   9535,    588,   2867,   9859,   5582,   6867,   6710,
+	  3222,   2794,   9738,    206,  10417,   3663,  11025,   1528,
+	  8132,   3703,   9062,   4601,   5436,   9451,   8397,   5016,
+	    34,  11159,   9371,   2283,   4786,  12259,  10689,   6912,
+	  9827,   3754,  11782,    224,   5481,   4341,  10318,   2616,
+	  8221,   7251,   5761,   8047,  12181,  12264,   2763,   5760,
+	  6141,  11321,   5722,   4283,  10712,   9762,   4502,   2180,
+	 10873,   5134,  11648,   1786,   4530,   9924,    853,   4180,
+	 10729,   9197,   3043,   9466,   8115,   4268,  10521,   9604,
+	  4260,   3717,   1616,   6291,   7617,   3470,   4828,  11586,
+	 10317,   4095,   9487,   2765,   5059,   1740,   6777,   4641,
+	  9748,   9994,    490,    341,  10264,   8748,  11867,   9688,
+	  7615,   6428,   2831,   3500,   4226,   4847,   4534,   4008,
+	 11122,   5533,   8350,    795,  11388,   5367,   3593,   7090,
+	  7879,   9220,   8366,   1709,   3798,  11120,   7291,   6353,
+	 10034,   4826,   3414,   1473,   5704,   6327,   5637,   7108,
+	   640,  11982,     12,   6830,    452,   7387,   8918,   8664,
+	  9596,   1311,   8475,    255,  12000,   9605,    225,  11317,
+	  9947,  10609,   8712,   6113,   8638,   4958,  10454,  10385,
+	  5769,   8504,   2950,  11834,   4612,  11536,   8996,   3903,
+	  9480,    829,   3250,  10538,   3623,  11876,  10744,  11590,
+	  2487,   8427,   7036,   2539,  11050,   1420,  10192,   4635,
+	 10030,  10742,  11709,   9879,  10924,   3439,   7271,  11355,
+	  4237,    867,   9373,  11614,    765,  11442,   8079,   8356,
+	  2585,  10953,   6577,   5505,   6050,  10731,   7026,   5040,
+	  3812,   2703,   8981,   1510,   2385,  11817,   3501,   7979,
+	  8782,    895,   6770,   2705,   5127,  11769,    941,   9207,
+	  6692,   7466,   9035,   7667,   4419,   2047,   6765,  10100,
+	  9872,  10933,   1414,  10113,   8201,  12253,  10369,    921,
+	 12214,    324,   4991,   4000,  11852,   7295,  12204,   2825,
+	  4708,   4731,   6540,  11072,    560,   7412,   6155,   2904,
+	  5194,  10988,    251,   9730,   5358,   1923,   4248,   9176,
+	   515,    233,   4234,   5304,   3820,   3160,   4680,   9276,
+	 10410,   1727,  10180,  10094,   6584,   7441,  11798,   1138,
+	  5220,   9401,   1634,   4247,   8295,   8406,   5916,      4,
+	  1666,   6075,   4486,   1266,   1023,  10819,   7623,   6885,
+	  2252,  11900,  12024,  10976,  10500,   3796,   1733,   5294,
+	  2930,   4547,    823,  11683,  10518,   1752,   7417,   4334,
+	  6144,   6884,   2573,   4123,   6797,  11928,   9421,   2067,
+	  6820,   2489,   1664,   9033,   9425,   8440,   3633,   9375,
+	  8555,   4825,   7457,   6619,   6426,   7632,   1503,   1372,
+	 11142,    531,   3742,   7921,   9866,   7518,   7712,  10433,
+	  4985,    585,   6622,    395,   7745,  10782,   9746,    663,
+	 11926,   8450,     70,   7071,   6733,   8272,   6962,   1384,
+	  4599,   6185,   2160,    500,   7626,   2448,   7670,  11106,
+	  5100,   2546,   4704,  10647,   5138,   7789,   5780,   4524,
+	 11659,  10095,   9973,   9022,  11076,  12122,  11575,  11441,
+	  3189,   2445,   7510,   1966,   4326,   4415,   6072,   2771,
+	  1847,   8734,   7024,   7998,  10598,   6322,   1274,   8260,
+	  4882,   5454,   8233,   1792,   6981,  10150,   8810,   8639,
+	  1421,  12049,  11778,   6140,   1234,   5975,   3249,  12017,
+	  9602,   4726,   2177,  12224,   4170,   1648,  10063,  11091,
+	  6621,   1874,   5731,   3261,  11051,  12230,   5046,   8678,
+	  5622,   4715,   9783,   7385,  12112,   3714,   1456,   9440,
+	  4944,  12068,   8695,   6678,  12094,   5758,   8061,  10400,
+	  5872,   3635,   1339,  10437,   5376,  12168,   9932,   8216,
+	  5636,   8587,  11473,   2542,   6131,   1533,   8026,    720,
+	 11078,   9164,   1283,   7238,   7363,  10466,   9278,   4651,
+	 11788,   3639,   9745,   2142,   2488,   6948,   1890,   6582,
+	   956,  11600,   8313,   6362,   5898,   2048,   2722,   4954,
+	  6677,   5073,    202,   8467,  11705,   3506,   6748,  10665,
+	  5256,   5313,    713,   2327,  10471,   9820,   3499,  10937,
+	 11206,   4187,   6201,   8604,     80,   4570,   6146,   3926,
+	   742,   8592,   3547,   1390,   2521,   7297,   4118,   4822,
+	 10607,   5300,   4116,   7780,   7568,   2207,  11202,  10103,
+	 10265,   7269,   6721,   1442,  11474,   1063,   3441,  10696,
+	  7768,   1343,   1989,   7629,   1185,   4712,   9623,  10534,
+	   238,   4379,   4152,   3692,   8924,  12079,   1089,  11517,
+	  7344,   1700,   8740,   1568,   1500,   5809,  10781,   6023,
+	  8391,   1601,   3460,   7173,  11533,  12114,   7052,   3453,
+	  6120,   5513,   3187,   5403,   1250,   6889,   6936,   2971,
+	  2377,  11360,   7802,    213,   7132,   8023,   5971,   4682,
+	  1369,   2934,   9012,   4817,   7649,   5298,  12202,   5783,
+	  5242,   1441,  11312,   7170,   4163,  12001,   9218,   7368,
+	 10774,   4087,   4964,   7066,  10835,  12180,  10572,   7909,
+	  6791,   8513,   3430,   2387,  10403,  12080,   9335,   6371,
+	  4149,   8129,   7528,  12211,   5004,   9351,   7160,   3478,
+	  4120,   1864,   9294,   5565,   5982,    702,    573,    474,
+	  5997,   3095,   9406,  11963,   2008,   4106,   1881,   7604,
+	  8793,   9204,  11609,  10311,   3061,   7422,   2592,    600,
+	  4480,  10140,     84,  10943,   3164,   2553,    981,  11492,
+	  5727,   9177,  10169,   1785,  10266,   5790,   1575,   5485,
+	  8184,    529,  11828,   5924,  11310,  10128,  11733,  11250,
+	  3516,  10372,   8361,   9104,   7706,   7018,   1527,   2743,
+	  4915,   5803,  10461,     32,    783,   9398,   1474,   7396,
+	  5120,   9833,     96,   5484,   3616,   9940,   9899,   7867,
+	  8765,   1460,   8229,   7708,   2734,  11784,   1741,   5751,
+	  5081,   6069,   4166,   7564,   5355,   6360,   7397,   9336,
+	  5806,   2937,   9172,   1668,   5483,   1383,     26,  10702,
+	  2106,   6632,   1422,  10570,   4406,   8985,  12218,   6697,
+	    29,   6265,  10523,   6646,  11311,   8649,   6587,   3004,
+	  9977,   3106,   1800,   4513,   6355,   2040,  10488,   9255,
+	  7659,   2797,   9898,   9346,   8251,  12037,  11138,   6447,
+	 11764,   2268,  10359,   3422,   9230,   1909,  11694,   7486,
+	  8378,   8539,   8913,   3770,   3920,   2728,   6218,   8039,
+	 11780,   3182,   1757,   6665,    639,   1172,   5158,   2787,
+	  3605,   1631,   5060,    261,   2162,   9831,   8182,   3487,
+	 11425,  12089,   9815,   9213,   9221,   2931,   8852,   7966,
+	 11962,   4362,  11438,   5151,   8909,   9686,   4545,     28,
+	 11662,   5658,   6824,   8862,   7161,   1999,   4205,  11328,
+	  3475,   9566,  10434,   3098,  12055,   1994,  12131,    191
+#elif Q == 18433
+	  4564,  17110,  12162,  16208,  10701,   9705,   3451,   5078,
+	 12400,  10202,   8245,  13131,   4631,   3492,  17179,   5622,
+	  5537,   3399,   2485,   9938,    345,  14064,  10152,    789,
+	  5092,  15713,  12632,   6516,  16107,   2314,  15385,  17281,
+	   383,   5515,   5019,  13218,   3293,   4728,   9704,  14263,
+	  4417,    218,  16011,   2568,   5635,   8516,  18352,  12887,
+	 13102,  15257,  14316,  12813,   8886,  11051,  13356,  15353,
+	 12059,   6880,  17926,  11710,   8052,   1737,  16384,  18094,
+	  3410,  14787,  13788,  14210,   2656,  17550,   7950,   4311,
+	 18150,   4973,  11548,   7848,  15326,  15517,     97,  11648,
+	 17990,  17685,  10847,  14695,    282,   1558,   6535,  10743,
+	  2399,    181,  10165,   8051,  12204,  18401,  13377,   7233,
+	 10892,  15728,  15002,  11766,   8462,  15245,  12420,   8613,
+	  5053,  12360,  17415,  12678,   4606,    870,   8429,   9572,
+	  6542,   1892,   4008,  17045,    371,  10155,    819,  15114,
+	  1522,  13638,  16576,  17586,  16840,   7671,  13873,  12065,
+	  7433,   7599,   2497,   5298,  16406,   3443,   9437,   6905,
+	 14589,  17851,    209,  17496,   1698,   7028,   4444,   8211,
+	  9159,  16089,  16741,   9085,   2658,   4488,   8650,   3995,
+	  6532,  11903,    508,    192,   4039,  17347,  12742,   6993,
+	 14812,  17645,   4527,    695,   8380,  16230,   2153,   3136,
+	  2133,   4725,   9230,  13213,   6548,  18005,   6108,  16097,
+	 16952,  13519,  16207,  12802,  16047,   7081,  12818,   8328,
+	 17091,   8927,   9558,   9273,   9301,  10337,  11142,   5082,
+	 11846,  15508,  17108,   8498,  16135,   3776,   6752,  12857,
+	  3590,    486,   3056,   4203,  10364,  17125,  14532,   3025,
+	 18386,  12029,   1983,   7426,  13553,    623,   6269,  15287,
+	 16399,  12294,   6987,   8011,   1378,  14019,   3042,   3472,
+	  4734,  12820,  16363,   7781,   3644,  16472,   3523,  14104,
+	 11521,  18288,  13956,   4549,   6314,  16320,  16373,  16203,
+	 15299,   7524,   9080,  15914,  17765,  12520,   5829,  13379,
+	  9482,   7938,    760,  13350,   9526,  15502,  16160,   6398,
+	  7067,   1655,   3428,   7827,  10564,   1235,  10800,   8291,
+	 15614,  14755,   8732,   3010,  12821,   7168,   8131,   1912,
+	  8093,  10461,  12301,  11616,  13947,   8029,  15138,   8334,
+	 13345,  13462,   7201,  11285,   8232,   5869,   5652,   8087,
+	  9232,    151,   5425,  15984,   9061,  10972,    874,   6136,
+	  9299,   4966,  10442,   5398,   6605,  14398,   7625,   7091,
+	 10974,  14743,   6836,  17243,    504,   7883,  10503,  12533,
+	  3111,  13658,   1303,   6153,  12791,    335,  16064,   6652,
+	 14432,  10970,    558,   5436,    300,  13031,  12835,   7899,
+	  8435,   7252,   2970,  12879,   2786,  16438,  16584,   2204,
+	  5974,   6467,   7971,  14624,   9637,   9448,  18144,   7293,
+	 18121,  10042,   1398,  12430,    157,   6881,  18084,  12060,
+	  5783,    444,  14853,   7936,  13337,  10411,   4401,  12549,
+	 17699,   1174,   1162,   5374,   3796,    709,   1424,   8521,
+	  2238,    991,   9114,  15056,   4900,  16221,    731,  18419,
+	 14885,   1707,  11644,   7594,  14783,   4281,  12810,   5277,
+	 10528,  15155,  16633,  13979,   5573,   7912,  15085,   4250,
+	 13224,  11094,   1717,  11970,   4689,  11787,    613,  14891,
+	  6892,   1734,  15910,  17044,   1022,  16497,   7473,   4421,
+	 17061,   2094,  17491,  14013,   1872,  13480,  10045,  17585,
+	  1562,  10460,  12143,  11266,   2168,  15769,   3047,   7683,
+	 14260,   9889,  14090,  14179,   4404,  11389,  11461,   4622,
+	 18349,  14047,   7466,  13272,   5005,  12487,    615,   1829,
+	 13229,  15305,   3467,  11180,   2855,   8191,   3868,   9735,
+	 18383,  13189,    933,   7900,  18340,  17527,   4316,  14694,
+	  5680,   9549,  15669,   5777,  17938,   7070,  11080,   4478,
+	  1466,  10714,  15409,   8001,   7888,   3707,  14283,   7140,
+	  3046,  14214,  15419,  16423,  18200,  10217,  10615,  18381,
+	 13138,   1337,   8483,   7125,   6741,  10966,  18359,   4036,
+	 11656,   2954,   5907,   1652,  12095,  11393,  12093,   6022,
+	 11472,   6513,  15239,  12291,  16914,   3635,   2907,    373,
+	 12897,   8503,  16298,   8337,  10348,  11023,   8932,   5553,
+	 12984,  11729,   9882,  13024,    556,     65,  10270,   4317,
+	 14404,   9508,   9191,   9860,  14257,  11049,  13040,  14653,
+	 13038,   9282,  10349,   4492,   6555,   9154,   8558,  14991,
+	  4583,   3619,    379,  13206,  11105,   7100,  15820,  14978,
+	  7277,  12620,   3196,  11513,   7268,  16100,     46,  12935,
+	 10191,   4142,   9281,  11926,  14900,  14340,  16894,   5224,
+	  9309,  13388,  13942,   3818,   2937,   7206,  14135,  15212,
+	  7925,   1689,   8800,   1294,   5524,  14570,  16368,  11992,
+	  9491,   4458,   3910,  11928,  13598,   1656,   3586,   8177,
+	 13056,   2322,  16649,   1648,  14699,  18328,   1843,    116,
+	 10016,   4221,   3330,   2710,   5358,  11169,  13567,   1354,
+	  8715,   3439,   8805,   5505,  10680,  17825,  14534,   8396,
+	  4185,   3904,   8543,   2358,  13314,  13160,  14784,  16183,
+	  3842,  13644,  17524,   1253,  13782,  16530,  12687,  15971,
+	 13700,  17515,   2420,  10494,   7049,   8615,  15561,  10671,
+	 10485,   1060,   1583,   2340,   6599,  16718,   5525,   8039,
+	 12196,  15350,  10577,   8497,  16786,  10118,  13406,   2164,
+	   696,   7375,   3971,    630,  13829,   4501,  10704,   8545,
+	  8124,  10763,   4718,   6718,  13636,  11540,  16886,   2173,
+	 13510,   4961,   9652,   3648,   3009,  16232,   2469,   3836,
+	  4933,   3461,  12281,  13205,  11756,  13442,   4041,   4285,
+	  3661,  16043,   9473,  11418,  13814,  10301,   5454,  10915,
+	  8727,  17232,  13005,   3609,   9965,   5508,   3913,  10768,
+	 11368,   3716,  15705,  10290,  10822,  12073,   8935,   4393,
+	  3878,  18157,  11691,  13998,  11637,  16445,  17690,   4654,
+	 12911,   9234,   2765,   6125,  12586,  12014,  18046,   2176,
+	 17540,   7355,    811,  12063,  17878,  11837,   8513,  13958,
+	 16653,  12390,   3722,   4745,   7749,   8299,   2499,  10669,
+	 16214,   3951,  15969,    375,  13937,  18040,  11638,   9914,
+	 16136,  15678,   7102,  12699,   9368,  15152,  16159,  12929,
+	 14186,  13925,   6623,   7438,   5741,  16684,    153,  14572,
+	 14261,   3358,  14440,  14021,  15097,  18043,  12112,  10964,
+	  5242,  13012,   9833,   1249,  16386,   5032,   2437,  10065,
+	  1738,   3850,     11,   1891,   3970,   7161,   7025,  17895,
+	  6303,  14429,  12523,  17941,   6931,   5087,  11127,  10882,
+	 13926,  16149,   7788,  11652,   8944,    913,  15223,   6189,
+	  9511,   2869,  10910,   8768,   6262,   5705,  16606,   5986,
+	 10784,   2189,  14068,  10397,  14897,  15500,  15844,   5698,
+	  5743,   3622,    853,  14256,   9576,   2313,  15227,  16931,
+	  3810,   1440,   6324,   6309,   3400,   6365,  10288,  15790,
+	 16146,   5667,  10602,  11119,   5700,   7960,   4236,   2617,
+	 12801,   8757,   1131,   5072,  16068,  17394,   1735,   5010,
+	  2908,  12275,   3985,   1361,  17206,  13615,  12942,   9536,
+	 12505,   6468,   8129,  14974,   2983,   1708,  11802,   7944,
+	 17712,   8436,   5712,   3320,  13774,  13479,   9887,  17235,
+	  4487,   3873,   3645,   9941,  16825,  13471,   8623,  14435,
+	  5656,    396,   7269,   9569,    935,  13271,  13889,  18167,
+	  6320,  14000,     40,  15255,   4382,   7607,   3761,   8098,
+	 15702,  11450,   2666,   7539,  13722,   2864,  10120,   7018,
+	 11627,   8023,  14190,   6234,  15359,   2757,  11647,   6434,
+	  1917,  14513,   7362,  10475,    985,     82,  12956,  10267,
+	 10798,   2920,    535,   8185,  17135,  16491,   6525,   2321,
+	 11245,  14410,   9521,  11291,   4326,   4683,   2594,  16946,
+	 12878,   3561,   9648,  11339,   9944,  13628,  14996,  14086,
+	 16837,   8831,  12823,  12539,   2930,  16057,  11685,  16318,
+	 11722,  14300,  10574,   9657,  17379,   8165,  18193,    635,
+	 17483,  10962,  17727,   2636,  16666,   1219,   8272,   2691,
+	 15755,  15534,   2783,  17598,   9028,   5299,   7757,  11350,
+	  9421,    803,  16276,   4555,   2408,  15134,  13315,   6629,
+	  2575,  12004,  16466,  17109,  14006,   9793,  17355,  17445,
+	  9993,   6970,  13713,   6344,  17481,   5591,  17027,   2952,
+	   268,    827,   1635,  12955,   8609,  13704,   8571,   3820,
+	 15205,  13149,  13046,  12333,   8005,  13766,  18367,   7087,
+	  5414,  14093,  14734,  10939,  12282,   6674,   3811,  13342
+#else
+#error Unknown modulus
+#endif
+};
+
+/*
+ * mq_iGM[x] = (2^31)*((1/g)^rev(x)) mod q.
+ * (Note: 2^31 instead of 2^32 because we want to pre-divide it by 2)
+ */
+ALIGNED_AVX2
+static const uint16_t Zq(iGM)[] = {
+#if Q == 12289
+	  5476,    553,   5310,    819,   1446,    348,   3386,   6271,
+	  9508,   3716,  11437,   5659,   5850,    694,   4775,   8339,
+	 12191,   2526,   2966,  11830,    405,   9123,   9311,   7289,
+	  8986,   5885,   8175,  10738,  10766,   8659,    700,   3024,
+	  6229,   8230,   8603,   4722,   5231,   6868,    436,   5816,
+	  8679,   6525,   8187,   3908,   7395,  12284,   1152,   7926,
+	  2586,   2815,   2741,  10858,  11383,  11816,    836,   7544,
+	 10666,   8227,  11752,   4562,    312,   6755,   4351,   7982,
+	  8158,  10173,    882,   1844,   4156,   2224,   8644,   3916,
+	 10619,    159,   5149,   8480,   2638,   5989,   1418,   8092,
+	  1775,   7668,    451,   3423,   1317,   6181,   8795,   6043,
+	  7283,   6393,  11564,   9157,  12161,   7312,   1366,   4918,
+	 11699,  12198,   1304,  11532,   6451,   4765,   8154,   4257,
+	  4191,   4833,   2318,  11980,  10393,   9997,   9481,    650,
+	 10594,     51,   7912,   2720,   9889,   1921,   7179,     45,
+	  3188,   8365,   2077,  11922,   5384,  11953,   8596,   6658,
+	 10981,   7130,   3974,   3404,  12177,   6398,  10412,   1231,
+	  8833,    800,     15,   9896,   5003,   1459,    565,  12272,
+	  9781,   1946,   1419,   9571,   3844,   7758,   4293,   8223,
+	 11525,    632,   4313,    936,  12186,   7420,  10892,  10678,
+	  8934,   2711,   9498,   1215,   4711,  11995,   1377,   8898,
+	 10189,   3217,  10890,   7720,   6923,   2380,   4653,  12236,
+	 10253,  11850,  10207,   5261,   1141,   3946,   7601,   9733,
+	 10630,   4139,   9832,   3641,  11245,   4338,   5765,  10158,
+	   116,  11807,  10283,   7064,    273,  10519,   2271,   3912,
+	  8424,  10339,   6876,   6601,  10079,    284,    927,   6954,
+	  3041,  12154,   6526,   5089,  12136,   7204,   4129,  11447,
+	 11079,   4604,   1008,   3863,  11772,   9564,   1101,   6231,
+	 10482,   6449,   6035,   3951,   1574,   5325,   2020,   1353,
+	  8191,   9824,   2642,  11905,   5399,   9560,   9396,  10114,
+	  8035,    302,   6611,   7914,  11812,   7279,  11427,   3158,
+	  6348,  12185,   6757,   2646,   5617,    179,    541,   1354,
+	  9642,   5278,  10391,   7039,   6801,   6277,   6339,  11163,
+	  2702,   2333,    735,   5633,  11656,  10046,   3107,  11456,
+	 12287,   9331,   8086,   1997,   4021,  11472,   1444,   9679,
+	 11720,   6390,   2424,   8997,   7242,   7199,   5281,   7084,
+	  7651,   9949,  10709,  10379,   9637,  10172,   6028,   5887,
+	  7701,  10165,   5183,   9610,   7424,   6019,   6795,   9692,
+	 10837,   3067,   8583,  12009,   6753,   9019,   3779,   9935,
+	  4732,   6187,   2497,   6363,  10289,   3649,  12127,   6182,
+	  5684,    960,     18,   2044,   1088,  11582,    678,   7353,
+	  7239,   2762,   5121,   3935,   2311,   1627,   8556,   8943,
+	  1541,   5674,    260,   3581,   4792,   8904,   5697,   7898,
+	  2155,   4394,    236,   4952,  11534,   1654,   4793,  10383,
+	  9769,   8776,    779,   9264,   3392,   2856,    668,   4852,
+	  8111,   2105,   6568,   5762,   6482,   1458,   5711,   4026,
+	   467,   2509,   4425,   6827,   1205,    290,   6918,   7274,
+	  3827,   7193,  11579,   6764,   4875,   8771,   1931,   4901,
+	  6494,   6917,   6351,   4333,   7020,  10664,   5730,   7549,
+	  4193,   7791,   6521,   9983,   6372,  10814,   8037,   3260,
+	   952,   7062,   9810,   7970,   3088,   7933,    840,   1171,
+	   486,   6032,   1342,   6289,   6017,   1907,   5489,   7491,
+	  7957,   7830,   2451,  12063,   8874,  12283,   6298,  11969,
+	  8735,   3326,   2981,   9437,   5408,  10582,   9876,   7272,
+	  2968,   2499,   6729,  10390,   5290,   8106,   7679,   2205,
+	  8744,   4348,   3461,   6595,   5747,   8114,   3378,   6728,
+	 10285,  10022,   3721,  10176,  10539,   4729,   9075,   2337,
+	  7445,    211,   7915,   7157,   5974,  12044,   7292,   7415,
+	  3824,   2756,  11419,   3615,   4762,   1401,   4097,    986,
+	  6496,   9875,  10554,   2336,   2999,  11481,   4286,  10159,
+	  7487,    884,  10155,   2087,   7556,   4623,   1546,    780,
+	 10199,   5718,   7327,  10024,  11396,   6465,   9722,    708,
+	 11199,  10038,   7408,   6933,   4003,   9428,    484,   3074,
+	  9409,   4763,   6157,     54,   2121,   3264,   2519,   2034,
+	  6049,     79,  11292,    117,  10740,   7072,   7506,   4407,
+	  6625,   4042,   5145,   2564,   7858,   8877,   9460,   6458,
+	 12275,   3872,   7446,   1690,   3569,   6570,  10108,   6308,
+	  8306,   7863,   4679,   1534,   1538,   1237,    100,    432,
+	  4401,   8198,   1229,  11208,   6014,   9759,   5329,   4342,
+	  4751,   9710,  11703,   5825,   2812,   5266,  10698,   6399,
+	  2125,   9180,  10925,  10329,  10404,   1688,   1875,   8100,
+	  8546,   6442,   5190,   7674,  10578,    965,  11155,   6407,
+	  2921,   6720,    126,   2019,   7616,   7340,   4746,   2315,
+	  1517,   7045,  11269,   2967,   3888,  11389,  10736,   1156,
+	 10787,   2851,   1820,    489,   8966,    883,   3012,   6130,
+	  2796,   6180,   1652,  10086,   7004,  11578,   8973,  11236,
+	  6938,  12276,   5453,   3403,  11455,   7703,   4676,   9386,
+	  7621,   2446,   9109,   3467,   8507,  10206,   3110,   3604,
+	  3269,   5274,   6397,  10922,   8435,   2030,  11559,   1762,
+	  2211,   1195,   7319,  10481,   9547,  12241,   1228,   9729,
+	  8591,  11552,   7590,   5753,  12273,    914,   3243,   3687,
+	  4773,   5381,   8780,   8436,   7737,   1964,   7103,  10531,
+	  6664,    278,   7225,   6634,   9327,   6375,   5880,   8197,
+	  3402,   5357,   9394,   7156,   5252,   1060,   1556,   3281,
+	  6543,   5654,   4868,  10707,    673,  12247,   7219,  10049,
+	 11989,  10993,   8578,   4614,    989,    340,   7687,   1748,
+	  8487,   5204,  10236,  11285,    163,   7586,   4597,   3146,
+	 12052,   5858,  11938,   9298,   3362,   7642,  11357,  10229,
+	 10550,   8709,   1469,   9787,     39,   8525,   2080,   4070,
+	  2959,   1477,   6249,    943,   4951,  10574,   1888,   2749,
+	  2190,   7003,   6199,    727,   8756,   9807,   4101,   6902,
+	  8605,   7680,    144,   4063,   8704,   6633,   5424,   9668,
+	  3253,   6188,   9640,   2320,   3736,   7783,  10822,   5460,
+	  9948,   3159,   2133,   8723,   6038,   8388,   6609,   4956,
+	  4659,   8821,   2700,  11664,   3443,   4551,   3388,   9229,
+	  4418,   8763,   6232,    378,   2558,  10559,   5344,   1949,
+	  3133,    754,   3240,  11539,  11505,   7919,  11439,   8617,
+	   386,   5600,    105,   7827,  10443,  10213,   3955,  12170,
+	  7022,   1333,   9933,   5552,   2330,   5150,   5473,   8405,
+	  6941,   4424,   5613,   6552,  11568,   2784,   2510,   1012,
+	  1093,   6688,   5041,   8505,   8399,  10231,   9639,    841,
+	  9878,  10230,   2496,   4884,  11594,   4371,   7993,  11918,
+	 10326,   9216,  10004,  12249,   7987,   3044,   4051,   6686,
+	   676,   4395,   7379,    909,   4981,   5788,   3488,   9661,
+	   812,   8915,  10536,    292,   1911,  12188,   3608,   2806,
+	  9812,  10928,  11265,   9340,   9108,   1988,   6489,  11811,
+	  8998,  11344,   8815,  11045,  11218,   1272,   4325,   6395,
+	  3819,   7650,   7056,   2463,   8670,   5503,   7707,   6750,
+	 11929,   8276,   5378,   3079,  11018,    408,   1851,   9471,
+	  8181,   7323,   6205,   9601,    926,   5475,   4327,   9353,
+	  7089,   2114,   9410,   6242,   8950,   1797,   6255,   9817,
+	  7569,  11561,  10432,   6233,   2452,   1253,   3787,   9478,
+	  7950,   9766,   6174,    619,   4514,   3279,  11352,   2834,
+	   599,   1113,  11465,  10204,   6177,   5056,   9926,   7488,
+	   136,   4520,   3157,  11672,   9219,   6400,    120,   5434,
+	  1825,   7884,   7214,   2654,  11393,   2028,   9562,   9848,
+	  8159,  11652,   9128,   6990,   8290,   8777,   7922,   5221,
+	  4759,   9253,   3937,  10126,  11306,   8534,   4922,   4550,
+	   424,    357,   6228,   6751,   7778,   1158,   1097,    315,
+	 10027,   9399,   2250,   9720,    821,   9937,  11016,   9739,
+	  6736,   8454,  11065,   8476,  12039,  11209,   3052,   3845,
+	 11597,   8808,   8153,   2778,   2609,  12254,   8064,   6326,
+	  5813,   7416,   6898,   2272,   5947,   8978,   5852,   3652,
+	   928,   8433,   8530,   7356,   2184,  10418,   5879,   6718,
+	 11603,   5393,   8473,   9076,   2835,   2416,   3732,   1867,
+	  1457,   4328,   8069,   1432,   1628,  11457,   4900,   8879,
+	  5111,   1434,   6325,   2746,   4083,   4858,   8847,   9217,
+	 10122,   2436,  11413,   7030,    303,   5733,   3871,  10824
+#elif Q == 18433
+	  2282,   9878,  10329,  12352,  15894,   7491,   4364,   3866,
+	 15622,    627,  16687,   6901,   2651,   5094,  13332,  12233,
+	   576,   1524,  17276,   1163,  15175,  12117,   1360,  15887,
+	  8822,  13357,  11401,   9044,  13464,   7974,   7517,   6448,
+	  9386,  10241,   8348,  14407,  12578,   9470,  14993,   3187,
+	  1540,  11755,   3691,  13990,   2810,  11275,   1588,  11882,
+	  2773,   9257,  14175,   6399,  17149,   1211,  18324,   7008,
+	  2085,  13581,  16069,   7570,  11824,   6707,   6459,   9025,
+	  3184,   2280,   5381,  10013,   9640,  10145,  11614,  17672,
+	 10876,   8807,   4139,   9031,    694,  16429,  17487,  15162,
+	 13647,   5002,  17998,  16130,  12094,    509,  12253,   6690,
+	  4910,  12223,   1594,  14202,  12550,  10932,  10569,  12987,
+	  5600,   2528,     16,  12331,   5191,   4134,   9126,   8017,
+	  3845,   5949,  17654,  18292,   1869,   3793,    374,   9438,
+	 12609,   9168,   1458,  10770,  14509,  12659,   6730,   9358,
+	  7061,  14458,   9658,  17105,  11328,  11539,   1823,  16728,
+	 15234,  10353,  10682,  13670,  11758,  18053,  14464,  13692,
+	  2527,   6302,  12173,    334,  10476,  13893,  14671,   1567,
+	  1115,   1030,  10273,  15276,   6942,  11455,   9289,   3456,
+	 11381,   7455,  10197,  16611,   5326,   1035,  12023,  16066,
+	 16697,  16912,   2207,  17744,   5211,   5723,  12286,   1017,
+	  1573,   6082,   8905,   2440,  14720,   8225,   3202,   9240,
+	  7704,  11167,    654,  13251,   7115,  16905,  18190,  16638,
+	  2788,  15057,  16545,   1149,  14184,   9879,  10679,  12510,
+	 15892,  12862,   4048,   4566,   4580,  13654,   4753,    671,
+	 14269,  12024,   5676,   1193,  12032,   1113,   2457,   9957,
+	  1168,  15379,    214,  15159,   2610,  13818,   6854,   8150,
+	 16865,   8140,  10318,  14243,   8869,   6953,    394,  11027,
+	  5720,  12062,    543,   7197,  18337,  18179,   3265,  15167,
+	  7219,  14108,  16189,  17104,   4674,    846,   1172,   4637,
+	  5111,  16211,  14919,  17584,   9685,   9112,    291,   1922,
+	  5764,   4498,   7495,  10230,  15784,   7968,   5417,   5500,
+	  6440,  13967,   3705,  13259,   5048,  10284,   4965,   2768,
+	  9030,   7763,   7399,   9976,   3071,   1597,   5960,  12697,
+	 15422,   3170,   3520,   3169,  17607,   6263,  16956,  12605,
+	 16415,     37,  12950,   5846,   5654,   4975,   8548,  11864,
+	    26,   3909,   4108,   9333,   1005,   1507,  11326,  16910,
+	 14863,   2075,   7363,  14489,   5216,   1512,  13076,  17700,
+	 16194,  12893,  14898,   9464,   6328,   1382,   4442,  15593,
+	 11086,  16275,    453,   9263,  14483,   8750,   2622,     25,
+	  4349,  16499,   5121,   7789,  12843,   7483,   1564,   2602,
+	  8302,   8909,   2973,   6714,  11797,  14700,   2193,     42,
+	 16122,   3486,   3522,  16231,   2127,  11388,   4272,  11303,
+	  5375,   7693,   1332,  17349,  12800,   3145,  13203,  17652,
+	   424,   4194,  11693,  17497,   2210,    471,  17386,    686,
+	  7006,   5480,    968,  17922,   9911,  10478,  17566,  14987,
+	  1771,   8910,   3323,   6872,  12448,   8358,  12886,  11821,
+	 16308,   1674,  14477,   6430,   2227,    900,   1639,  13169,
+	  6578,  12028,   7076,   1825,  14636,  12611,   8363,   1774,
+	     7,   8851,   1106,  15983,  10905,  13876,   8721,  17314,
+	  4956,  17721,   8862,  16535,  15746,  17852,  17846,    367,
+	  2942,   7016,   4011,   2548,  14465,   1790,  18211,   6325,
+	 12403,   9391,   5776,   9138,  12218,  17734,  13412,    156,
+	  5570,   9361,  13709,   4398,  11121,   5231,   5983,  15446,
+	 17331,  10141,  10214,  17040,   2777,  16948,  14807,   4999,
+	  5267,   2799,   2701,  18283,  15715,  18154,  12948,  11217,
+	 15107,  10401,   9049,   2821,   6140,   8565,  11604,   7661,
+	  2950,   3965,   5275,  18181,    595,  15015,   1845,  12946,
+	  5671,   5404,  11234,   5914,  15734,  13212,  15950,   4567,
+	 15365,  17996,  12947,   4686,  10441,   6504,   9141,  13817,
+	  5173,  15607,   6282,  14317,   3574,   5616,  11702,   2544,
+	 14266,  10864,   5202,   2243,  12625,   3066,   3986,   5170,
+	 17477,   5151,  14849,   2806,  16928,  14067,   1839,  10626,
+	  5071,  13033,   8599,  13151,   5303,  16719,   8389,   5683,
+	 11762,   7311,  15096,  12292,   3747,  11066,   2170,  15726,
+	  5673,     33,  11550,   5214,   3050,  11910,   2642,   1614,
+	 16523,   4931,  11581,   4912,   2739,   8399,   8803,  18299,
+	 16957,    703,   6421,    476,  15261,   2360,  14948,   4220,
+	   494,    539,   4320,  11430,    662,  10200,  12431,   7929,
+	  5902,   2559,  10866,  17229,   6939,  10295,   8815,   4506,
+	 12758,   5338,   6567,  13919,   9634,   7825,  10666,   1339,
+	  7871,  14297,   8607,  10100,  17115,    353,  12952,    475,
+	  8899,    120,   5134,    527,   4388,  13146,  11283,  12572,
+	 10274,   3374,   1188,  16968,   2947,   2805,   4801,    798,
+	 11390,  10935,  11619,  13461,   3547,  13609,   7436,  11994,
+	  9960,  17136,   6875,  16270,   3571,   4456,  11228,   3594,
+	  8056,   5954,    971,    649,   5124,   8949,  16973,  13034,
+	  4083,  11955,  18392,   8724,   3979,  14752,   1960,   8258,
+	 15216,   3393,   7838,   1537,  15316,  11338,   5205,   3403,
+	 14924,  13373,  17001,  11572,   5447,  17100,  12708,  10582,
+	 14384,   7336,   5413,  16242,   1589,  18413,  11433,  15273,
+	   133,   2272,   2581,   8749,   4432,   5582,  18235,  15605,
+	  1999,   4905,   2481,    804,   4246,   7394,   7280,   6973,
+	   599,   4273,   2477,  11546,  16773,  15577,  14215,   9577,
+	 14461,  12532,  17579,   7725,  10946,   5152,  15199,   2964,
+	 13665,  11962,   2409,   9830,   8536,   7224,   3079,  16979,
+	 15928,   8349,   9736,  10399,  15897,   8651,   4838,   2816,
+	  7908,  16315,  14453,  15583,   3657,  13132,   6383,  10360,
+	 10538,  13289,   6034,  16733,   6062,  15271,  17713,  16528,
+	   751,   1603,   8060,  13645,  11305,   8790,  16622,   6345,
+	 15584,  10511,  10683,   1768,   4018,  11399,   8122,  13041,
+	 15440,  10130,   6364,  15302,  14049,  12978,   7782,   4461,
+	  6122,   1605,   8760,  13961,  12607,  14539,   1142,  11470,
+	 12992,   3653,   6673,   5751,    246,   2955,   2002,   6065,
+	   269,   5704,   5636,  16448,   8271,   9211,  16508,  17564,
+	  4184,   7998,  15917,  10240,   8592,   4300,  11927,  15812,
+	 12951,  12377,    195,   1668,   2206,  11213,  16754,   2086,
+	 11147,   9140,  10091,   6346,  14714,   5905,   2254,  11340,
+	  2752,   1137,  10857,  13749,   2867,  14882,  10594,  10365,
+	 13476,  12614,   9413,   2248,   9029,   1232,   7241,  10326,
+	  3882,   7967,   5067,   5342,   6844,  16572,  12238,    890,
+	 11454,   4960,   3298,   9494,   3185,   8811,   5539,   9663,
+	 17345,   9410,  12426,  12140,   6154,   7834,  13816,   2761,
+	 16106,   9588,    994,   3398,  11434,   3371,    138,  16494,
+	  7020,   4749,   3180,  13022,  13288,   1364,  16575,  12749,
+	 13049,   7260,  15679,   4234,   7412,   2714,   9817,   4853,
+	  3759,  15706,   4066,  11526,  12724,   4480,   1195,   7386,
+	  7074,   7196,  11712,  12555,   2614,   3076,   7486,   6750,
+	 16515,   7982,  10317,   7712,  16609,  13607,   6736,  11678,
+	  8130,   9990,  12663,  11615,  15074,  16074,   3835,  14371,
+	  4944,  13081,   6966,   2302,  18118,   7231,   5529,  18085,
+	 17351,  11730,  13374,  10040,   4968,   3928,  10758,  12335,
+	  5197,   6454,  10074,   5917,  17263,   8425,  17903,   3974,
+	  3881,   1436,   4909,   5692,  13186,  17223,    459,  11583,
+	  1231,   2873,  10168,  11542,   8590,   9671,  11611,  16512,
+	  1125,  11041,  11853,  11776,  17254,   4945,  16481,   7124,
+	 14235,  11166,    304,  13093,   6464,   4814,   7497,   4859,
+	 17756,   2433,   3632,  15754,  17078,  16768,   7106,  13425,
+	 18375,   8295,   9269,   1867,  17609,    892,  17272,  11905,
+	  5128,  16640,  17605,  11634,  12469,  16478,  16204,   4471,
+	 12437,  10249,  11148,  15671,  17786,  14033,   8372,   5254,
+	 10827,   2149,  14830,   7748,  16524,  11462,  11739,   4562,
+	 15821,   9986,  11263,  10983,  12470,   4576,  16362,   4121,
+	  2749,  18410,  10383,  14799,   3460,  16835,  12123,   5578,
+	 10944,  10523,  14883,   3664,  11830,   9027,   7407,   6925,
+	  1721,  14154,  13856,   5939,  16187,   4042,  13792,  11914,
+	  1890,  11913,   3692,   2088,  13503,   4621,  13679,  11231,
+	  7058,  13298,   9184,  18155,  11921,  13492,   3352,  11941
+#else
+#error Unknown modulus
+#endif
+};
+
+#if HAWK_AVX2
+MQ_UNUSED TARGET_AVX2
+static void
+Zq(NTT32)(__m256i *a0, __m256i *a1, size_t k)
+{
+	__m256i yt1, yt2, yt3, yt4;
+
+	__m256i ya0 = *a0;
+	__m256i ya1 = *a1;
+
+	/* t = 32, m = 1 */
+	yt1 = ya0;
+	yt2 = Zq(montymul_x16)(ya1, _mm256_set1_epi16(Zq(GM)[k]));
+	ya0 = Zq(add_x16)(yt1, yt2);
+	ya1 = Zq(sub_x16)(yt1, yt2);
+
+	/* ya0:  0  1  2  3  4  5  6  7 |  8  9 10 11 12 13 14 15
+	   ya1: 16 17 18 19 20 21 22 23 | 24 25 26 27 28 29 30 31 */
+
+	/* t = 16, m = 2 */
+	yt1 = _mm256_permute2x128_si256(ya0, ya1, 0x20);
+	yt2 = _mm256_permute2x128_si256(ya0, ya1, 0x31);
+	uint16_t g1_0 = Zq(GM)[(k << 1) + 0];
+	uint16_t g1_1 = Zq(GM)[(k << 1) + 1];
+	__m256i yg1 = _mm256_setr_epi16(
+		g1_0, g1_0, g1_0, g1_0, g1_0, g1_0, g1_0, g1_0,
+		g1_1, g1_1, g1_1, g1_1, g1_1, g1_1, g1_1, g1_1);
+	yt2 = Zq(montymul_x16)(yt2, yg1);
+	ya0 = Zq(add_x16)(yt1, yt2);
+	ya1 = Zq(sub_x16)(yt1, yt2);
+
+	/* ya0:  0  1  2  3  4  5  6  7 | 16 17 18 19 20 21 22 23
+	   ya1:  8  9 10 11 12 13 14 15 | 24 25 26 27 28 29 30 31 */
+
+	/* t = 8, m = 4 */
+	yt1 = _mm256_unpacklo_epi64(ya0, ya1);
+	yt2 = _mm256_unpackhi_epi64(ya0, ya1);
+	__m256i yg2 = _mm256_setr_epi64x(
+		Zq(GM)[(k << 2) + 0], Zq(GM)[(k << 2) + 1],
+		Zq(GM)[(k << 2) + 2], Zq(GM)[(k << 2) + 3]);
+	yg2 = _mm256_or_si256(yg2, _mm256_slli_epi64(yg2, 32));
+	yg2 = _mm256_or_si256(yg2, _mm256_slli_epi32(yg2, 16));
+	yt2 = Zq(montymul_x16)(yt2, yg2);
+	ya0 = Zq(add_x16)(yt1, yt2);
+	ya1 = Zq(sub_x16)(yt1, yt2);
+
+	/* ya0:  0  1  2  3  8  9 10 11 | 16 17 18 19 24 25 26 27
+	   ya1:  4  5  6  7 12 13 14 15 | 20 21 22 23 28 29 30 31 */
+
+	/* t = 4, m = 8 */
+	yt3 = _mm256_shuffle_epi32(ya0, 0xD8);
+	yt4 = _mm256_shuffle_epi32(ya1, 0xD8);
+	yt1 = _mm256_unpacklo_epi32(yt3, yt4);
+	yt2 = _mm256_unpackhi_epi32(yt3, yt4);
+	__m256i yg3 = _mm256_cvtepi16_epi32(
+		_mm_loadu_si128((const __m128i *)&Zq(GM)[k << 3]));
+	yg3 = _mm256_or_si256(yg3, _mm256_slli_epi32(yg3, 16));
+	yt2 = Zq(montymul_x16)(yt2, yg3);
+	ya0 = Zq(add_x16)(yt1, yt2);
+	ya1 = Zq(sub_x16)(yt1, yt2);
+
+	/* ya0:  0  1  4  5  8  9 12 13 | 16 17 20 21 24 25 28 29
+	   ya1:  2  3  6  7 10 11 14 15 | 18 19 22 23 26 27 30 31 */
+
+	/* t = 2, m = 16 */
+	__m256i ysk = _mm256_setr_epi8(
+		0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15,
+		0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15);
+	yt3 = _mm256_shuffle_epi8(ya0, ysk);
+	yt4 = _mm256_shuffle_epi8(ya1, ysk);
+	yt1 = _mm256_unpacklo_epi16(yt3, yt4);
+	yt2 = _mm256_unpackhi_epi16(yt3, yt4);
+	yt2 = Zq(montymul_x16)(yt2,
+		_mm256_loadu_si256((const __m256i *)&Zq(GM)[k << 4]));
+	ya0 = Zq(add_x16)(yt1, yt2);
+	ya1 = Zq(sub_x16)(yt1, yt2);
+
+	/* ya0:  0  2  4  6  8 10 12 14 | 16 18 20 22 24 26 28 30
+	   ya1:  1  3  5  7  9 11 13 15 | 17 19 21 23 25 27 29 31 */
+	yt1 = _mm256_unpacklo_epi16(ya0, ya1);
+	yt2 = _mm256_unpackhi_epi16(ya0, ya1);
+	*a0 = _mm256_permute2x128_si256(yt1, yt2, 0x20);
+	*a1 = _mm256_permute2x128_si256(yt1, yt2, 0x31);
+}
+#endif // HAWK_AVX2
+
+MQ_UNUSED TARGET_AVX2
+static void
+Zq(NTT)(unsigned logn, uint16_t *restrict a)
+{
+#if HAWK_AVX2
+	switch (logn) {
+	case 1:
+	case 2:
+	case 3:
+	case 4:
+		/* We use the plain integer code for degrees < 32. */
+		break;
+	default: {
+		size_t n = (size_t)1 << logn;
+		size_t t = n;
+		for (unsigned lm = 0; lm < (logn - 5); lm ++) {
+			size_t m = (size_t)1 << lm;
+			size_t ht = t >> 1;
+			size_t v0 = 0;
+			for (size_t u = 0; u < m; u ++) {
+				__m256i ys = _mm256_set1_epi16(Zq(GM)[u + m]);
+				for (size_t v = 0; v < ht; v += 16) {
+					size_t k1 = v0 + v;
+					size_t k2 = k1 + ht;
+					__m256i *a1 = (__m256i *)(a + k1);
+					__m256i *a2 = (__m256i *)(a + k2);
+					__m256i y1 = _mm256_loadu_si256(a1);
+					__m256i y2 = _mm256_loadu_si256(a2);
+					y2 = Zq(montymul_x16)(y2, ys);
+					_mm256_storeu_si256(a1,
+						Zq(add_x16)(y1, y2));
+					_mm256_storeu_si256(a2,
+						Zq(sub_x16)(y1, y2));
+				}
+				v0 += t;
+			}
+			t = ht;
+		}
+		size_t m = n >> 5;
+		for (size_t u = 0; u < m; u ++) {
+			__m256i *pa = (__m256i *)(a + (u << 5));
+			__m256i ya0 = _mm256_loadu_si256(pa + 0);
+			__m256i ya1 = _mm256_loadu_si256(pa + 1);
+			Zq(NTT32)(&ya0, &ya1, u + m);
+			_mm256_storeu_si256(pa + 0, ya0);
+			_mm256_storeu_si256(pa + 1, ya1);
+		}
+		return;
+	}
+	}
+#endif // HAWK_AVX2
+	size_t t = (size_t)1 << logn;
+	for (unsigned lm = 0; lm < logn; lm ++) {
+		size_t m = (size_t)1 << lm;
+		size_t ht = t >> 1;
+		size_t v0 = 0;
+		for (size_t u = 0; u < m; u ++) {
+			uint32_t s = Zq(GM)[u + m];
+			for (size_t v = 0; v < ht; v ++) {
+				size_t k1 = v0 + v;
+				size_t k2 = k1 + ht;
+				uint32_t x1 = a[k1];
+				uint32_t x2 = Zq(montymul)(a[k2], s);
+				a[k1] = Zq(add)(x1, x2);
+				a[k2] = Zq(sub)(x1, x2);
+			}
+			v0 += t;
+		}
+		t = ht;
+	}
+}
+
+#if HAWK_AVX2
+MQ_UNUSED TARGET_AVX2
+static void
+Zq(iNTT32)(__m256i *a0, __m256i *a1, size_t k)
+{
+	__m256i yt1, yt2, yt3, yt4;
+
+	__m256i ya0 = *a0;
+	__m256i ya1 = *a1;
+
+	/* ya0:  0  1  2  3  4  5  6  7 |  8  9 10 11 12 13 14 15
+	   ya1: 16 17 18 19 20 21 22 23 | 24 25 26 27 28 29 30 31 */
+
+	yt1 = _mm256_permute2x128_si256(ya0, ya1, 0x20);
+	yt2 = _mm256_permute2x128_si256(ya0, ya1, 0x31);
+
+	/* yt1:  0  1  2  3  4  5  6  7 | 16 17 18 19 20 21 22 23
+	   yt2:  8  9 10 11 12 13 14 15 | 24 25 26 27 28 29 30 31 */
+
+	__m256i ysk = _mm256_setr_epi8(
+		0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15,
+		0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15);
+	yt3 = _mm256_shuffle_epi8(yt1, ysk);
+	yt4 = _mm256_shuffle_epi8(yt2, ysk);
+
+	/* yt3:  0  2  4  6  1  3  5  7 | 16 18 20 22 17 19 21 23
+	   yt4:  8 10 12 14  9 11 13 15 | 24 26 28 30 25 27 29 31 */
+
+	yt1 = _mm256_unpacklo_epi64(yt3, yt4);
+	yt2 = _mm256_unpackhi_epi64(yt3, yt4);
+	ya0 = Zq(half_x16)(Zq(add_x16)(yt1, yt2));
+	ya1 = Zq(montymul_x16)(Zq(sub_x16)(yt1, yt2),
+		_mm256_loadu_si256((const __m256i *)&Zq(iGM)[k << 4]));
+
+	/* ya0:  0  2  4  6  8 10 12 14 | 16 18 20 22 24 26 28 30
+	   ya1:  1  3  5  7  9 11 13 15 | 17 19 21 23 25 27 29 31 */
+
+	yt1 = _mm256_blend_epi16(ya0, _mm256_slli_epi32(ya1, 16), 0xAA);
+	yt2 = _mm256_blend_epi16(_mm256_srli_epi32(ya0, 16), ya1, 0xAA);
+	__m256i yig3 = _mm256_cvtepi16_epi32(
+		_mm_loadu_si128((const __m128i *)&Zq(iGM)[k << 3]));
+	yig3 = _mm256_or_si256(yig3, _mm256_slli_epi32(yig3, 16));
+	ya0 = Zq(half_x16)(Zq(add_x16)(yt1, yt2));
+	ya1 = Zq(montymul_x16)(Zq(sub_x16)(yt1, yt2), yig3);
+
+	/* ya0:  0  1  4  5  8  9 12 13 | 16 17 20 21 24 25 28 29
+	   ya1:  2  3  6  7 10 11 14 15 | 18 19 22 23 26 27 30 31 */
+
+	yt1 = _mm256_blend_epi16(ya0, _mm256_slli_epi64(ya1, 32), 0xCC);
+	yt2 = _mm256_blend_epi16(_mm256_srli_epi64(ya0, 32), ya1, 0xCC);
+	__m256i yig2 = _mm256_setr_epi64x(
+		Zq(iGM)[(k << 2) + 0], Zq(iGM)[(k << 2) + 1],
+		Zq(iGM)[(k << 2) + 2], Zq(iGM)[(k << 2) + 3]);
+	yig2 = _mm256_or_si256(yig2, _mm256_slli_epi64(yig2, 32));
+	yig2 = _mm256_or_si256(yig2, _mm256_slli_epi32(yig2, 16));
+	ya0 = Zq(half_x16)(Zq(add_x16)(yt1, yt2));
+	ya1 = Zq(montymul_x16)(Zq(sub_x16)(yt1, yt2), yig2);
+
+	/* ya0:  0  1  2  3  8  9 10 11 | 16 17 18 19 24 25 26 27
+	   ya1:  4  5  6  7 12 13 14 15 | 20 21 22 23 28 29 30 31 */
+
+	yt1 = _mm256_unpacklo_epi64(ya0, ya1);
+	yt2 = _mm256_unpackhi_epi64(ya0, ya1);
+	uint16_t ig1_0 = Zq(iGM)[(k << 1) + 0];
+	uint16_t ig1_1 = Zq(iGM)[(k << 1) + 1];
+	__m256i yig1 = _mm256_setr_epi16(
+		ig1_0, ig1_0, ig1_0, ig1_0, ig1_0, ig1_0, ig1_0, ig1_0,
+		ig1_1, ig1_1, ig1_1, ig1_1, ig1_1, ig1_1, ig1_1, ig1_1);
+	ya0 = Zq(half_x16)(Zq(add_x16)(yt1, yt2));
+	ya1 = Zq(montymul_x16)(Zq(sub_x16)(yt1, yt2), yig1);
+
+	/* ya0:  0  1  2  3  4  5  6  7 | 16 17 18 19 20 21 22 23
+	   ya1:  8  9 10 11 12 13 14 15 | 24 25 26 27 28 29 30 31 */
+
+	yt1 = _mm256_permute2x128_si256(ya0, ya1, 0x20);
+	yt2 = _mm256_permute2x128_si256(ya0, ya1, 0x31);
+	__m256i yig0 = _mm256_set1_epi16(Zq(iGM)[k]);
+	ya0 = Zq(half_x16)(Zq(add_x16)(yt1, yt2));
+	ya1 = Zq(montymul_x16)(Zq(sub_x16)(yt1, yt2), yig0);
+
+	/* ya0:  0  1  2  3  4  5  6  7 |  8  9 10 11 12 13 14 15
+	   ya1: 16 17 18 19 20 21 22 23 | 24 25 26 27 28 29 30 31 */
+
+	*a0 = ya0;
+	*a1 = ya1;
+}
+#endif // HAWK_AVX2
+
+MQ_UNUSED TARGET_AVX2
+static void
+Zq(iNTT)(unsigned logn, uint16_t *restrict a)
+{
+#if HAWK_AVX2
+	switch (logn) {
+	case 1:
+	case 2:
+	case 3:
+	case 4:
+		/* We use the plain integer code for degrees < 32. */
+		break;
+	default: {
+		size_t n = (size_t)1 << logn;
+		size_t m = n >> 5;
+		for (size_t u = 0; u < m; u ++) {
+			__m256i *pa = (__m256i *)(a + (u << 5));
+			__m256i ya0 = _mm256_loadu_si256(pa + 0);
+			__m256i ya1 = _mm256_loadu_si256(pa + 1);
+			Zq(iNTT32)(&ya0, &ya1, u + m);
+			_mm256_storeu_si256(pa + 0, ya0);
+			_mm256_storeu_si256(pa + 1, ya1);
+		}
+		size_t t = 32;
+		for (unsigned lm = 5; lm < logn; lm ++) {
+			size_t hm = (size_t)1 << (logn - 1 - lm);
+			size_t dt = t << 1;
+			size_t v0 = 0;
+			for (size_t u = 0; u < hm; u ++) {
+				__m256i ys = _mm256_set1_epi16(Zq(iGM)[u + hm]);
+				for (size_t v = 0; v < t; v += 16) {
+					size_t k1 = v0 + v;
+					size_t k2 = k1 + t;
+					__m256i *a1 = (__m256i *)(a + k1);
+					__m256i *a2 = (__m256i *)(a + k2);
+					__m256i y1 = _mm256_loadu_si256(a1);
+					__m256i y2 = _mm256_loadu_si256(a2);
+					_mm256_storeu_si256(a1,
+						Zq(half_x16)(
+							Zq(add_x16)(y1, y2)));
+					_mm256_storeu_si256(a2,
+						Zq(montymul_x16)(ys,
+							Zq(sub_x16)(y1, y2)));
+				}
+				v0 += dt;
+			}
+			t = dt;
+		}
+		return;
+	}
+	}
+#endif // HAWK_AVX2
+	size_t t = 1;
+	for (unsigned lm = 0; lm < logn; lm ++) {
+		size_t hm = (size_t)1 << (logn - 1 - lm);
+		size_t dt = t << 1;
+		size_t v0 = 0;
+		for (size_t u = 0; u < hm; u ++) {
+			uint32_t s = Zq(iGM)[u + hm];
+			for (size_t v = 0; v < t; v ++) {
+				size_t k1 = v0 + v;
+				size_t k2 = k1 + t;
+				uint32_t x1 = a[k1];
+				uint32_t x2 = a[k2];
+				a[k1] = Zq(half)(Zq(add)(x1, x2));
+				a[k2] = Zq(montymul)(s, Zq(sub)(x1, x2));
+			}
+			v0 += dt;
+		}
+		t = dt;
+	}
+}
+
+MQ_UNUSED TARGET_AVX2
+static inline void
+Zq(poly_set_small)(unsigned logn, uint16_t *d, const int8_t *a)
+{
+	size_t n = (size_t)1 << logn;
+#if HAWK_AVX2
+	if (logn >= 4) {
+		for (size_t u = 0; u < n; u += 16) {
+			__m128i xa = _mm_loadu_si128((const __m128i *)(a + u));
+			__m256i ya = _mm256_cvtepi8_epi16(xa);
+			__m256i yd = Zq(set_small_x16(ya));
+			_mm256_storeu_si256((__m256i *)(d + u), yd);
+		}
+		return;
+	}
+#endif // HAWK_AVX2
+	for (size_t u = 0; u < n; u ++) {
+		d[u] = Zq(set_small)(a[u]);
+	}
+}
+
+/*
+ * Convert a small polynomial (signed 8-bit coefficients) to a mod q
+ * representation that _starts_ at the same address (i.e. the n first
+ * bytes of d are read, and 2*n bytes are written into d).
+ */
+MQ_UNUSED TARGET_AVX2
+static inline void
+Zq(poly_set_small_inplace_low)(unsigned logn, uint16_t *d)
+{
+	size_t n = (size_t)1 << logn;
+#if HAWK_AVX2
+	if (logn >= 4) {
+		size_t u = n;
+		while (u > 0) {
+			u -= 16;
+			__m128i xa = _mm_loadu_si128((__m128i *)&d[u >> 1]);
+			__m256i ya = _mm256_cvtepi8_epi16(xa);
+			__m256i yd = Zq(set_small_x16(ya));
+			_mm256_storeu_si256((__m256i *)&d[u], yd);
+		}
+		return;
+	}
+#endif // HAWK_AVX2
+	size_t u = n;
+	while (u > 0) {
+		u -= 2;
+		uint32_t x = d[u >> 1];
+		uint8_t x0 = (uint8_t)x;
+		uint8_t x1 = (uint8_t)(x >> 8);
+		d[u + 0] = Zq(set_small)(*(int8_t *)&x0);
+		d[u + 1] = Zq(set_small)(*(int8_t *)&x1);
+	}
+}
+
+/*
+ * Convert a small polynomial (signed 8-bit coefficients) to a mod q
+ * representation that _ends_ at the same address (i.e. the n last
+ * bytes of d are read, and 2*n bytes are written into d).
+ */
+MQ_UNUSED TARGET_AVX2
+static inline void
+Zq(poly_set_small_inplace_high)(unsigned logn, uint16_t *d)
+{
+	size_t n = (size_t)1 << logn;
+	size_t hn = n >> 1;
+#if HAWK_AVX2
+	if (logn >= 4) {
+		for (size_t u = 0; u < n; u += 16) {
+			__m128i xa = _mm_loadu_si128(
+				(const __m128i *)&d[hn + (u >> 1)]);
+			__m256i ya = _mm256_cvtepi8_epi16(xa);
+			__m256i yd = Zq(set_small_x16(ya));
+			_mm256_storeu_si256((__m256i *)&d[u], yd);
+		}
+		return;
+	}
+#endif // HAWK_AVX2
+	for (size_t u = 0; u < n; u += 2) {
+		uint32_t x = d[hn + (u >> 1)];
+		uint8_t x0 = (uint8_t)x;
+		uint8_t x1 = (uint8_t)(x >> 8);
+		d[u + 0] = Zq(set_small)(*(int8_t *)&x0);
+		d[u + 1] = Zq(set_small)(*(int8_t *)&x1);
+	}
+}
+
+MQ_UNUSED TARGET_AVX2
+static inline void
+Zq(poly_snorm)(unsigned logn, uint16_t *d)
+{
+	size_t n = (size_t)1 << logn;
+#if HAWK_AVX2
+	if (logn >= 4) {
+		for (size_t u = 0; u < n; u += 16) {
+			__m256i y = _mm256_loadu_si256((__m256i *)(d + u));
+			y = Zq(snorm_x16)(y);
+			_mm256_storeu_si256((__m256i *)(d + u), y);
+		}
+		return;
+	}
+#endif // HAWK_AVX2
+	for (size_t u = 0; u < n; u ++) {
+		d[u] = (uint16_t)Zq(snorm)(d[u]);
+	}
+}
+
+#undef Q
+#undef Q0I
+#undef R
+#undef R2
+#undef MSF
+#undef Q0Ilo
+#undef Q0Ihi
+#undef Zq
+#undef MQ_UNUSED
diff --git a/lib/dns/hawk/ng_config.h b/lib/dns/hawk/ng_config.h
new file mode 100644
index 0000000000..8e3f16ba53
--- /dev/null
+++ b/lib/dns/hawk/ng_config.h
@@ -0,0 +1,30 @@
+/*
+ * Compile-time configuration for the NTRUGEN code. This ensures that
+ * the keygen code (imported from NTRUGEN) uses the same options as
+ * the rest of the Hawk code.
+ */
+
+#ifndef NG_CONFIG_H__
+#define NG_CONFIG_H__
+
+#include "hawk_config.h"
+
+/*
+ * If the prefix for Hawk code is 'xxx', then all NTRUGEN functions will
+ * use the 'xxx_ntrugen' prefix. Default prefix for Hawk is 'hawk'.
+ */
+#ifdef HAWK_PREFIX
+#define NTRUGEN_PREFIX           NTRUGEN_PREFIX_(HAWK_PREFIX, _ntrugen)
+#define NTRUGEN_PREFIX_(x, y)    NTRUGEN_PREFIX__(x, y)
+#define NTRUGEN_PREFIX__(x, y)   x ## y
+#else
+#define NTRUGEN_PREFIX   hawk_ntrugen
+#endif
+
+#ifndef NTRUGEN_AVX2
+#if defined HAWK_AVX2
+#define NTRUGEN_AVX2   HAWK_AVX2
+#endif
+#endif // NTRUGEN_AVX2
+
+#endif
diff --git a/lib/dns/hawk/ng_fxp.c b/lib/dns/hawk/ng_fxp.c
new file mode 100644
index 0000000000..c01ea9503a
--- /dev/null
+++ b/lib/dns/hawk/ng_fxp.c
@@ -0,0 +1,1829 @@
+#include "ng_inner.h"
+
+/* see ng_inner.h */
+uint64_t
+inner_fxr_div(uint64_t x, uint64_t y)
+{
+	/*
+	 * Get absolute values and signs. From now on, we can suppose
+	 * that x and y fit on 63 bits (we ignore edge conditions).
+	 */
+	uint64_t sx = x >> 63;
+	x = (x ^ -sx) + sx;
+	uint64_t sy = y >> 63;
+	y = (y ^ -sy) + sy;
+
+	/*
+	 * Do a bit by bit division, assuming that the quotient fits.
+	 * The numerator starts at x*2^31, and is shifted one bit a time.
+	 */
+	uint64_t q = 0;
+	uint64_t num = x >> 31;
+	for (int i = 63; i >= 33; i --) {
+		uint64_t b = 1 - ((num - y) >> 63);
+		q |= b << i;
+		num -= y & -b;
+		num <<= 1;
+		num |= (x >> (i - 33)) & 1;
+	}
+	for (int i = 32; i >= 0; i --) {
+		uint64_t b = 1 - ((num - y) >> 63);
+		q |= b << i;
+		num -= y & -b;
+		num <<= 1;
+	}
+
+	/*
+	 * Rounding: if the remainder is at least y/2 (scaled), we add
+	 * 2^(-32) to the quotient.
+	 */
+	uint64_t b = 1 - ((num - y) >> 63);
+	q += b;
+
+	/*
+	 * Sign management: if the original x and y had different signs,
+	 * then we must negate the quotient.
+	 */
+	sx ^= sy;
+	q = (q ^ -sx) + sx;
+
+	return q;
+}
+
+#if NTRUGEN_AVX2
+/* see inner.h */
+TARGET_AVX2
+__m256i
+fxr_div_x4(__m256i yn, __m256i yd)
+{
+	/*
+	 * Get absolute values and signs. From now on, we can suppose
+	 * that n and d fit on 63 bits (we ignore edge conditions).
+	 */
+	__m256i ysn = _mm256_sub_epi64(_mm256_setzero_si256(),
+		_mm256_srli_epi64(yn, 63));
+	__m256i ysd = _mm256_sub_epi64(_mm256_setzero_si256(),
+		_mm256_srli_epi64(yd, 63));
+	yn = _mm256_sub_epi64(_mm256_xor_si256(yn, ysn), ysn);
+	yd = _mm256_sub_epi64(_mm256_xor_si256(yd, ysd), ysd);
+
+	/*
+	 * Do a bit by bit division, assuming that the quotient fits.
+	 * The numerator starts at n*2^31, and is shifted one bit a time.
+	 */
+	__m256i yq = _mm256_setzero_si256();
+	__m256i ynum = _mm256_srli_epi64(yn, 31);
+	__m256i y1 = _mm256_set1_epi64x(1);
+	for (int i = 63; i >= 33; i --) {
+		__m256i yb = _mm256_srli_epi64(_mm256_sub_epi64(ynum, yd), 63);
+		__m256i yc = _mm256_sub_epi64(yb, y1);
+		yq = _mm256_add_epi64(yq, yq);
+		yq = _mm256_sub_epi64(yq, yc);
+		ynum = _mm256_sub_epi64(ynum, _mm256_and_si256(yc, yd));
+		ynum = _mm256_add_epi64(ynum, ynum);
+		ynum = _mm256_or_si256(ynum,
+			_mm256_and_si256(_mm256_srli_epi64(yn, 30), y1));
+		yn = _mm256_add_epi64(yn, yn);
+	}
+	for (int i = 32; i >= 0; i --) {
+		__m256i yb = _mm256_srli_epi64(_mm256_sub_epi64(ynum, yd), 63);
+		__m256i yc = _mm256_sub_epi64(yb, y1);
+		yq = _mm256_add_epi64(yq, yq);
+		yq = _mm256_sub_epi64(yq, yc);
+		ynum = _mm256_sub_epi64(ynum, _mm256_and_si256(yc, yd));
+		ynum = _mm256_add_epi64(ynum, ynum);
+	}
+
+	/*
+	 * Rounding: if the remainder is at least d/2 (scaled), we add
+	 * 2^(-32) to the quotient.
+	 */
+	__m256i yb0 = _mm256_srli_epi64(_mm256_sub_epi64(ynum, yd), 63);
+	yq = _mm256_add_epi64(_mm256_xor_si256(y1, yb0), yq);
+
+	/*
+	 * Sign management: if the original yn and yd had different signs,
+	 * then we must negate the quotient.
+	 */
+	ysn = _mm256_xor_si256(ysn, ysd);
+	yq = _mm256_sub_epi64(_mm256_xor_si256(yq, ysn), ysn);
+
+	return yq;
+}
+#endif // NTRUGEN_AVX2
+
+/*
+ * Primitive 2048-th roots of unity, in the proper order for FFT.
+ */
+static const fxc GM_TAB[1024] = {
+	FXC(          4294967296ull,                    0ull),
+	FXC(                   0ull,           4294967296ull),
+	FXC(          3037000500ull,           3037000500ull),
+	FXC(18446744070672551116ull,           3037000500ull),
+	FXC(          3968032378ull,           1643612827ull),
+	FXC(18446744072065938789ull,           3968032378ull),
+	FXC(          1643612827ull,           3968032378ull),
+	FXC(18446744069741519238ull,           1643612827ull),
+	FXC(          4212440704ull,            837906553ull),
+	FXC(18446744072871645063ull,           4212440704ull),
+	FXC(          2386155981ull,           3571134792ull),
+	FXC(18446744070138416824ull,           2386155981ull),
+	FXC(          3571134792ull,           2386155981ull),
+	FXC(18446744071323395635ull,           3571134792ull),
+	FXC(           837906553ull,           4212440704ull),
+	FXC(18446744069497110912ull,            837906553ull),
+	FXC(          4274285855ull,            420980412ull),
+	FXC(18446744073288571204ull,           4274285855ull),
+	FXC(          2724698408ull,           3320054617ull),
+	FXC(18446744070389496999ull,           2724698408ull),
+	FXC(          3787822988ull,           2024633568ull),
+	FXC(18446744071684918048ull,           3787822988ull),
+	FXC(          1246763195ull,           4110027446ull),
+	FXC(18446744069599524170ull,           1246763195ull),
+	FXC(          4110027446ull,           1246763195ull),
+	FXC(18446744072462788421ull,           4110027446ull),
+	FXC(          2024633568ull,           3787822988ull),
+	FXC(18446744069921728628ull,           2024633568ull),
+	FXC(          3320054617ull,           2724698408ull),
+	FXC(18446744070984853208ull,           3320054617ull),
+	FXC(           420980412ull,           4274285855ull),
+	FXC(18446744069435265761ull,            420980412ull),
+	FXC(          4289793820ull,            210744057ull),
+	FXC(18446744073498807559ull,           4289793820ull),
+	FXC(          2884323748ull,           3182360851ull),
+	FXC(18446744070527190765ull,           2884323748ull),
+	FXC(          3882604450ull,           1836335144ull),
+	FXC(18446744071873216472ull,           3882604450ull),
+	FXC(          1446930903ull,           4043900968ull),
+	FXC(18446744069665650648ull,           1446930903ull),
+	FXC(          4166252509ull,           1043591926ull),
+	FXC(18446744072665959690ull,           4166252509ull),
+	FXC(          2208054473ull,           3683916329ull),
+	FXC(18446744070025635287ull,           2208054473ull),
+	FXC(          3449750080ull,           2558509031ull),
+	FXC(18446744071151042585ull,           3449750080ull),
+	FXC(           630202589ull,           4248480760ull),
+	FXC(18446744069461070856ull,            630202589ull),
+	FXC(          4248480760ull,            630202589ull),
+	FXC(18446744073079349027ull,           4248480760ull),
+	FXC(          2558509031ull,           3449750080ull),
+	FXC(18446744070259801536ull,           2558509031ull),
+	FXC(          3683916329ull,           2208054473ull),
+	FXC(18446744071501497143ull,           3683916329ull),
+	FXC(          1043591926ull,           4166252509ull),
+	FXC(18446744069543299107ull,           1043591926ull),
+	FXC(          4043900968ull,           1446930903ull),
+	FXC(18446744072262620713ull,           4043900968ull),
+	FXC(          1836335144ull,           3882604450ull),
+	FXC(18446744069826947166ull,           1836335144ull),
+	FXC(          3182360851ull,           2884323748ull),
+	FXC(18446744070825227868ull,           3182360851ull),
+	FXC(           210744057ull,           4289793820ull),
+	FXC(18446744069419757796ull,            210744057ull),
+	FXC(          4293673732ull,            105403774ull),
+	FXC(18446744073604147842ull,           4293673732ull),
+	FXC(          2961554089ull,           3110617535ull),
+	FXC(18446744070598934081ull,           2961554089ull),
+	FXC(          3926501002ull,           1740498191ull),
+	FXC(18446744071969053425ull,           3926501002ull),
+	FXC(          1545737412ull,           4007173558ull),
+	FXC(18446744069702378058ull,           1545737412ull),
+	FXC(          4190608739ull,            941032661ull),
+	FXC(18446744072768518955ull,           4190608739ull),
+	FXC(          2297797281ull,           3628618433ull),
+	FXC(18446744070080933183ull,           2297797281ull),
+	FXC(          3511500034ull,           2473077351ull),
+	FXC(18446744071236474265ull,           3511500034ull),
+	FXC(           734275721ull,           4231735252ull),
+	FXC(18446744069477816364ull,            734275721ull),
+	FXC(          4262667143ull,            525749847ull),
+	FXC(18446744073183801769ull,           4262667143ull),
+	FXC(          2642399561ull,           3385922125ull),
+	FXC(18446744070323629491ull,           2642399561ull),
+	FXC(          3736995171ull,           2116981616ull),
+	FXC(18446744071592570000ull,           3736995171ull),
+	FXC(          1145522571ull,           4139386683ull),
+	FXC(18446744069570164933ull,           1145522571ull),
+	FXC(          4078192482ull,           1347252816ull),
+	FXC(18446744072362298800ull,           4078192482ull),
+	FXC(          1931065957ull,           3836369162ull),
+	FXC(18446744069873182454ull,           1931065957ull),
+	FXC(          3252187232ull,           2805355999ull),
+	FXC(18446744070904195617ull,           3252187232ull),
+	FXC(           315957395ull,           4283329896ull),
+	FXC(18446744069426221720ull,            315957395ull),
+	FXC(          4283329896ull,            315957395ull),
+	FXC(18446744073393594221ull,           4283329896ull),
+	FXC(          2805355999ull,           3252187232ull),
+	FXC(18446744070457364384ull,           2805355999ull),
+	FXC(          3836369162ull,           1931065957ull),
+	FXC(18446744071778485659ull,           3836369162ull),
+	FXC(          1347252816ull,           4078192482ull),
+	FXC(18446744069631359134ull,           1347252816ull),
+	FXC(          4139386683ull,           1145522571ull),
+	FXC(18446744072564029045ull,           4139386683ull),
+	FXC(          2116981616ull,           3736995171ull),
+	FXC(18446744069972556445ull,           2116981616ull),
+	FXC(          3385922125ull,           2642399561ull),
+	FXC(18446744071067152055ull,           3385922125ull),
+	FXC(           525749847ull,           4262667143ull),
+	FXC(18446744069446884473ull,            525749847ull),
+	FXC(          4231735252ull,            734275721ull),
+	FXC(18446744072975275895ull,           4231735252ull),
+	FXC(          2473077351ull,           3511500034ull),
+	FXC(18446744070198051582ull,           2473077351ull),
+	FXC(          3628618433ull,           2297797281ull),
+	FXC(18446744071411754335ull,           3628618433ull),
+	FXC(           941032661ull,           4190608739ull),
+	FXC(18446744069518942877ull,            941032661ull),
+	FXC(          4007173558ull,           1545737412ull),
+	FXC(18446744072163814204ull,           4007173558ull),
+	FXC(          1740498191ull,           3926501002ull),
+	FXC(18446744069783050614ull,           1740498191ull),
+	FXC(          3110617535ull,           2961554089ull),
+	FXC(18446744070747997527ull,           3110617535ull),
+	FXC(           105403774ull,           4293673732ull),
+	FXC(18446744069415877884ull,            105403774ull),
+	FXC(          4294643893ull,             52705856ull),
+	FXC(18446744073656845760ull,           4294643893ull),
+	FXC(          2999503152ull,           3074040487ull),
+	FXC(18446744070635511129ull,           2999503152ull),
+	FXC(          3947563934ull,           1692182927ull),
+	FXC(18446744072017368689ull,           3947563934ull),
+	FXC(          1594795204ull,           3987903250ull),
+	FXC(18446744069721648366ull,           1594795204ull),
+	FXC(          4201841112ull,            889536587ull),
+	FXC(18446744072820015029ull,           4201841112ull),
+	FXC(          2342152991ull,           3600147697ull),
+	FXC(18446744070109403919ull,           2342152991ull),
+	FXC(          3541584088ull,           2429799626ull),
+	FXC(18446744071279751990ull,           3541584088ull),
+	FXC(           786150333ull,           4222405917ull),
+	FXC(18446744069487145699ull,            786150333ull),
+	FXC(          4268797931ull,            473400776ull),
+	FXC(18446744073236150840ull,           4268797931ull),
+	FXC(          2683751066ull,           3353240863ull),
+	FXC(18446744070356310753ull,           2683751066ull),
+	FXC(          3762692404ull,           2070963532ull),
+	FXC(18446744071638588084ull,           3762692404ull),
+	FXC(          1196232957ull,           4125017671ull),
+	FXC(18446744069584533945ull,           1196232957ull),
+	FXC(          4094418266ull,           1297105676ull),
+	FXC(18446744072412445940ull,           4094418266ull),
+	FXC(          1977998702ull,           3812383140ull),
+	FXC(18446744069897168476ull,           1977998702ull),
+	FXC(          3286368382ull,           2765235421ull),
+	FXC(18446744070944316195ull,           3286368382ull),
+	FXC(           368496651ull,           4279130086ull),
+	FXC(18446744069430421530ull,            368496651ull),
+	FXC(          4286884652ull,            263370557ull),
+	FXC(18446744073446181059ull,           4286884652ull),
+	FXC(          2845054101ull,           3217516315ull),
+	FXC(18446744070492035301ull,           2845054101ull),
+	FXC(          3859777440ull,           1883842400ull),
+	FXC(18446744071825709216ull,           3859777440ull),
+	FXC(          1397197066ull,           4061352537ull),
+	FXC(18446744069648199079ull,           1397197066ull),
+	FXC(          4153132319ull,           1094639673ull),
+	FXC(18446744072614911943ull,           4153132319ull),
+	FXC(          2162680890ull,           3710735162ull),
+	FXC(18446744069998816454ull,           2162680890ull),
+	FXC(          3418093478ull,           2600650120ull),
+	FXC(18446744071108901496ull,           3418093478ull),
+	FXC(           578019742ull,           4255894413ull),
+	FXC(18446744069453657203ull,            578019742ull),
+	FXC(          4240427302ull,            682290530ull),
+	FXC(18446744073027261086ull,           4240427302ull),
+	FXC(          2515982640ull,           3480887161ull),
+	FXC(18446744070228664455ull,           2515982640ull),
+	FXC(          3656542712ull,           2253095531ull),
+	FXC(18446744071456456085ull,           3656542712ull),
+	FXC(           992387019ull,           4178745276ull),
+	FXC(18446744069530806340ull,            992387019ull),
+	FXC(          4025840401ull,           1496446837ull),
+	FXC(18446744072213104779ull,           4025840401ull),
+	FXC(          1788551342ull,           3904846754ull),
+	FXC(18446744069804704862ull,           1788551342ull),
+	FXC(          3146726136ull,           2923159027ull),
+	FXC(18446744070786392589ull,           3146726136ull),
+	FXC(           158085819ull,           4292056960ull),
+	FXC(18446744069417494656ull,            158085819ull),
+	FXC(          4292056960ull,            158085819ull),
+	FXC(18446744073551465797ull,           4292056960ull),
+	FXC(          2923159027ull,           3146726136ull),
+	FXC(18446744070562825480ull,           2923159027ull),
+	FXC(          3904846754ull,           1788551342ull),
+	FXC(18446744071921000274ull,           3904846754ull),
+	FXC(          1496446837ull,           4025840401ull),
+	FXC(18446744069683711215ull,           1496446837ull),
+	FXC(          4178745276ull,            992387019ull),
+	FXC(18446744072717164597ull,           4178745276ull),
+	FXC(          2253095531ull,           3656542712ull),
+	FXC(18446744070053008904ull,           2253095531ull),
+	FXC(          3480887161ull,           2515982640ull),
+	FXC(18446744071193568976ull,           3480887161ull),
+	FXC(           682290530ull,           4240427302ull),
+	FXC(18446744069469124314ull,            682290530ull),
+	FXC(          4255894413ull,            578019742ull),
+	FXC(18446744073131531874ull,           4255894413ull),
+	FXC(          2600650120ull,           3418093478ull),
+	FXC(18446744070291458138ull,           2600650120ull),
+	FXC(          3710735162ull,           2162680890ull),
+	FXC(18446744071546870726ull,           3710735162ull),
+	FXC(          1094639673ull,           4153132319ull),
+	FXC(18446744069556419297ull,           1094639673ull),
+	FXC(          4061352537ull,           1397197066ull),
+	FXC(18446744072312354550ull,           4061352537ull),
+	FXC(          1883842400ull,           3859777440ull),
+	FXC(18446744069849774176ull,           1883842400ull),
+	FXC(          3217516315ull,           2845054101ull),
+	FXC(18446744070864497515ull,           3217516315ull),
+	FXC(           263370557ull,           4286884652ull),
+	FXC(18446744069422666964ull,            263370557ull),
+	FXC(          4279130086ull,            368496651ull),
+	FXC(18446744073341054965ull,           4279130086ull),
+	FXC(          2765235421ull,           3286368382ull),
+	FXC(18446744070423183234ull,           2765235421ull),
+	FXC(          3812383140ull,           1977998702ull),
+	FXC(18446744071731552914ull,           3812383140ull),
+	FXC(          1297105676ull,           4094418266ull),
+	FXC(18446744069615133350ull,           1297105676ull),
+	FXC(          4125017671ull,           1196232957ull),
+	FXC(18446744072513318659ull,           4125017671ull),
+	FXC(          2070963532ull,           3762692404ull),
+	FXC(18446744069946859212ull,           2070963532ull),
+	FXC(          3353240863ull,           2683751066ull),
+	FXC(18446744071025800550ull,           3353240863ull),
+	FXC(           473400776ull,           4268797931ull),
+	FXC(18446744069440753685ull,            473400776ull),
+	FXC(          4222405917ull,            786150333ull),
+	FXC(18446744072923401283ull,           4222405917ull),
+	FXC(          2429799626ull,           3541584088ull),
+	FXC(18446744070167967528ull,           2429799626ull),
+	FXC(          3600147697ull,           2342152991ull),
+	FXC(18446744071367398625ull,           3600147697ull),
+	FXC(           889536587ull,           4201841112ull),
+	FXC(18446744069507710504ull,            889536587ull),
+	FXC(          3987903250ull,           1594795204ull),
+	FXC(18446744072114756412ull,           3987903250ull),
+	FXC(          1692182927ull,           3947563934ull),
+	FXC(18446744069761987682ull,           1692182927ull),
+	FXC(          3074040487ull,           2999503152ull),
+	FXC(18446744070710048464ull,           3074040487ull),
+	FXC(            52705856ull,           4294643893ull),
+	FXC(18446744069414907723ull,             52705856ull),
+	FXC(          4294886444ull,             26353424ull),
+	FXC(18446744073683198192ull,           4294886444ull),
+	FXC(          3018308645ull,           3055578014ull),
+	FXC(18446744070653973602ull,           3018308645ull),
+	FXC(          3957872662ull,           1667929275ull),
+	FXC(18446744072041622341ull,           3957872662ull),
+	FXC(          1619234497ull,           3978042699ull),
+	FXC(18446744069731508917ull,           1619234497ull),
+	FXC(          4207220108ull,            863737830ull),
+	FXC(18446744072845813786ull,           4207220108ull),
+	FXC(          2364198992ull,           3585708745ull),
+	FXC(18446744070123842871ull,           2364198992ull),
+	FXC(          3556426389ull,           2408023134ull),
+	FXC(18446744071301528482ull,           3556426389ull),
+	FXC(           812043729ull,           4217502704ull),
+	FXC(18446744069492048912ull,            812043729ull),
+	FXC(          4271622305ull,            447199012ull),
+	FXC(18446744073262352604ull,           4271622305ull),
+	FXC(          2704275644ull,           3336710553ull),
+	FXC(18446744070372841063ull,           2704275644ull),
+	FXC(          3775328765ull,           2047837100ull),
+	FXC(18446744071661714516ull,           3775328765ull),
+	FXC(          1221521071ull,           4117600071ull),
+	FXC(18446744069591951545ull,           1221521071ull),
+	FXC(          4102300081ull,           1271958380ull),
+	FXC(18446744072437593236ull,           4102300081ull),
+	FXC(          2001353810ull,           3800174601ull),
+	FXC(18446744069909377015ull,           2001353810ull),
+	FXC(          3303273682ull,           2745018589ull),
+	FXC(18446744070964533027ull,           3303273682ull),
+	FXC(           394745962ull,           4276788480ull),
+	FXC(18446744069432763136ull,            394745962ull),
+	FXC(          4288419964ull,            237061769ull),
+	FXC(18446744073472489847ull,           4288419964ull),
+	FXC(          2864742853ull,           3199998822ull),
+	FXC(18446744070509552794ull,           2864742853ull),
+	FXC(          3871263820ull,           1860123788ull),
+	FXC(18446744071849427828ull,           3871263820ull),
+	FXC(          1422090755ull,           4052703044ull),
+	FXC(18446744069656848572ull,           1422090755ull),
+	FXC(          4159770720ull,           1069135926ull),
+	FXC(18446744072640415690ull,           4159770720ull),
+	FXC(          2185408821ull,           3697395348ull),
+	FXC(18446744070012156268ull,           2185408821ull),
+	FXC(          3433986423ull,           2579628136ull),
+	FXC(18446744071129923480ull,           3433986423ull),
+	FXC(           604122538ull,           4252267634ull),
+	FXC(18446744069457283982ull,            604122538ull),
+	FXC(          4244533933ull,            656258914ull),
+	FXC(18446744073053292702ull,           4244533933ull),
+	FXC(          2537293599ull,           3465383855ull),
+	FXC(18446744070244167761ull,           2537293599ull),
+	FXC(          3670298613ull,           2230616993ull),
+	FXC(18446744071478934623ull,           3670298613ull),
+	FXC(          1018008636ull,           4172577440ull),
+	FXC(18446744069536974176ull,           1018008636ull),
+	FXC(          4034946641ull,           1471716574ull),
+	FXC(18446744072237835042ull,           4034946641ull),
+	FXC(          1812477362ull,           3893798902ull),
+	FXC(18446744069815752714ull,           1812477362ull),
+	FXC(          3164603066ull,           2903796051ull),
+	FXC(18446744070805755565ull,           3164603066ull),
+	FXC(           184418409ull,           4291006167ull),
+	FXC(18446744069418545449ull,            184418409ull),
+	FXC(          4292946160ull,            131747276ull),
+	FXC(18446744073577804340ull,           4292946160ull),
+	FXC(          2942411948ull,           3128730733ull),
+	FXC(18446744070580820883ull,           2942411948ull),
+	FXC(          3915747591ull,           1764557983ull),
+	FXC(18446744071944993633ull,           3915747591ull),
+	FXC(          1521120759ull,           4016582591ull),
+	FXC(18446744069692969025ull,           1521120759ull),
+	FXC(          4184755784ull,            966728038ull),
+	FXC(18446744072742823578ull,           4184755784ull),
+	FXC(          2275489241ull,           3642649144ull),
+	FXC(18446744070066902472ull,           2275489241ull),
+	FXC(          3496259414ull,           2494576955ull),
+	FXC(18446744071214974661ull,           3496259414ull),
+	FXC(           708296459ull,           4236161021ull),
+	FXC(18446744069473390595ull,            708296459ull),
+	FXC(          4259360959ull,            551895183ull),
+	FXC(18446744073157656433ull,           4259360959ull),
+	FXC(          2621574191ull,           3402071844ull),
+	FXC(18446744070307479772ull,           2621574191ull),
+	FXC(          3723935269ull,           2139871536ull),
+	FXC(18446744071569680080ull,           3723935269ull),
+	FXC(          1120102207ull,           4146337555ull),
+	FXC(18446744069563214061ull,           1120102207ull),
+	FXC(          4069849124ull,           1372250773ull),
+	FXC(18446744072337300843ull,           4069849124ull),
+	FXC(          1907490086ull,           3848145741ull),
+	FXC(18446744069861405875ull,           1907490086ull),
+	FXC(          3234912670ull,           2825258235ull),
+	FXC(18446744070884293381ull,           3234912670ull),
+	FXC(           289669429ull,           4285187942ull),
+	FXC(18446744069424363674ull,            289669429ull),
+	FXC(          4281310585ull,            342233465ull),
+	FXC(18446744073367318151ull,           4281310585ull),
+	FXC(          2785348143ull,           3269339351ull),
+	FXC(18446744070440212265ull,           2785348143ull),
+	FXC(          3824448145ull,           1954569124ull),
+	FXC(18446744071754982492ull,           3824448145ull),
+	FXC(          1322204136ull,           4086382299ull),
+	FXC(18446744069623169317ull,           1322204136ull),
+	FXC(          4132279966ull,           1170899806ull),
+	FXC(18446744072538651810ull,           4132279966ull),
+	FXC(          2094011993ull,           3749914379ull),
+	FXC(18446744069959637237ull,           2094011993ull),
+	FXC(          3369644927ull,           2663125446ull),
+	FXC(18446744071046426170ull,           3369644927ull),
+	FXC(           499584716ull,           4265812840ull),
+	FXC(18446744069443738776ull,            499584716ull),
+	FXC(          4227150159ull,            760227338ull),
+	FXC(18446744072949324278ull,           4227150159ull),
+	FXC(          2451484637ull,           3526608449ull),
+	FXC(18446744070182943167ull,           2451484637ull),
+	FXC(          3614451106ull,           2320018810ull),
+	FXC(18446744071389532806ull,           3614451106ull),
+	FXC(           915301854ull,           4196303920ull),
+	FXC(18446744069513247696ull,            915301854ull),
+	FXC(          3997613658ull,           1570295869ull),
+	FXC(18446744072139255747ull,           3997613658ull),
+	FXC(          1716372869ull,           3937106583ull),
+	FXC(18446744069772445033ull,           1716372869ull),
+	FXC(          3092387225ull,           2980584729ull),
+	FXC(18446744070728966887ull,           3092387225ull),
+	FXC(            79056303ull,           4294239650ull),
+	FXC(18446744069415311966ull,             79056303ull),
+	FXC(          4294239650ull,             79056303ull),
+	FXC(18446744073630495313ull,           4294239650ull),
+	FXC(          2980584729ull,           3092387225ull),
+	FXC(18446744070617164391ull,           2980584729ull),
+	FXC(          3937106583ull,           1716372869ull),
+	FXC(18446744071993178747ull,           3937106583ull),
+	FXC(          1570295869ull,           3997613658ull),
+	FXC(18446744069711937958ull,           1570295869ull),
+	FXC(          4196303920ull,            915301854ull),
+	FXC(18446744072794249762ull,           4196303920ull),
+	FXC(          2320018810ull,           3614451106ull),
+	FXC(18446744070095100510ull,           2320018810ull),
+	FXC(          3526608449ull,           2451484637ull),
+	FXC(18446744071258066979ull,           3526608449ull),
+	FXC(           760227338ull,           4227150159ull),
+	FXC(18446744069482401457ull,            760227338ull),
+	FXC(          4265812840ull,            499584716ull),
+	FXC(18446744073209966900ull,           4265812840ull),
+	FXC(          2663125446ull,           3369644927ull),
+	FXC(18446744070339906689ull,           2663125446ull),
+	FXC(          3749914379ull,           2094011993ull),
+	FXC(18446744071615539623ull,           3749914379ull),
+	FXC(          1170899806ull,           4132279966ull),
+	FXC(18446744069577271650ull,           1170899806ull),
+	FXC(          4086382299ull,           1322204136ull),
+	FXC(18446744072387347480ull,           4086382299ull),
+	FXC(          1954569124ull,           3824448145ull),
+	FXC(18446744069885103471ull,           1954569124ull),
+	FXC(          3269339351ull,           2785348143ull),
+	FXC(18446744070924203473ull,           3269339351ull),
+	FXC(           342233465ull,           4281310585ull),
+	FXC(18446744069428241031ull,            342233465ull),
+	FXC(          4285187942ull,            289669429ull),
+	FXC(18446744073419882187ull,           4285187942ull),
+	FXC(          2825258235ull,           3234912670ull),
+	FXC(18446744070474638946ull,           2825258235ull),
+	FXC(          3848145741ull,           1907490086ull),
+	FXC(18446744071802061530ull,           3848145741ull),
+	FXC(          1372250773ull,           4069849124ull),
+	FXC(18446744069639702492ull,           1372250773ull),
+	FXC(          4146337555ull,           1120102207ull),
+	FXC(18446744072589449409ull,           4146337555ull),
+	FXC(          2139871536ull,           3723935269ull),
+	FXC(18446744069985616347ull,           2139871536ull),
+	FXC(          3402071844ull,           2621574191ull),
+	FXC(18446744071087977425ull,           3402071844ull),
+	FXC(           551895183ull,           4259360959ull),
+	FXC(18446744069450190657ull,            551895183ull),
+	FXC(          4236161021ull,            708296459ull),
+	FXC(18446744073001255157ull,           4236161021ull),
+	FXC(          2494576955ull,           3496259414ull),
+	FXC(18446744070213292202ull,           2494576955ull),
+	FXC(          3642649144ull,           2275489241ull),
+	FXC(18446744071434062375ull,           3642649144ull),
+	FXC(           966728038ull,           4184755784ull),
+	FXC(18446744069524795832ull,            966728038ull),
+	FXC(          4016582591ull,           1521120759ull),
+	FXC(18446744072188430857ull,           4016582591ull),
+	FXC(          1764557983ull,           3915747591ull),
+	FXC(18446744069793804025ull,           1764557983ull),
+	FXC(          3128730733ull,           2942411948ull),
+	FXC(18446744070767139668ull,           3128730733ull),
+	FXC(           131747276ull,           4292946160ull),
+	FXC(18446744069416605456ull,            131747276ull),
+	FXC(          4291006167ull,            184418409ull),
+	FXC(18446744073525133207ull,           4291006167ull),
+	FXC(          2903796051ull,           3164603066ull),
+	FXC(18446744070544948550ull,           2903796051ull),
+	FXC(          3893798902ull,           1812477362ull),
+	FXC(18446744071897074254ull,           3893798902ull),
+	FXC(          1471716574ull,           4034946641ull),
+	FXC(18446744069674604975ull,           1471716574ull),
+	FXC(          4172577440ull,           1018008636ull),
+	FXC(18446744072691542980ull,           4172577440ull),
+	FXC(          2230616993ull,           3670298613ull),
+	FXC(18446744070039253003ull,           2230616993ull),
+	FXC(          3465383855ull,           2537293599ull),
+	FXC(18446744071172258017ull,           3465383855ull),
+	FXC(           656258914ull,           4244533933ull),
+	FXC(18446744069465017683ull,            656258914ull),
+	FXC(          4252267634ull,            604122538ull),
+	FXC(18446744073105429078ull,           4252267634ull),
+	FXC(          2579628136ull,           3433986423ull),
+	FXC(18446744070275565193ull,           2579628136ull),
+	FXC(          3697395348ull,           2185408821ull),
+	FXC(18446744071524142795ull,           3697395348ull),
+	FXC(          1069135926ull,           4159770720ull),
+	FXC(18446744069549780896ull,           1069135926ull),
+	FXC(          4052703044ull,           1422090755ull),
+	FXC(18446744072287460861ull,           4052703044ull),
+	FXC(          1860123788ull,           3871263820ull),
+	FXC(18446744069838287796ull,           1860123788ull),
+	FXC(          3199998822ull,           2864742853ull),
+	FXC(18446744070844808763ull,           3199998822ull),
+	FXC(           237061769ull,           4288419964ull),
+	FXC(18446744069421131652ull,            237061769ull),
+	FXC(          4276788480ull,            394745962ull),
+	FXC(18446744073314805654ull,           4276788480ull),
+	FXC(          2745018589ull,           3303273682ull),
+	FXC(18446744070406277934ull,           2745018589ull),
+	FXC(          3800174601ull,           2001353810ull),
+	FXC(18446744071708197806ull,           3800174601ull),
+	FXC(          1271958380ull,           4102300081ull),
+	FXC(18446744069607251535ull,           1271958380ull),
+	FXC(          4117600071ull,           1221521071ull),
+	FXC(18446744072488030545ull,           4117600071ull),
+	FXC(          2047837100ull,           3775328765ull),
+	FXC(18446744069934222851ull,           2047837100ull),
+	FXC(          3336710553ull,           2704275644ull),
+	FXC(18446744071005275972ull,           3336710553ull),
+	FXC(           447199012ull,           4271622305ull),
+	FXC(18446744069437929311ull,            447199012ull),
+	FXC(          4217502704ull,            812043729ull),
+	FXC(18446744072897507887ull,           4217502704ull),
+	FXC(          2408023134ull,           3556426389ull),
+	FXC(18446744070153125227ull,           2408023134ull),
+	FXC(          3585708745ull,           2364198992ull),
+	FXC(18446744071345352624ull,           3585708745ull),
+	FXC(           863737830ull,           4207220108ull),
+	FXC(18446744069502331508ull,            863737830ull),
+	FXC(          3978042699ull,           1619234497ull),
+	FXC(18446744072090317119ull,           3978042699ull),
+	FXC(          1667929275ull,           3957872662ull),
+	FXC(18446744069751678954ull,           1667929275ull),
+	FXC(          3055578014ull,           3018308645ull),
+	FXC(18446744070691242971ull,           3055578014ull),
+	FXC(            26353424ull,           4294886444ull),
+	FXC(18446744069414665172ull,             26353424ull),
+	FXC(          4294947083ull,             13176774ull),
+	FXC(18446744073696374842ull,           4294947083ull),
+	FXC(          3027668821ull,           3046303593ull),
+	FXC(18446744070663248023ull,           3027668821ull),
+	FXC(          3962971170ull,           1655778843ull),
+	FXC(18446744072053772773ull,           3962971170ull),
+	FXC(          1631431340ull,           3973056236ull),
+	FXC(18446744069736495380ull,           1631431340ull),
+	FXC(          4209850218ull,            850826195ull),
+	FXC(18446744072858725421ull,           4209850218ull),
+	FXC(          2375188665ull,           3578438609ull),
+	FXC(18446744070131113007ull,           2375188665ull),
+	FXC(          3563797363ull,           2397100839ull),
+	FXC(18446744071312450777ull,           3563797363ull),
+	FXC(           824979024ull,           4214991540ull),
+	FXC(18446744069494560076ull,            824979024ull),
+	FXC(          4272974189ull,            434091755ull),
+	FXC(18446744073275459861ull,           4272974189ull),
+	FXC(          2714499801ull,           3328398249ull),
+	FXC(18446744070381153367ull,           2714499801ull),
+	FXC(          3781593674ull,           2036244917ull),
+	FXC(18446744071673306699ull,           3781593674ull),
+	FXC(          1234147941ull,           4113833119ull),
+	FXC(18446744069595718497ull,           1234147941ull),
+	FXC(          4106183088ull,           1259366714ull),
+	FXC(18446744072450184902ull,           4106183088ull),
+	FXC(          2013003163ull,           3794016650ull),
+	FXC(18446744069915534966ull,           2013003163ull),
+	FXC(          3311679735ull,           2734871369ull),
+	FXC(18446744070974680247ull,           3311679735ull),
+	FXC(           407865107ull,           4275557289ull),
+	FXC(18446744069433994327ull,            407865107ull),
+	FXC(          4289127078ull,            223903967ull),
+	FXC(18446744073485647649ull,           4289127078ull),
+	FXC(          2874546829ull,           3191194855ull),
+	FXC(18446744070518356761ull,           2874546829ull),
+	FXC(          3876952381ull,           1848238164ull),
+	FXC(18446744071861313452ull,           3876952381ull),
+	FXC(          1434517580ull,           4048321058ull),
+	FXC(18446744069661230558ull,           1434517580ull),
+	FXC(          4163031206ull,           1056368897ull),
+	FXC(18446744072653182719ull,           4163031206ull),
+	FXC(          2196741986ull,           3690673207ull),
+	FXC(18446744070018878409ull,           2196741986ull),
+	FXC(          3441884449ull,           2569080674ull),
+	FXC(18446744071140470942ull,           3441884449ull),
+	FXC(           617165468ull,           4250394200ull),
+	FXC(18446744069459157416ull,            617165468ull),
+	FXC(          4246527332ull,            643233779ull),
+	FXC(18446744073066317837ull,           4246527332ull),
+	FXC(          2547913306ull,           3457583240ull),
+	FXC(18446744070251968376ull,           2547913306ull),
+	FXC(          3677124776ull,           2219346178ull),
+	FXC(18446744071490205438ull,           3677124776ull),
+	FXC(          1030805132ull,           4169434596ull),
+	FXC(18446744069540117020ull,           1030805132ull),
+	FXC(          4039442815ull,           1459330606ull),
+	FXC(18446744072250221010ull,           4039442815ull),
+	FXC(          1824414839ull,           3888219974ull),
+	FXC(18446744069821331642ull,           1824414839ull),
+	FXC(          3173496894ull,           2894073520ull),
+	FXC(18446744070815478096ull,           3173496894ull),
+	FXC(           197582163ull,           4290420185ull),
+	FXC(18446744069419131431ull,            197582163ull),
+	FXC(          4293330151ull,            118576083ull),
+	FXC(18446744073590975533ull,           4293330151ull),
+	FXC(          2951996911ull,           3119688816ull),
+	FXC(18446744070589862800ull,           2951996911ull),
+	FXC(          3921142750ull,           1752536335ull),
+	FXC(18446744071957015281ull,           3921142750ull),
+	FXC(          1533436302ull,           4011896955ull),
+	FXC(18446744069697654661ull,           1533436302ull),
+	FXC(          4187701970ull,            953884839ull),
+	FXC(18446744072755666777ull,           4187701970ull),
+	FXC(          2286654023ull,           3635650898ull),
+	FXC(18446744070073900718ull,           2286654023ull),
+	FXC(          3503896214ull,           2483838842ull),
+	FXC(18446744071225712774ull,           3503896214ull),
+	FXC(           721289485ull,           4233968062ull),
+	FXC(18446744069475583554ull,            721289485ull),
+	FXC(          4261034104ull,            538825051ull),
+	FXC(18446744073170726565ull,           4261034104ull),
+	FXC(          2631999263ull,           3394012957ull),
+	FXC(18446744070315538659ull,           2631999263ull),
+	FXC(          3730482776ull,           2128436593ull),
+	FXC(18446744071581115023ull,           3730482776ull),
+	FXC(          1132817720ull,           4142881616ull),
+	FXC(18446744069566670000ull,           1132817720ull),
+	FXC(          4074039976ull,           1359758194ull),
+	FXC(18446744072349793422ull,           4074039976ull),
+	FXC(          1919287054ull,           3842275534ull),
+	FXC(18446744069867276082ull,           1919287054ull),
+	FXC(          3243565216ull,           2815320366ull),
+	FXC(18446744070894231250ull,           3243565216ull),
+	FXC(           302814837ull,           4284279082ull),
+	FXC(18446744069425272534ull,            302814837ull),
+	FXC(          4282340394ull,            329096979ull),
+	FXC(18446744073380454637ull,           4282340394ull),
+	FXC(          2795365227ull,           3260778637ull),
+	FXC(18446744070448772979ull,           2795365227ull),
+	FXC(          3830426680ull,           1942826684ull),
+	FXC(18446744071766724932ull,           3830426680ull),
+	FXC(          1334734758ull,           4082306603ull),
+	FXC(18446744069627245013ull,           1334734758ull),
+	FXC(          4135852789ull,           1158216639ull),
+	FXC(18446744072551334977ull,           4135852789ull),
+	FXC(          2105506713ull,           3743472393ull),
+	FXC(18446744069966079223ull,           2105506713ull),
+	FXC(          3377799422ull,           2652774988ull),
+	FXC(18446744071056776628ull,           3377799422ull),
+	FXC(           512669694ull,           4264260060ull),
+	FXC(18446744069445291556ull,            512669694ull),
+	FXC(          4229462610ull,            747255046ull),
+	FXC(18446744072962296570ull,           4229462610ull),
+	FXC(          2462292582ull,           3519070803ull),
+	FXC(18446744070190480813ull,           2462292582ull),
+	FXC(          3621551813ull,           2308918911ull),
+	FXC(18446744071400632705ull,           3621551813ull),
+	FXC(           928171626ull,           4193476065ull),
+	FXC(18446744069516075551ull,            928171626ull),
+	FXC(          4002412444ull,           1558023973ull),
+	FXC(18446744072151527643ull,           4002412444ull),
+	FXC(          1728443664ull,           3931822297ull),
+	FXC(18446744069777729319ull,           1728443664ull),
+	FXC(          3101516976ull,           2971083391ull),
+	FXC(18446744070738468225ull,           3101516976ull),
+	FXC(            92230472ull,           4293976900ull),
+	FXC(18446744069415574716ull,             92230472ull),
+	FXC(          4294461982ull,             65881389ull),
+	FXC(18446744073643670227ull,           4294461982ull),
+	FXC(          2990058012ull,           3083228366ull),
+	FXC(18446744070626323250ull,           2990058012ull),
+	FXC(          3942353812ull,           1704285919ull),
+	FXC(18446744072005265697ull,           3942353812ull),
+	FXC(          1582552984ull,           3992777245ull),
+	FXC(18446744069716774371ull,           1582552984ull),
+	FXC(          4199092278ull,            902423468ull),
+	FXC(18446744072807128148ull,           4199092278ull),
+	FXC(          2331096871ull,           3607316378ull),
+	FXC(18446744070102235238ull,           2331096871ull),
+	FXC(          3534112901ull,           2440653617ull),
+	FXC(18446744071268897999ull,           3534112901ull),
+	FXC(           773192474ull,           4224797921ull),
+	FXC(18446744069484753695ull,            773192474ull),
+	FXC(          4267325469ull,            486495035ull),
+	FXC(18446744073223056581ull,           4267325469ull),
+	FXC(          2673450838ull,           3361458715ull),
+	FXC(18446744070348092901ull,           2673450838ull),
+	FXC(          3756321069ull,           2082497563ull),
+	FXC(18446744071627054053ull,           3756321069ull),
+	FXC(          1183571952ull,           4128668249ull),
+	FXC(18446744069580883367ull,           1183571952ull),
+	FXC(          4090419533ull,           1309661069ull),
+	FXC(18446744072399890547ull,           4090419533ull),
+	FXC(          1966293167ull,           3818433613ull),
+	FXC(18446744069891118003ull,           1966293167ull),
+	FXC(          3277869293ull,           2775304843ull),
+	FXC(18446744070934246773ull,           3277869293ull),
+	FXC(           355366730ull,           4280240479ull),
+	FXC(18446744069429311137ull,            355366730ull),
+	FXC(          4286056468ull,            276521294ull),
+	FXC(18446744073433030322ull,           4286056468ull),
+	FXC(          2835169511ull,           3226229675ull),
+	FXC(18446744070483321941ull,           2835169511ull),
+	FXC(          3853979728ull,           1895675165ull),
+	FXC(18446744071813876451ull,           3853979728ull),
+	FXC(          1384730436ull,           4065619964ull),
+	FXC(18446744069643931652ull,           1384730436ull),
+	FXC(          4149754467ull,           1107376152ull),
+	FXC(18446744072602175464ull,           4149754467ull),
+	FXC(          2151286337ull,           3717352710ull),
+	FXC(18446744069992198906ull,           2151286337ull),
+	FXC(          3410098710ull,           2611124444ull),
+	FXC(18446744071098427172ull,           3410098710ull),
+	FXC(           564960121ull,           4257647723ull),
+	FXC(18446744069451903893ull,            564960121ull),
+	FXC(          4238314108ull,            695296767ull),
+	FXC(18446744073014254849ull,           4238314108ull),
+	FXC(          2505291588ull,           3488589706ull),
+	FXC(18446744070220961910ull,           2505291588ull),
+	FXC(          3649613104ull,           2264303042ull),
+	FXC(18446744071445248574ull,           3649613104ull),
+	FXC(           979562138ull,           4181770210ull),
+	FXC(18446744069527781406ull,            979562138ull),
+	FXC(          4021230421ull,           1508790899ull),
+	FXC(18446744072200760717ull,           4021230421ull),
+	FXC(          1776563023ull,           3910315575ull),
+	FXC(18446744069799236041ull,           1776563023ull),
+	FXC(          3137743202ull,           2932799290ull),
+	FXC(18446744070776752326ull,           3137743202ull),
+	FXC(           144917230ull,           4292521761ull),
+	FXC(18446744069417029855ull,            144917230ull),
+	FXC(          4291551760ull,            171252920ull),
+	FXC(18446744073538298696ull,           4291551760ull),
+	FXC(          2913491250ull,           3155679453ull),
+	FXC(18446744070553872163ull,           2913491250ull),
+	FXC(          3899341179ull,           1800522825ull),
+	FXC(18446744071909028791ull,           3899341179ull),
+	FXC(          1484088690ull,           4030412489ull),
+	FXC(18446744069679139127ull,           1484088690ull),
+	FXC(          4175681009ull,           1005202558ull),
+	FXC(18446744072704349058ull,           4175681009ull),
+	FXC(          2241866812ull,           3663437903ull),
+	FXC(18446744070046113713ull,           2241866812ull),
+	FXC(          3473151854ull,           2526650010ull),
+	FXC(18446744071182901606ull,           3473151854ull),
+	FXC(           669277872ull,           4242500584ull),
+	FXC(18446744069467051032ull,            669277872ull),
+	FXC(          4254101044ull,            591073921ull),
+	FXC(18446744073118477695ull,           4254101044ull),
+	FXC(          2590151318ull,           3426056074ull),
+	FXC(18446744070283495542ull,           2590151318ull),
+	FXC(          3704082687ull,           2174055087ull),
+	FXC(18446744071535496529ull,           3704082687ull),
+	FXC(          1081892891ull,           4156471081ull),
+	FXC(18446744069553080535ull,           1081892891ull),
+	FXC(          4057046884ull,           1409650544ull),
+	FXC(18446744072299901072ull,           4057046884ull),
+	FXC(          1871991904ull,           3865538822ull),
+	FXC(18446744069844012794ull,           1871991904ull),
+	FXC(          3208772670ull,           2854911913ull),
+	FXC(18446744070854639703ull,           3208772670ull),
+	FXC(           250217341ull,           4287672487ull),
+	FXC(18446744069421879129ull,            250217341ull),
+	FXC(          4277979416ull,            381623102ull),
+	FXC(18446744073327928514ull,           4277979416ull),
+	FXC(          2755139971ull,           3294836538ull),
+	FXC(18446744070414715078ull,           2755139971ull),
+	FXC(          3806296784ull,           1989685620ull),
+	FXC(18446744071719865996ull,           3806296784ull),
+	FXC(          1284538073ull,           4098378461ull),
+	FXC(18446744069611173155ull,           1284538073ull),
+	FXC(          4121328267ull,           1208882703ull),
+	FXC(18446744072500668913ull,           4121328267ull),
+	FXC(          2059410008ull,           3769028322ull),
+	FXC(18446744069940523294ull,           2059410008ull),
+	FXC(          3344991450ull,           2694026034ull),
+	FXC(18446744071015525582ull,           3344991450ull),
+	FXC(           460302060ull,           4270230215ull),
+	FXC(18446744069439321401ull,            460302060ull),
+	FXC(          4219974170ull,            799100792ull),
+	FXC(18446744072910450824ull,           4219974170ull),
+	FXC(          2418922764ull,           3549021941ull),
+	FXC(18446744070160529675ull,           2418922764ull),
+	FXC(          3592945130ull,           2353187066ull),
+	FXC(18446744071356364550ull,           3592945130ull),
+	FXC(           876641334ull,           4204550397ull),
+	FXC(18446744069505001219ull,            876641334ull),
+	FXC(          3982991719ull,           1607022414ull),
+	FXC(18446744072102529202ull,           3982991719ull),
+	FXC(          1680064008ull,           3952736900ull),
+	FXC(18446744069756814716ull,           1680064008ull),
+	FXC(          3064823674ull,           3008920059ull),
+	FXC(18446744070700631557ull,           3064823674ull),
+	FXC(            39529826ull,           4294785381ull),
+	FXC(18446744069414766235ull,             39529826ull),
+	FXC(          4294785381ull,             39529826ull),
+	FXC(18446744073670021790ull,           4294785381ull),
+	FXC(          3008920059ull,           3064823674ull),
+	FXC(18446744070644727942ull,           3008920059ull),
+	FXC(          3952736900ull,           1680064008ull),
+	FXC(18446744072029487608ull,           3952736900ull),
+	FXC(          1607022414ull,           3982991719ull),
+	FXC(18446744069726559897ull,           1607022414ull),
+	FXC(          4204550397ull,            876641334ull),
+	FXC(18446744072832910282ull,           4204550397ull),
+	FXC(          2353187066ull,           3592945130ull),
+	FXC(18446744070116606486ull,           2353187066ull),
+	FXC(          3549021941ull,           2418922764ull),
+	FXC(18446744071290628852ull,           3549021941ull),
+	FXC(           799100792ull,           4219974170ull),
+	FXC(18446744069489577446ull,            799100792ull),
+	FXC(          4270230215ull,            460302060ull),
+	FXC(18446744073249249556ull,           4270230215ull),
+	FXC(          2694026034ull,           3344991450ull),
+	FXC(18446744070364560166ull,           2694026034ull),
+	FXC(          3769028322ull,           2059410008ull),
+	FXC(18446744071650141608ull,           3769028322ull),
+	FXC(          1208882703ull,           4121328267ull),
+	FXC(18446744069588223349ull,           1208882703ull),
+	FXC(          4098378461ull,           1284538073ull),
+	FXC(18446744072425013543ull,           4098378461ull),
+	FXC(          1989685620ull,           3806296784ull),
+	FXC(18446744069903254832ull,           1989685620ull),
+	FXC(          3294836538ull,           2755139971ull),
+	FXC(18446744070954411645ull,           3294836538ull),
+	FXC(           381623102ull,           4277979416ull),
+	FXC(18446744069431572200ull,            381623102ull),
+	FXC(          4287672487ull,            250217341ull),
+	FXC(18446744073459334275ull,           4287672487ull),
+	FXC(          2854911913ull,           3208772670ull),
+	FXC(18446744070500778946ull,           2854911913ull),
+	FXC(          3865538822ull,           1871991904ull),
+	FXC(18446744071837559712ull,           3865538822ull),
+	FXC(          1409650544ull,           4057046884ull),
+	FXC(18446744069652504732ull,           1409650544ull),
+	FXC(          4156471081ull,           1081892891ull),
+	FXC(18446744072627658725ull,           4156471081ull),
+	FXC(          2174055087ull,           3704082687ull),
+	FXC(18446744070005468929ull,           2174055087ull),
+	FXC(          3426056074ull,           2590151318ull),
+	FXC(18446744071119400298ull,           3426056074ull),
+	FXC(           591073921ull,           4254101044ull),
+	FXC(18446744069455450572ull,            591073921ull),
+	FXC(          4242500584ull,            669277872ull),
+	FXC(18446744073040273744ull,           4242500584ull),
+	FXC(          2526650010ull,           3473151854ull),
+	FXC(18446744070236399762ull,           2526650010ull),
+	FXC(          3663437903ull,           2241866812ull),
+	FXC(18446744071467684804ull,           3663437903ull),
+	FXC(          1005202558ull,           4175681009ull),
+	FXC(18446744069533870607ull,           1005202558ull),
+	FXC(          4030412489ull,           1484088690ull),
+	FXC(18446744072225462926ull,           4030412489ull),
+	FXC(          1800522825ull,           3899341179ull),
+	FXC(18446744069810210437ull,           1800522825ull),
+	FXC(          3155679453ull,           2913491250ull),
+	FXC(18446744070796060366ull,           3155679453ull),
+	FXC(           171252920ull,           4291551760ull),
+	FXC(18446744069417999856ull,            171252920ull),
+	FXC(          4292521761ull,            144917230ull),
+	FXC(18446744073564634386ull,           4292521761ull),
+	FXC(          2932799290ull,           3137743202ull),
+	FXC(18446744070571808414ull,           2932799290ull),
+	FXC(          3910315575ull,           1776563023ull),
+	FXC(18446744071932988593ull,           3910315575ull),
+	FXC(          1508790899ull,           4021230421ull),
+	FXC(18446744069688321195ull,           1508790899ull),
+	FXC(          4181770210ull,            979562138ull),
+	FXC(18446744072729989478ull,           4181770210ull),
+	FXC(          2264303042ull,           3649613104ull),
+	FXC(18446744070059938512ull,           2264303042ull),
+	FXC(          3488589706ull,           2505291588ull),
+	FXC(18446744071204260028ull,           3488589706ull),
+	FXC(           695296767ull,           4238314108ull),
+	FXC(18446744069471237508ull,            695296767ull),
+	FXC(          4257647723ull,            564960121ull),
+	FXC(18446744073144591495ull,           4257647723ull),
+	FXC(          2611124444ull,           3410098710ull),
+	FXC(18446744070299452906ull,           2611124444ull),
+	FXC(          3717352710ull,           2151286337ull),
+	FXC(18446744071558265279ull,           3717352710ull),
+	FXC(          1107376152ull,           4149754467ull),
+	FXC(18446744069559797149ull,           1107376152ull),
+	FXC(          4065619964ull,           1384730436ull),
+	FXC(18446744072324821180ull,           4065619964ull),
+	FXC(          1895675165ull,           3853979728ull),
+	FXC(18446744069855571888ull,           1895675165ull),
+	FXC(          3226229675ull,           2835169511ull),
+	FXC(18446744070874382105ull,           3226229675ull),
+	FXC(           276521294ull,           4286056468ull),
+	FXC(18446744069423495148ull,            276521294ull),
+	FXC(          4280240479ull,            355366730ull),
+	FXC(18446744073354184886ull,           4280240479ull),
+	FXC(          2775304843ull,           3277869293ull),
+	FXC(18446744070431682323ull,           2775304843ull),
+	FXC(          3818433613ull,           1966293167ull),
+	FXC(18446744071743258449ull,           3818433613ull),
+	FXC(          1309661069ull,           4090419533ull),
+	FXC(18446744069619132083ull,           1309661069ull),
+	FXC(          4128668249ull,           1183571952ull),
+	FXC(18446744072525979664ull,           4128668249ull),
+	FXC(          2082497563ull,           3756321069ull),
+	FXC(18446744069953230547ull,           2082497563ull),
+	FXC(          3361458715ull,           2673450838ull),
+	FXC(18446744071036100778ull,           3361458715ull),
+	FXC(           486495035ull,           4267325469ull),
+	FXC(18446744069442226147ull,            486495035ull),
+	FXC(          4224797921ull,            773192474ull),
+	FXC(18446744072936359142ull,           4224797921ull),
+	FXC(          2440653617ull,           3534112901ull),
+	FXC(18446744070175438715ull,           2440653617ull),
+	FXC(          3607316378ull,           2331096871ull),
+	FXC(18446744071378454745ull,           3607316378ull),
+	FXC(           902423468ull,           4199092278ull),
+	FXC(18446744069510459338ull,            902423468ull),
+	FXC(          3992777245ull,           1582552984ull),
+	FXC(18446744072126998632ull,           3992777245ull),
+	FXC(          1704285919ull,           3942353812ull),
+	FXC(18446744069767197804ull,           1704285919ull),
+	FXC(          3083228366ull,           2990058012ull),
+	FXC(18446744070719493604ull,           3083228366ull),
+	FXC(            65881389ull,           4294461982ull),
+	FXC(18446744069415089634ull,             65881389ull),
+	FXC(          4293976900ull,             92230472ull),
+	FXC(18446744073617321144ull,           4293976900ull),
+	FXC(          2971083391ull,           3101516976ull),
+	FXC(18446744070608034640ull,           2971083391ull),
+	FXC(          3931822297ull,           1728443664ull),
+	FXC(18446744071981107952ull,           3931822297ull),
+	FXC(          1558023973ull,           4002412444ull),
+	FXC(18446744069707139172ull,           1558023973ull),
+	FXC(          4193476065ull,            928171626ull),
+	FXC(18446744072781379990ull,           4193476065ull),
+	FXC(          2308918911ull,           3621551813ull),
+	FXC(18446744070087999803ull,           2308918911ull),
+	FXC(          3519070803ull,           2462292582ull),
+	FXC(18446744071247259034ull,           3519070803ull),
+	FXC(           747255046ull,           4229462610ull),
+	FXC(18446744069480089006ull,            747255046ull),
+	FXC(          4264260060ull,            512669694ull),
+	FXC(18446744073196881922ull,           4264260060ull),
+	FXC(          2652774988ull,           3377799422ull),
+	FXC(18446744070331752194ull,           2652774988ull),
+	FXC(          3743472393ull,           2105506713ull),
+	FXC(18446744071604044903ull,           3743472393ull),
+	FXC(          1158216639ull,           4135852789ull),
+	FXC(18446744069573698827ull,           1158216639ull),
+	FXC(          4082306603ull,           1334734758ull),
+	FXC(18446744072374816858ull,           4082306603ull),
+	FXC(          1942826684ull,           3830426680ull),
+	FXC(18446744069879124936ull,           1942826684ull),
+	FXC(          3260778637ull,           2795365227ull),
+	FXC(18446744070914186389ull,           3260778637ull),
+	FXC(           329096979ull,           4282340394ull),
+	FXC(18446744069427211222ull,            329096979ull),
+	FXC(          4284279082ull,            302814837ull),
+	FXC(18446744073406736779ull,           4284279082ull),
+	FXC(          2815320366ull,           3243565216ull),
+	FXC(18446744070465986400ull,           2815320366ull),
+	FXC(          3842275534ull,           1919287054ull),
+	FXC(18446744071790264562ull,           3842275534ull),
+	FXC(          1359758194ull,           4074039976ull),
+	FXC(18446744069635511640ull,           1359758194ull),
+	FXC(          4142881616ull,           1132817720ull),
+	FXC(18446744072576733896ull,           4142881616ull),
+	FXC(          2128436593ull,           3730482776ull),
+	FXC(18446744069979068840ull,           2128436593ull),
+	FXC(          3394012957ull,           2631999263ull),
+	FXC(18446744071077552353ull,           3394012957ull),
+	FXC(           538825051ull,           4261034104ull),
+	FXC(18446744069448517512ull,            538825051ull),
+	FXC(          4233968062ull,            721289485ull),
+	FXC(18446744072988262131ull,           4233968062ull),
+	FXC(          2483838842ull,           3503896214ull),
+	FXC(18446744070205655402ull,           2483838842ull),
+	FXC(          3635650898ull,           2286654023ull),
+	FXC(18446744071422897593ull,           3635650898ull),
+	FXC(           953884839ull,           4187701970ull),
+	FXC(18446744069521849646ull,            953884839ull),
+	FXC(          4011896955ull,           1533436302ull),
+	FXC(18446744072176115314ull,           4011896955ull),
+	FXC(          1752536335ull,           3921142750ull),
+	FXC(18446744069788408866ull,           1752536335ull),
+	FXC(          3119688816ull,           2951996911ull),
+	FXC(18446744070757554705ull,           3119688816ull),
+	FXC(           118576083ull,           4293330151ull),
+	FXC(18446744069416221465ull,            118576083ull),
+	FXC(          4290420185ull,            197582163ull),
+	FXC(18446744073511969453ull,           4290420185ull),
+	FXC(          2894073520ull,           3173496894ull),
+	FXC(18446744070536054722ull,           2894073520ull),
+	FXC(          3888219974ull,           1824414839ull),
+	FXC(18446744071885136777ull,           3888219974ull),
+	FXC(          1459330606ull,           4039442815ull),
+	FXC(18446744069670108801ull,           1459330606ull),
+	FXC(          4169434596ull,           1030805132ull),
+	FXC(18446744072678746484ull,           4169434596ull),
+	FXC(          2219346178ull,           3677124776ull),
+	FXC(18446744070032426840ull,           2219346178ull),
+	FXC(          3457583240ull,           2547913306ull),
+	FXC(18446744071161638310ull,           3457583240ull),
+	FXC(           643233779ull,           4246527332ull),
+	FXC(18446744069463024284ull,            643233779ull),
+	FXC(          4250394200ull,            617165468ull),
+	FXC(18446744073092386148ull,           4250394200ull),
+	FXC(          2569080674ull,           3441884449ull),
+	FXC(18446744070267667167ull,           2569080674ull),
+	FXC(          3690673207ull,           2196741986ull),
+	FXC(18446744071512809630ull,           3690673207ull),
+	FXC(          1056368897ull,           4163031206ull),
+	FXC(18446744069546520410ull,           1056368897ull),
+	FXC(          4048321058ull,           1434517580ull),
+	FXC(18446744072275034036ull,           4048321058ull),
+	FXC(          1848238164ull,           3876952381ull),
+	FXC(18446744069832599235ull,           1848238164ull),
+	FXC(          3191194855ull,           2874546829ull),
+	FXC(18446744070835004787ull,           3191194855ull),
+	FXC(           223903967ull,           4289127078ull),
+	FXC(18446744069420424538ull,            223903967ull),
+	FXC(          4275557289ull,            407865107ull),
+	FXC(18446744073301686509ull,           4275557289ull),
+	FXC(          2734871369ull,           3311679735ull),
+	FXC(18446744070397871881ull,           2734871369ull),
+	FXC(          3794016650ull,           2013003163ull),
+	FXC(18446744071696548453ull,           3794016650ull),
+	FXC(          1259366714ull,           4106183088ull),
+	FXC(18446744069603368528ull,           1259366714ull),
+	FXC(          4113833119ull,           1234147941ull),
+	FXC(18446744072475403675ull,           4113833119ull),
+	FXC(          2036244917ull,           3781593674ull),
+	FXC(18446744069927957942ull,           2036244917ull),
+	FXC(          3328398249ull,           2714499801ull),
+	FXC(18446744070995051815ull,           3328398249ull),
+	FXC(           434091755ull,           4272974189ull),
+	FXC(18446744069436577427ull,            434091755ull),
+	FXC(          4214991540ull,            824979024ull),
+	FXC(18446744072884572592ull,           4214991540ull),
+	FXC(          2397100839ull,           3563797363ull),
+	FXC(18446744070145754253ull,           2397100839ull),
+	FXC(          3578438609ull,           2375188665ull),
+	FXC(18446744071334362951ull,           3578438609ull),
+	FXC(           850826195ull,           4209850218ull),
+	FXC(18446744069499701398ull,            850826195ull),
+	FXC(          3973056236ull,           1631431340ull),
+	FXC(18446744072078120276ull,           3973056236ull),
+	FXC(          1655778843ull,           3962971170ull),
+	FXC(18446744069746580446ull,           1655778843ull),
+	FXC(          3046303593ull,           3027668821ull),
+	FXC(18446744070681882795ull,           3046303593ull),
+	FXC(            13176774ull,           4294947083ull),
+	FXC(18446744069414604533ull,             13176774ull)
+};
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static inline void
+fxp_FFT4(__m256i *ya0_re, __m256i *ya1_re, __m256i *ya0_im, __m256i *ya1_im,
+	size_t k)
+{
+	__m256i yv0_re = *ya0_re;
+	__m256i yv1_re = *ya1_re;
+	__m256i yv0_im = *ya0_im;
+	__m256i yv1_im = *ya1_im;
+	__m256i yt0_re, yt0_im, yt1_re, yt1_im;
+
+	/*
+	 * yv0: 0:1:2:3
+	 * yv1: 4:5:6:7
+	 * combine 0/2 and 1/3 with gm[k+0]
+	 * combine 4/6 and 5/7 with gm[k+1]
+	 */
+
+	/* yt0 <- 0:1:4:5
+	   yt1 <- 2:3:6:7 */
+	yt0_re = _mm256_permute2x128_si256(yv0_re, yv1_re, 0x20);
+	yt0_im = _mm256_permute2x128_si256(yv0_im, yv1_im, 0x20);
+	yt1_re = _mm256_permute2x128_si256(yv0_re, yv1_re, 0x31);
+	yt1_im = _mm256_permute2x128_si256(yv0_im, yv1_im, 0x31);
+
+	/* yg0 <- gm0:gm0:gm1:gm1 */
+	__m256i yg0 = _mm256_loadu_si256((const __m256i *)(GM_TAB + k));
+	__m256i yg0_re = _mm256_shuffle_epi32(yg0, 0x44);
+	__m256i yg0_im = _mm256_shuffle_epi32(yg0, 0xEE);
+
+	fxc_mul_x4(&yt1_re, &yt1_im, yt1_re, yt1_im, yg0_re, yg0_im);
+
+	yv0_re = _mm256_add_epi64(yt0_re, yt1_re);
+	yv0_im = _mm256_add_epi64(yt0_im, yt1_im);
+	yv1_re = _mm256_sub_epi64(yt0_re, yt1_re);
+	yv1_im = _mm256_sub_epi64(yt0_im, yt1_im);
+
+	/*
+	 * v0: 0:1:4:5
+	 * v1: 2:3:6:7
+	 * combine 0/1 with gm[2*k+0], 2/3 with gm[2*k+1]
+	 * combine 4/5 with gm[2*k+2], 6/7 with gm[2*k+3]
+	 */
+
+	/* yt0 <- 0:2:4:6
+	   yt1 <- 1:3:5:7 */
+	yt0_re = _mm256_unpacklo_epi64(yv0_re, yv1_re);
+	yt0_im = _mm256_unpacklo_epi64(yv0_im, yv1_im);
+	yt1_re = _mm256_unpackhi_epi64(yv0_re, yv1_re);
+	yt1_im = _mm256_unpackhi_epi64(yv0_im, yv1_im);
+
+	/* yg1 <- gm4:gm5:gm6:gm7 */
+	__m256i yg1_re = _mm256_setr_epi64x(
+		GM_TAB[(k << 1) + 0].re.v,
+		GM_TAB[(k << 1) + 1].re.v,
+		GM_TAB[(k << 1) + 2].re.v,
+		GM_TAB[(k << 1) + 3].re.v);
+	__m256i yg1_im = _mm256_setr_epi64x(
+		GM_TAB[(k << 1) + 0].im.v,
+		GM_TAB[(k << 1) + 1].im.v,
+		GM_TAB[(k << 1) + 2].im.v,
+		GM_TAB[(k << 1) + 3].im.v);
+
+	fxc_mul_x4(&yt1_re, &yt1_im, yt1_re, yt1_im, yg1_re, yg1_im);
+
+	yv0_re = _mm256_add_epi64(yt0_re, yt1_re);
+	yv0_im = _mm256_add_epi64(yt0_im, yt1_im);
+	yv1_re = _mm256_sub_epi64(yt0_re, yt1_re);
+	yv1_im = _mm256_sub_epi64(yt0_im, yt1_im);
+
+	/*
+	 * Reorder into 0:1:2:3 and 4:5:6:7
+	 */
+	yt0_re = _mm256_unpacklo_epi64(yv0_re, yv1_re);
+	yt0_im = _mm256_unpacklo_epi64(yv0_im, yv1_im);
+	yt1_re = _mm256_unpackhi_epi64(yv0_re, yv1_re);
+	yt1_im = _mm256_unpackhi_epi64(yv0_im, yv1_im);
+	yv0_re = _mm256_permute2x128_si256(yt0_re, yt1_re, 0x20);
+	yv0_im = _mm256_permute2x128_si256(yt0_im, yt1_im, 0x20);
+	yv1_re = _mm256_permute2x128_si256(yt0_re, yt1_re, 0x31);
+	yv1_im = _mm256_permute2x128_si256(yt0_im, yt1_im, 0x31);
+
+	*ya0_re = yv0_re;
+	*ya0_im = yv0_im;
+	*ya1_re = yv1_re;
+	*ya1_im = yv1_im;
+}
+#endif // NTRUGEN_AVX2
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+vect_FFT(unsigned logn, fxr *f)
+{
+	size_t hn = (size_t)1 << (logn - 1);
+	size_t t = hn;
+#if NTRUGEN_AVX2
+	if (logn >= 4) {
+		for (unsigned lm = 1; lm < (logn - 2); lm ++) {
+			size_t m = (size_t)1 << lm;
+			size_t ht = t >> 1;
+			size_t j0 = 0;
+			size_t hm = m >> 1;
+			for (size_t i = 0; i < hm; i ++) {
+				fxc s = GM_TAB[m + i];
+				__m256i ys_re = _mm256_set1_epi64x(s.re.v);
+				__m256i ys_im = _mm256_set1_epi64x(s.im.v);
+				for (size_t j = j0; j < j0 + ht; j += 4) {
+					__m256i ya_re = _mm256_loadu_si256(
+						(__m256i *)(f + j));
+					__m256i ya_im = _mm256_loadu_si256(
+						(__m256i *)(f + j + hn));
+					__m256i yb_re = _mm256_loadu_si256(
+						(__m256i *)(f + j + ht));
+					__m256i yb_im = _mm256_loadu_si256(
+						(__m256i *)(f + j + ht + hn));
+					fxc_mul_x4(&yb_re, &yb_im,
+						yb_re, yb_im, ys_re, ys_im);
+					__m256i yc_re = _mm256_add_epi64(
+						ya_re, yb_re);
+					__m256i yc_im = _mm256_add_epi64(
+						ya_im, yb_im);
+					__m256i yd_re = _mm256_sub_epi64(
+						ya_re, yb_re);
+					__m256i yd_im = _mm256_sub_epi64(
+						ya_im, yb_im);
+					_mm256_storeu_si256(
+						(__m256i *)(f + j),
+						yc_re);
+					_mm256_storeu_si256(
+						(__m256i *)(f + j + hn),
+						yc_im);
+					_mm256_storeu_si256(
+						(__m256i *)(f + j + ht),
+						yd_re);
+					_mm256_storeu_si256(
+						(__m256i *)(f + j + ht + hn),
+						yd_im);
+				}
+				j0 += t;
+			}
+			t = ht;
+		}
+
+		size_t m = hn >> 1;
+		size_t hm = m >> 1;
+		for (size_t i = 0; i < hm; i += 2) {
+			__m256i ya0_re = _mm256_loadu_si256(
+				(const __m256i *)(f + (i << 2) + 0));
+			__m256i ya1_re = _mm256_loadu_si256(
+				(const __m256i *)(f + (i << 2) + 4));
+			__m256i ya0_im = _mm256_loadu_si256(
+				(const __m256i *)(f + hn + (i << 2) + 0));
+			__m256i ya1_im = _mm256_loadu_si256(
+				(const __m256i *)(f + hn + (i << 2) + 4));
+			fxp_FFT4(&ya0_re, &ya1_re, &ya0_im, &ya1_im, m + i);
+			_mm256_storeu_si256(
+				(__m256i *)(f + (i << 2) + 0), ya0_re);
+			_mm256_storeu_si256(
+				(__m256i *)(f + (i << 2) + 4), ya1_re);
+			_mm256_storeu_si256(
+				(__m256i *)(f + hn + (i << 2) + 0), ya0_im);
+			_mm256_storeu_si256(
+				(__m256i *)(f + hn + (i << 2) + 4), ya1_im);
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (unsigned lm = 1; lm < logn; lm ++) {
+		size_t m = (size_t)1 << lm;
+		size_t ht = t >> 1;
+		size_t j0 = 0;
+		size_t hm = m >> 1;
+		for (size_t i = 0; i < hm; i ++) {
+			fxc s = GM_TAB[m + i];
+			for (size_t j = j0; j < j0 + ht; j ++) {
+				fxc x, y;
+				x.re = f[j];
+				x.im = f[j + hn];
+				y.re = f[j + ht];
+				y.im = f[j + ht + hn];
+				y = fxc_mul(s, y);
+				fxc z1 = fxc_add(x, y);
+				f[j] = z1.re;
+				f[j + hn] = z1.im;
+				fxc z2 = fxc_sub(x, y);
+				f[j + ht] = z2.re;
+				f[j + ht + hn] = z2.im;
+			}
+			j0 += t;
+		}
+		t = ht;
+	}
+}
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static inline void
+fxp_iFFT4(__m256i *ya0_re, __m256i *ya1_re, __m256i *ya0_im, __m256i *ya1_im,
+	size_t k)
+{
+	__m256i yv0_re = *ya0_re;
+	__m256i yv1_re = *ya1_re;
+	__m256i yv0_im = *ya0_im;
+	__m256i yv1_im = *ya1_im;
+	__m256i yt0_re, yt0_im, yt1_re, yt1_im;
+
+	/*
+	 * v0: 0:1:2:3
+	 * v1: 4:5:6:7
+	 * combine 0/1 with gm[2*k+0], 2/3 with gm[2*k+1]
+	 * combine 4/5 with gm[2*k+2], 6/7 with gm[2*k+3]
+	 */
+
+	/* yt0 <- 0:4:2:6
+	   yt1 <- 1:5:3:7 */
+	yt0_re = _mm256_unpacklo_epi64(yv0_re, yv1_re);
+	yt0_im = _mm256_unpacklo_epi64(yv0_im, yv1_im);
+	yt1_re = _mm256_unpackhi_epi64(yv0_re, yv1_re);
+	yt1_im = _mm256_unpackhi_epi64(yv0_im, yv1_im);
+
+	/* yg1 <- gm4:gm6:gm5:gm7 */
+	__m256i yg1_re = _mm256_setr_epi64x(
+		GM_TAB[(k << 1) + 0].re.v,
+		GM_TAB[(k << 1) + 2].re.v,
+		GM_TAB[(k << 1) + 1].re.v,
+		GM_TAB[(k << 1) + 3].re.v);
+	__m256i yg1_im = _mm256_setr_epi64x(
+		GM_TAB[(k << 1) + 0].im.v,
+		GM_TAB[(k << 1) + 2].im.v,
+		GM_TAB[(k << 1) + 1].im.v,
+		GM_TAB[(k << 1) + 3].im.v);
+	yg1_im = _mm256_sub_epi64(_mm256_setzero_si256(), yg1_im);
+
+	yv0_re = fxr_half_x4(_mm256_add_epi64(yt0_re, yt1_re));
+	yv0_im = fxr_half_x4(_mm256_add_epi64(yt0_im, yt1_im));
+	yv1_re = fxr_half_x4(_mm256_sub_epi64(yt0_re, yt1_re));
+	yv1_im = fxr_half_x4(_mm256_sub_epi64(yt0_im, yt1_im));
+	fxc_mul_x4(&yv1_re, &yv1_im, yv1_re, yv1_im, yg1_re, yg1_im);
+
+	/*
+	 * v0: 0:4:2:6
+	 * v1: 1:5:3:7
+	 * combine 0/2 and 1/3 with gm[k+0]
+	 * combine 4/6 and 5/7 with gm[k+1]
+	 */
+
+	/* yt0 <- 0:4:1:5
+	   yt1 <- 2:6:3:7 */
+	yt0_re = _mm256_permute2x128_si256(yv0_re, yv1_re, 0x20);
+	yt0_im = _mm256_permute2x128_si256(yv0_im, yv1_im, 0x20);
+	yt1_re = _mm256_permute2x128_si256(yv0_re, yv1_re, 0x31);
+	yt1_im = _mm256_permute2x128_si256(yv0_im, yv1_im, 0x31);
+
+	/* yg0 <- gm0:gm1:gm0:gm1 */
+	__m256i yg0 = _mm256_loadu_si256((const __m256i *)(GM_TAB + k));
+	__m256i yg0_re = _mm256_permute4x64_epi64(yg0, 0x88);
+	__m256i yg0_im = _mm256_permute4x64_epi64(yg0, 0xDD);
+	yg0_im = _mm256_sub_epi64(_mm256_setzero_si256(), yg0_im);
+
+	yv0_re = fxr_half_x4(_mm256_add_epi64(yt0_re, yt1_re));
+	yv0_im = fxr_half_x4(_mm256_add_epi64(yt0_im, yt1_im));
+	yv1_re = fxr_half_x4(_mm256_sub_epi64(yt0_re, yt1_re));
+	yv1_im = fxr_half_x4(_mm256_sub_epi64(yt0_im, yt1_im));
+	fxc_mul_x4(&yv1_re, &yv1_im, yv1_re, yv1_im, yg0_re, yg0_im);
+
+	/*
+	 * Reorder into 0:1:2:3 and 4:5:6:7
+	 */
+	yt0_re = _mm256_unpacklo_epi64(yv0_re, yv1_re);
+	yt0_im = _mm256_unpacklo_epi64(yv0_im, yv1_im);
+	yt1_re = _mm256_unpackhi_epi64(yv0_re, yv1_re);
+	yt1_im = _mm256_unpackhi_epi64(yv0_im, yv1_im);
+	yv0_re = _mm256_permute4x64_epi64(yt0_re, 0xD8);
+	yv0_im = _mm256_permute4x64_epi64(yt0_im, 0xD8);
+	yv1_re = _mm256_permute4x64_epi64(yt1_re, 0xD8);
+	yv1_im = _mm256_permute4x64_epi64(yt1_im, 0xD8);
+
+	*ya0_re = yv0_re;
+	*ya0_im = yv0_im;
+	*ya1_re = yv1_re;
+	*ya1_im = yv1_im;
+}
+#endif // NTRUGEN_AVX2
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+vect_iFFT(unsigned logn, fxr *f)
+{
+	size_t hn = (size_t)1 << (logn - 1);
+	size_t ht = 1;
+#if NTRUGEN_AVX2
+	if (logn >= 4) {
+		size_t m1 = hn >> 1;
+		size_t hm1 = m1 >> 1;
+		for (size_t i = 0; i < hm1; i += 2) {
+			__m256i ya0_re = _mm256_loadu_si256(
+				(const __m256i *)(f + (i << 2) + 0));
+			__m256i ya1_re = _mm256_loadu_si256(
+				(const __m256i *)(f + (i << 2) + 4));
+			__m256i ya0_im = _mm256_loadu_si256(
+				(const __m256i *)(f + hn + (i << 2) + 0));
+			__m256i ya1_im = _mm256_loadu_si256(
+				(const __m256i *)(f + hn + (i << 2) + 4));
+			fxp_iFFT4(&ya0_re, &ya1_re, &ya0_im, &ya1_im, m1 + i);
+			_mm256_storeu_si256(
+				(__m256i *)(f + (i << 2) + 0), ya0_re);
+			_mm256_storeu_si256(
+				(__m256i *)(f + (i << 2) + 4), ya1_re);
+			_mm256_storeu_si256(
+				(__m256i *)(f + hn + (i << 2) + 0), ya0_im);
+			_mm256_storeu_si256(
+				(__m256i *)(f + hn + (i << 2) + 4), ya1_im);
+		}
+		ht = 4;
+
+		for (unsigned lm = logn - 3; lm > 0; lm --) {
+			size_t m = (size_t)1 << lm;
+			size_t t = ht << 1;
+			size_t j0 = 0;
+			size_t hm = m >> 1;
+			for (size_t i = 0; i < hm; i ++) {
+				fxc s = GM_TAB[m + i];
+				__m256i ys_re = _mm256_set1_epi64x(s.re.v);
+				__m256i ys_im = _mm256_set1_epi64x(-s.im.v);
+				for (size_t j = j0; j < j0 + ht; j += 4) {
+					__m256i ya_re = _mm256_loadu_si256(
+						(__m256i *)(f + j));
+					__m256i ya_im = _mm256_loadu_si256(
+						(__m256i *)(f + j + hn));
+					__m256i yb_re = _mm256_loadu_si256(
+						(__m256i *)(f + j + ht));
+					__m256i yb_im = _mm256_loadu_si256(
+						(__m256i *)(f + j + ht + hn));
+					__m256i yc_re = fxr_half_x4(
+						_mm256_add_epi64(ya_re, yb_re));
+					__m256i yc_im = fxr_half_x4(
+						_mm256_add_epi64(ya_im, yb_im));
+					__m256i yd_re = fxr_half_x4(
+						_mm256_sub_epi64(ya_re, yb_re));
+					__m256i yd_im = fxr_half_x4(
+						_mm256_sub_epi64(ya_im, yb_im));
+					fxc_mul_x4(&yd_re, &yd_im,
+						yd_re, yd_im, ys_re, ys_im);
+					_mm256_storeu_si256(
+						(__m256i *)(f + j),
+						yc_re);
+					_mm256_storeu_si256(
+						(__m256i *)(f + j + hn),
+						yc_im);
+					_mm256_storeu_si256(
+						(__m256i *)(f + j + ht),
+						yd_re);
+					_mm256_storeu_si256(
+						(__m256i *)(f + j + ht + hn),
+						yd_im);
+				}
+				j0 += t;
+			}
+			ht = t;
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (unsigned lm = logn - 1; lm > 0; lm --) {
+		size_t m = (size_t)1 << lm;
+		size_t t = ht << 1;
+		size_t j0 = 0;
+		size_t hm = m >> 1;
+		for (size_t i = 0; i < hm; i ++) {
+			fxc s = fxc_conj(GM_TAB[m + i]);
+			for (size_t j = j0; j < j0 + ht; j ++) {
+				fxc x, y;
+				x.re = f[j];
+				x.im = f[j + hn];
+				y.re = f[j + ht];
+				y.im = f[j + ht + hn];
+				fxc z1 = fxc_half(fxc_add(x, y));
+				f[j] = z1.re;
+				f[j + hn] = z1.im;
+				fxc z2 = fxc_mul(s, fxc_half(fxc_sub(x, y)));
+				f[j + ht] = z2.re;
+				f[j + ht + hn] = z2.im;
+			}
+			j0 += t;
+		}
+		ht = t;
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+vect_set(unsigned logn, fxr *d, const int8_t *f)
+{
+	size_t n = (size_t)1 << logn;
+#if NTRUGEN_AVX2
+	if (logn >= 4) {
+		for (size_t u = 0; u < n; u += 16) {
+			__m128i xf = _mm_loadu_si128((__m128i *)(f + u));
+			__m256i ya0 = _mm256_cvtepi8_epi64(xf);
+			__m256i ya1 = _mm256_cvtepi8_epi64(
+				_mm_bsrli_si128(xf, 4));
+			__m256i ya2 = _mm256_cvtepi8_epi64(
+				_mm_bsrli_si128(xf, 8));
+			__m256i ya3 = _mm256_cvtepi8_epi64(
+				_mm_bsrli_si128(xf, 12));
+			_mm256_storeu_si256((__m256i *)(d + u + 0),
+				_mm256_slli_epi64(ya0, 32));
+			_mm256_storeu_si256((__m256i *)(d + u + 4),
+				_mm256_slli_epi64(ya1, 32));
+			_mm256_storeu_si256((__m256i *)(d + u + 8),
+				_mm256_slli_epi64(ya2, 32));
+			_mm256_storeu_si256((__m256i *)(d + u + 12),
+				_mm256_slli_epi64(ya3, 32));
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (size_t u = 0; u < n; u ++) {
+		d[u] = fxr_of(f[u]);
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+vect_add(unsigned logn, fxr *restrict a, const fxr *restrict b)
+{
+	size_t n = (size_t)1 << logn;
+#if NTRUGEN_AVX2
+	if (logn >= 2) {
+		for (size_t u = 0; u < n; u += 4) {
+			__m256i ya = _mm256_loadu_si256((__m256i *)(a + u));
+			__m256i yb = _mm256_loadu_si256((__m256i *)(b + u));
+			__m256i yd = _mm256_add_epi64(ya, yb);
+			_mm256_storeu_si256((__m256i *)(a + u), yd);
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (size_t u = 0; u < n; u ++) {
+		a[u] = fxr_add(a[u], b[u]);
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+vect_mul_realconst(unsigned logn, fxr *a, fxr c)
+{
+	size_t n = (size_t)1 << logn;
+#if NTRUGEN_AVX2
+	if (logn >= 2) {
+		__m256i yc = _mm256_set1_epi64x(c.v);
+		for (size_t u = 0; u < n; u += 4) {
+			__m256i ya = _mm256_loadu_si256((__m256i *)(a + u));
+			__m256i yd = fxr_mul_x4(ya, yc);
+			_mm256_storeu_si256((__m256i *)(a + u), yd);
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (size_t u = 0; u < n; u ++) {
+		a[u] = fxr_mul(a[u], c);
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+vect_mul2e(unsigned logn, fxr *a, unsigned e)
+{
+	size_t n = (size_t)1 << logn;
+#if NTRUGEN_AVX2
+	if (logn >= 2) {
+		__m256i ye = _mm256_set1_epi64x(e);
+		for (size_t u = 0; u < n; u += 4) {
+			__m256i ya = _mm256_loadu_si256((__m256i *)(a + u));
+			ya = _mm256_sllv_epi64(ya, ye);
+			_mm256_storeu_si256((__m256i *)(a + u), ya);
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (size_t u = 0; u < n; u ++) {
+		a[u] = fxr_mul2e(a[u], e);
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+vect_mul_fft(unsigned logn, fxr *restrict a, const fxr *restrict b)
+{
+	size_t hn = (size_t)1 << (logn - 1);
+#if NTRUGEN_AVX2
+	if (logn >= 3) {
+		for (size_t u = 0; u < hn; u += 4) {
+			__m256i ya_re = _mm256_loadu_si256(
+				(const __m256i *)(a + u));
+			__m256i ya_im = _mm256_loadu_si256(
+				(const __m256i *)(a + u + hn));
+			__m256i yb_re = _mm256_loadu_si256(
+				(const __m256i *)(b + u));
+			__m256i yb_im = _mm256_loadu_si256(
+				(const __m256i *)(b + u + hn));
+			__m256i yd_re, yd_im;
+			fxc_mul_x4(&yd_re, &yd_im, ya_re, ya_im, yb_re, yb_im);
+			_mm256_storeu_si256((__m256i *)(a + u), yd_re);
+			_mm256_storeu_si256((__m256i *)(a + u + hn), yd_im);
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (size_t u = 0; u < hn; u ++) {
+		fxc x, y;
+		x.re = a[u];
+		x.im = a[u + hn];
+		y.re = b[u];
+		y.im = b[u + hn];
+		fxc z = fxc_mul(x, y);
+		a[u] = z.re;
+		a[u + hn] = z.im;
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+vect_adj_fft(unsigned logn, fxr *a)
+{
+	size_t hn = (size_t)1 << (logn - 1);
+	size_t n = (size_t)1 << logn;
+#if NTRUGEN_AVX2
+	if (logn >= 3) {
+		for (size_t u = hn; u < n; u += 4) {
+			__m256i y = _mm256_loadu_si256((__m256i *)(a + u));
+			y = _mm256_sub_epi64(_mm256_setzero_si256(), y);
+			_mm256_storeu_si256((__m256i *)(a + u), y);
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (size_t u = hn; u < n; u ++) {
+		a[u] = fxr_neg(a[u]);
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+vect_mul_autoadj_fft(unsigned logn, fxr *restrict a, const fxr *restrict b)
+{
+	size_t hn = (size_t)1 << (logn - 1);
+#if NTRUGEN_AVX2
+	if (logn >= 3) {
+		for (size_t u = 0; u < hn; u += 4) {
+			__m256i ya_re = _mm256_loadu_si256(
+				(__m256i *)(a + u));
+			__m256i ya_im = _mm256_loadu_si256(
+				(__m256i *)(a + u + hn));
+			__m256i yb = _mm256_loadu_si256(
+				(__m256i *)(b + u));
+			_mm256_storeu_si256((__m256i *)(a + u),
+				fxr_mul_x4(ya_re, yb));
+			_mm256_storeu_si256((__m256i *)(a + u + hn),
+				fxr_mul_x4(ya_im, yb));
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (size_t u = 0; u < hn; u ++) {
+		a[u] = fxr_mul(a[u], b[u]);
+		a[u + hn] = fxr_mul(a[u + hn], b[u]);
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+vect_div_autoadj_fft(unsigned logn, fxr *restrict a, const fxr *restrict b)
+{
+	size_t hn = (size_t)1 << (logn - 1);
+#if NTRUGEN_AVX2
+	if (logn >= 3) {
+		for (size_t u = 0; u < hn; u += 4) {
+			__m256i ya_re = _mm256_loadu_si256(
+				(const __m256i *)(a + u));
+			__m256i ya_im = _mm256_loadu_si256(
+				(const __m256i *)(a + u + hn));
+			__m256i yb = _mm256_loadu_si256(
+				(const __m256i *)(b + u));
+			ya_re = fxr_div_x4(ya_re, yb);
+			ya_im = fxr_div_x4(ya_im, yb);
+			_mm256_storeu_si256((__m256i *)(a + u), ya_re);
+			_mm256_storeu_si256((__m256i *)(a + u + hn), ya_im);
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (size_t u = 0; u < hn; u ++) {
+		a[u] = fxr_div(a[u], b[u]);
+		a[u + hn] = fxr_div(a[u + hn], b[u]);
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+vect_norm_fft(unsigned logn, fxr *restrict d,
+	const fxr *restrict a, const fxr *restrict b)
+{
+	size_t hn = (size_t)1 << (logn - 1);
+#if NTRUGEN_AVX2
+	if (logn >= 3) {
+		for (size_t u = 0; u < hn; u += 4) {
+			__m256i ya_re = _mm256_loadu_si256(
+				(const __m256i *)(a + u));
+			__m256i ya_im = _mm256_loadu_si256(
+				(const __m256i *)(a + u + hn));
+			__m256i yb_re = _mm256_loadu_si256(
+				(const __m256i *)(b + u));
+			__m256i yb_im = _mm256_loadu_si256(
+				(const __m256i *)(b + u + hn));
+			__m256i y0 = fxr_sqr_x4(ya_re);
+			__m256i y1 = fxr_sqr_x4(ya_im);
+			__m256i y2 = fxr_sqr_x4(yb_re);
+			__m256i y3 = fxr_sqr_x4(yb_im);
+			__m256i yd = _mm256_add_epi64(
+				_mm256_add_epi64(y0, y1),
+				_mm256_add_epi64(y2, y3));
+			_mm256_storeu_si256((__m256i *)(d + u), yd);
+		}
+	}
+#endif // NTRUGEN_AVX2
+	for (size_t u = 0; u < hn; u ++) {
+		d[u] = fxr_add(
+			fxr_add(fxr_sqr(a[u]), fxr_sqr(a[u + hn])),
+			fxr_add(fxr_sqr(b[u]), fxr_sqr(b[u + hn])));
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+vect_invnorm_fft(unsigned logn, fxr *restrict d,
+	const fxr *restrict a, const fxr *restrict b, unsigned e)
+{
+	size_t hn = (size_t)1 << (logn - 1);
+	fxr fe = fxr_of((int32_t)1 << e);
+#if NTRUGEN_AVX2
+	if (logn >= 3) {
+		__m256i yfe = _mm256_set1_epi64x(fe.v);
+		for (size_t u = 0; u < hn; u += 4) {
+			__m256i ya_re = _mm256_loadu_si256(
+				(const __m256i *)(a + u));
+			__m256i ya_im = _mm256_loadu_si256(
+				(const __m256i *)(a + u + hn));
+			__m256i yb_re = _mm256_loadu_si256(
+				(const __m256i *)(b + u));
+			__m256i yb_im = _mm256_loadu_si256(
+				(const __m256i *)(b + u + hn));
+			__m256i y0 = fxr_sqr_x4(ya_re);
+			__m256i y1 = fxr_sqr_x4(ya_im);
+			__m256i y2 = fxr_sqr_x4(yb_re);
+			__m256i y3 = fxr_sqr_x4(yb_im);
+			__m256i yz = _mm256_add_epi64(
+				_mm256_add_epi64(y0, y1),
+				_mm256_add_epi64(y2, y3));
+			__m256i yd = fxr_div_x4(yfe, yz);
+			_mm256_storeu_si256((__m256i *)(d + u), yd);
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (size_t u = 0; u < hn; u ++) {
+		fxr z = fxr_add(
+			fxr_add(fxr_sqr(a[u]), fxr_sqr(a[u + hn])),
+			fxr_add(fxr_sqr(b[u]), fxr_sqr(b[u + hn])));
+		d[u] = fxr_div(fe, z);
+	}
+}
diff --git a/lib/dns/hawk/ng_hawk.c b/lib/dns/hawk/ng_hawk.c
new file mode 100644
index 0000000000..c1f1039fd3
--- /dev/null
+++ b/lib/dns/hawk/ng_hawk.c
@@ -0,0 +1,934 @@
+#include "ng_inner.h"
+
+static const ntru_profile SOLVE_Hawk_256 = {
+	1,
+	8, 8,
+	{ 1, 1, 1, 2, 3, 5, 9, 17, 34, 0, 0 },
+	{ 1, 1, 2, 4, 7, 13, 26, 50, 0, 0 },
+	{ 1, 1, 1, 2, 3, 3, 3, 4, 0, 0 },
+	20,
+	{ 0, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127 },
+	{ 0, 0, 1, 2, 2, 2, 2, 2, 2, 3, 3 }
+};
+
+static const ntru_profile SOLVE_Hawk_512 = {
+	1,
+	9, 9,
+	{ 1, 1, 1, 2, 3, 6, 11, 21, 41, 82, 0 },
+	{ 1, 2, 3, 5, 8, 16, 31, 61, 121, 0 },
+	{ 1, 1, 1, 2, 2, 3, 3, 4, 6, 0 },
+	15,
+	{ 0, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127 },
+	{ 0, 0, 1, 2, 2, 2, 2, 2, 2, 3, 3 }
+};
+
+static const ntru_profile SOLVE_Hawk_1024 = {
+	1,
+	10, 10,
+	{ 1, 1, 2, 2, 4, 7, 13, 25, 48, 96, 191 },
+	{ 1, 2, 3, 5, 10, 19, 37, 72, 143, 284 },
+	{ 1, 1, 2, 2, 3, 3, 3, 4, 4, 7 },
+	12,
+	{ 0, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127 },
+	{ 0, 0, 1, 2, 2, 2, 2, 2, 2, 3, 3 }
+};
+
+#if 0 /* obsolete */
+/*
+ * Tables for a Gaussian distribution of (f,g), scaled by 32767.
+ */
+
+/* Hawk, q = 1, n = 256 -> kmax = 4 */
+static const uint16_t gauss_Hawk_256[] = {
+	4,
+	   16,   305,  2580, 10442, 22325, 30187, 32462, 32751
+};
+
+/* Hawk, q = 1, n = 512 -> kmax = 6 */
+static const uint16_t gauss_Hawk_512[] = {
+	6,
+	    3,    37,   286,  1465,  5048, 12026, 20741, 27719,
+	31302, 32481, 32730, 32764
+};
+
+/* Hawk, q = 1, n = 1024 -> kmax = 8 */
+static const uint16_t gauss_Hawk_1024[] = {
+	8,
+	    2,    17,    89,   377,  1261,  3383,  7347, 13115,
+	19652, 25420, 29384, 31506, 32390, 32678, 32750, 32765
+};
+#endif
+
+TARGET_AVX2
+static void
+regen_fg_8(int8_t *restrict f, int8_t *restrict g, const void *seed)
+{
+	size_t seed_len = 16;
+#if NTRUGEN_AVX2
+	/*
+	 * Initialize the four SHAKE256 instances, to run them in parallel.
+	 */
+	shake_context sc[4];
+	for (int i = 0; i < 4; i ++) {
+		shake_init(&sc[i], 256);
+		shake_inject(&sc[i], seed, seed_len);
+		uint8_t ix = (uint8_t)i;
+		shake_inject(&sc[i], &ix, 1);
+	}
+	shake_x4_context scx4;
+	shake_x4_flip(&scx4, sc);
+	__m256i ym4 = _mm256_set1_epi8(0x0F);
+	__m256i ytt = _mm256_setr_epi8(
+		-2, -1, -1, 0, -1, 0, 0, 1, -1, 0, 0, 1, 0, 1, 1, 2,
+		-2, -1, -1, 0, -1, 0, 0, 1, -1, 0, 0, 1, 0, 1, 1, 2);
+	for (size_t u = 0; u < 512; u += 64) {
+		union {
+			__m256i y;
+			uint64_t q[4];
+		} buf;
+		shake_x4_extract_words(&scx4, buf.q, 1);
+		__m256i y0 = _mm256_and_si256(buf.y, ym4);
+		__m256i y1 = _mm256_and_si256(_mm256_srli_epi16(buf.y, 4), ym4);
+		y0 = _mm256_shuffle_epi8(ytt, y0);
+		y1 = _mm256_shuffle_epi8(ytt, y1);
+		__m256i yv0 = _mm256_unpacklo_epi8(y0, y1);
+		__m256i yv1 = _mm256_unpackhi_epi8(y0, y1);
+		__m256i yd0 = _mm256_permute2x128_si256(yv0, yv1, 0x20);
+		__m256i yd1 = _mm256_permute2x128_si256(yv0, yv1, 0x31);
+		if (u < 256) {
+			_mm256_storeu_si256((__m256i *)(f + u +   0), yd0);
+			_mm256_storeu_si256((__m256i *)(f + u +  32), yd1);
+		} else {
+			_mm256_storeu_si256((__m256i *)(g + u - 256), yd0);
+			_mm256_storeu_si256((__m256i *)(g + u - 224), yd1);
+		}
+	}
+#else // NTRUGEN_AVX2
+	for (size_t j = 0; j < 4; j ++) {
+		shake_context sc;
+		shake_init(&sc, 256);
+		shake_inject(&sc, seed, seed_len);
+		uint8_t jx = (uint8_t)j;
+		shake_inject(&sc, &jx, 1);
+		shake_flip(&sc);
+		for (size_t u = 0; u < 512; u += 64) {
+			uint8_t qb[8];
+			shake_extract(&sc, qb, 8);
+			uint64_t q = dec64le(qb);
+			q = (q & (uint64_t)0x5555555555555555)
+				+ ((q >> 1) & (uint64_t)0x5555555555555555);
+			q = (q & (uint64_t)0x3333333333333333)
+				+ ((q >> 2) & (uint64_t)0x3333333333333333);
+			int8_t vv[16];
+			for (int i = 0; i < 16; i ++) {
+				vv[i] = (int)(q & 0x0F) - 2;
+				q >>= 4;
+			}
+			if (u < 256) {
+				memcpy(f + u + (j << 4), vv, 16);
+			} else {
+				memcpy(g + (u - 256) + (j << 4), vv, 16);
+			}
+		}
+	}
+#endif // NTRUGEN_AVX2
+}
+
+TARGET_AVX2
+static void
+regen_fg_9(int8_t *restrict f, int8_t *restrict g, const void *seed)
+{
+	size_t seed_len = 24;
+#if NTRUGEN_AVX2
+	/*
+	 * Initialize the four SHAKE256 instances, to run them in parallel.
+	 */
+	shake_context sc[4];
+	for (int i = 0; i < 4; i ++) {
+		shake_init(&sc[i], 256);
+		shake_inject(&sc[i], seed, seed_len);
+		uint8_t ix = (uint8_t)i;
+		shake_inject(&sc[i], &ix, 1);
+	}
+	shake_x4_context scx4;
+	shake_x4_flip(&scx4, sc);
+	__m256i ym4 = _mm256_set1_epi8(0x0F);
+	__m256i ytt = _mm256_setr_epi8(
+		-2, -1, -1, 0, -1, 0, 0, 1, -1, 0, 0, 1, 0, 1, 1, 2,
+		-2, -1, -1, 0, -1, 0, 0, 1, -1, 0, 0, 1, 0, 1, 1, 2);
+	for (size_t u = 0; u < 1024; u += 32) {
+		union {
+			__m256i y;
+			uint64_t q[4];
+		} buf;
+		shake_x4_extract_words(&scx4, buf.q, 1);
+		__m256i y0 = _mm256_and_si256(buf.y, ym4);
+		__m256i y1 = _mm256_and_si256(_mm256_srli_epi16(buf.y, 4), ym4);
+		y0 = _mm256_shuffle_epi8(ytt, y0);
+		y1 = _mm256_shuffle_epi8(ytt, y1);
+		__m256i yv = _mm256_add_epi8(y0, y1);
+		if (u < 512) {
+			_mm256_storeu_si256((__m256i *)(f + u +   0), yv);
+		} else {
+			_mm256_storeu_si256((__m256i *)(g + u - 512), yv);
+		}
+	}
+#else // NTRUGEN_AVX2
+	for (size_t j = 0; j < 4; j ++) {
+		shake_context sc;
+		shake_init(&sc, 256);
+		shake_inject(&sc, seed, seed_len);
+		uint8_t jx = (uint8_t)j;
+		shake_inject(&sc, &jx, 1);
+		shake_flip(&sc);
+		for (size_t u = 0; u < 1024; u += 32) {
+			uint8_t qb[8];
+			shake_extract(&sc, qb, 8);
+			uint64_t q = dec64le(qb);
+			q = (q & (uint64_t)0x5555555555555555)
+				+ ((q >> 1) & (uint64_t)0x5555555555555555);
+			q = (q & (uint64_t)0x3333333333333333)
+				+ ((q >> 2) & (uint64_t)0x3333333333333333);
+			q = (q & (uint64_t)0x0F0F0F0F0F0F0F0F)
+				+ ((q >> 4) & (uint64_t)0x0F0F0F0F0F0F0F0F);
+			int8_t vv[8];
+			for (int i = 0; i < 8; i ++) {
+				vv[i] = (int)(q & 0xFF) - 4;
+				q >>= 8;
+			}
+			if (u < 512) {
+				memcpy(f + u + (j << 3), vv, 8);
+			} else {
+				memcpy(g + (u - 512) + (j << 3), vv, 8);
+			}
+		}
+	}
+#endif // NTRUGEN_AVX2
+}
+
+TARGET_AVX2
+static void
+regen_fg_10(int8_t *restrict f, int8_t *restrict g, const void *seed)
+{
+	size_t seed_len = 40;
+#if NTRUGEN_AVX2
+	/*
+	 * Initialize the four SHAKE256 instances, to run them in parallel.
+	 */
+	shake_context sc[4];
+	for (int i = 0; i < 4; i ++) {
+		shake_init(&sc[i], 256);
+		shake_inject(&sc[i], seed, seed_len);
+		uint8_t ix = (uint8_t)i;
+		shake_inject(&sc[i], &ix, 1);
+	}
+	shake_x4_context scx4;
+	shake_x4_flip(&scx4, sc);
+	__m256i ym4 = _mm256_set1_epi8(0x0F);
+	__m256i ytt = _mm256_setr_epi8(
+		-2, -1, -1, 0, -1, 0, 0, 1, -1, 0, 0, 1, 0, 1, 1, 2,
+		-2, -1, -1, 0, -1, 0, 0, 1, -1, 0, 0, 1, 0, 1, 1, 2);
+	__m256i ys = _mm256_setr_epi8(
+		0, 2, 4, 6, 8, 10, 12, 14,
+		-1, -1, -1, -1, -1, -1, -1, -1,
+		0, 2, 4, 6, 8, 10, 12, 14,
+		-1, -1, -1, -1, -1, -1, -1, -1);
+	for (size_t u = 0; u < 2048; u += 16) {
+		union {
+			__m256i y;
+			uint64_t q[4];
+		} buf;
+		shake_x4_extract_words(&scx4, buf.q, 1);
+		__m256i y0 = _mm256_and_si256(buf.y, ym4);
+		__m256i y1 = _mm256_and_si256(_mm256_srli_epi16(buf.y, 4), ym4);
+		y0 = _mm256_shuffle_epi8(ytt, y0);
+		y1 = _mm256_shuffle_epi8(ytt, y1);
+		__m256i yv = _mm256_add_epi8(y0, y1);
+		yv = _mm256_add_epi8(yv, _mm256_srli_epi16(yv, 8));
+		yv = _mm256_shuffle_epi8(yv, ys);
+		yv = _mm256_permute4x64_epi64(yv, 0xD8);
+		__m128i xv = _mm256_castsi256_si128(yv);
+		if (u < 1024) {
+			_mm_storeu_si128((__m128i *)(f + u +    0), xv);
+		} else {
+			_mm_storeu_si128((__m128i *)(g + u - 1024), xv);
+		}
+	}
+#else // NTRUGEN_AVX2
+	for (size_t j = 0; j < 4; j ++) {
+		shake_context sc;
+		shake_init(&sc, 256);
+		shake_inject(&sc, seed, seed_len);
+		uint8_t jx = (uint8_t)j;
+		shake_inject(&sc, &jx, 1);
+		shake_flip(&sc);
+		for (size_t u = 0; u < 2048; u += 16) {
+			uint8_t qb[8];
+			shake_extract(&sc, qb, 8);
+			uint64_t q = dec64le(qb);
+			q = (q & (uint64_t)0x5555555555555555)
+				+ ((q >> 1) & (uint64_t)0x5555555555555555);
+			q = (q & (uint64_t)0x3333333333333333)
+				+ ((q >> 2) & (uint64_t)0x3333333333333333);
+			q = (q & (uint64_t)0x0F0F0F0F0F0F0F0F)
+				+ ((q >> 4) & (uint64_t)0x0F0F0F0F0F0F0F0F);
+			q = (q & (uint64_t)0x00FF00FF00FF00FF)
+				+ ((q >> 8) & (uint64_t)0x00FF00FF00FF00FF);
+			int8_t vv[4];
+			for (int i = 0; i < 4; i ++) {
+				vv[i] = (int)(q & 0xFFFF) - 8;
+				q >>= 16;
+			}
+			if (u < 1024) {
+				memcpy(f + u + (j << 2), vv, 4);
+			} else {
+				memcpy(g + (u - 1024) + (j << 2), vv, 4);
+			}
+		}
+	}
+#endif // NTRUGEN_AVX2
+}
+
+/* see ntrugen.h */
+TARGET_AVX2
+void
+Hawk_regen_fg(unsigned logn,
+	int8_t *restrict f, int8_t *restrict g, const void *seed)
+{
+	switch (logn) {
+	case 8:
+		regen_fg_8(f, g, seed);
+		break;
+	case 9:
+		regen_fg_9(f, g, seed);
+		break;
+	default:
+		regen_fg_10(f, g, seed);
+		break;
+	}
+
+#if 0
+	const uint16_t *tab;
+	switch (logn) {
+	case 8:   tab = gauss_Hawk_256;   break;
+	case 9:   tab = gauss_Hawk_512;   break;
+	default:  tab = gauss_Hawk_1024;  break;
+	}
+	size_t n = (size_t)1 << logn;
+	size_t kmax = tab[0];
+	size_t seed_len = 8 + ((size_t)1 << (logn - 5));
+
+#if NTRUGEN_AVX2
+
+	/*
+	 * Initialize the four SHAKE256 instances, to run them in parallel.
+	 */
+	shake_context sc[4];
+	for (int i = 0; i < 4; i ++) {
+		shake_init(&sc[i], 256);
+		shake_inject(&sc[i], seed, seed_len);
+		uint8_t ix = (uint8_t)i;
+		shake_inject(&sc[i], &ix, 1);
+	}
+	shake_x4_context scx4;
+	shake_x4_flip(&scx4, sc);
+
+	__m256i ytab[16];
+	for (size_t u = 0; u < (kmax << 1); u ++) {
+		ytab[u] = _mm256_set1_epi16(tab[u + 1]);
+	}
+	__m256i yb = _mm256_set1_epi16(-(int)kmax);
+	__m256i ys = _mm256_setr_epi8(
+		0, 2, 4, 6, 8, 10, 12, 14,
+		-1, -1, -1, -1, -1, -1, -1, -1,
+		0, 2, 4, 6, 8, 10, 12, 14,
+		-1, -1, -1, -1, -1, -1, -1, -1);
+	__m256i y15 = _mm256_set1_epi16(0x7FFF);
+
+	for (size_t u = 0; u < (n << 1); u += 16) {
+		union {
+			__m256i y;
+			uint64_t q[4];
+		} buf;
+		shake_x4_extract_words(&scx4, buf.q, 1);
+		__m256i yr = _mm256_and_si256(buf.y, y15);
+		__m256i yv = yb;
+		for (size_t k = 0; k < (kmax << 1); k ++) {
+			yv = _mm256_sub_epi16(yv,
+				_mm256_cmpgt_epi16(yr, ytab[k]));
+		}
+		yv = _mm256_shuffle_epi8(yv, ys);
+		yv = _mm256_permute4x64_epi64(yv, 0xD8);
+		__m128i xv = _mm256_castsi256_si128(yv);
+		if (u < n) {
+			_mm_storeu_si128((__m128i *)(f + u), xv);
+		} else {
+			_mm_storeu_si128((__m128i *)(g + (u - n)), xv);
+		}
+	}
+
+#else // NTRUGEN_AVX2
+
+	for (size_t j = 0; j < 4; j ++) {
+		shake_context sc;
+		shake_init(&sc, 256);
+		shake_inject(&sc, seed, seed_len);
+		uint8_t jx = (uint8_t)j;
+		shake_inject(&sc, &jx, 1);
+		shake_flip(&sc);
+		for (size_t u = 0; u < (n << 1); u += 16) {
+			uint8_t qb[8];
+			shake_extract(&sc, qb, 8);
+			uint64_t q = dec64le(qb);
+			int8_t vv[4];
+			for (int i = 0; i < 4; i ++) {
+				uint32_t x = (uint32_t)q & 0x7FFF;
+				q >>= 16;
+				uint32_t v = -(uint32_t)kmax;
+				for (size_t k = 1; k <= (kmax << 1); k ++) {
+					v += ((uint32_t)tab[k] - x) >> 31;
+				}
+				vv[i] = (int8_t)*(int32_t *)&v;
+			}
+			if (u < n) {
+				memcpy(f + u + (j << 2), vv, 4);
+			} else {
+				memcpy(g + (u - n) + (j << 2), vv, 4);
+			}
+		}
+	}
+
+#endif // NTRUGEN_AVX2
+#endif
+}
+
+TARGET_AVX2
+static unsigned
+parity(unsigned logn, int8_t *f)
+{
+	size_t n = (size_t)1 << logn;
+#if NTRUGEN_AVX2
+	__m256i yr = _mm256_setzero_si256();
+	for (size_t u = 0; u < n; u += 32) {
+		__m256i y = _mm256_loadu_si256((const __m256i *)(f + u));
+		yr = _mm256_xor_si256(y, yr);
+	}
+	uint32_t r = (uint32_t)_mm256_movemask_epi8(_mm256_slli_epi16(yr, 7));
+	r ^= (r >> 16);
+	r ^= (r >> 8);
+	r ^= (r >> 4);
+	r ^= (r >> 2);
+	r ^= (r >> 1);
+	return (unsigned)(r & 1);
+#else // NTRUGEN_AVX2
+	unsigned pp = 0;
+	for (size_t u = 0; u < n; u ++) {
+		pp += *(uint8_t *)&f[u];
+	}
+	return pp & 1;
+#endif // NTRUGEN_AVX2
+}
+
+/*
+ * Limits for q00, q01 and q11 (maximum bit size of the absolute value of
+ * a coefficient, excluding q00[0] and q11[0]).
+ */
+static const int8_t bits_lim00[11] = {
+	0, 0, 0, 0, 0, 0, 0, 0,  9,  9, 10
+};
+static const int8_t bits_lim01[11] = {
+	0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 14
+};
+static const int8_t bits_lim11[11] = {
+	0, 0, 0, 0, 0, 0, 0, 0, 13, 15, 17
+};
+
+/*
+ * Given f, g, F and G, compute:
+ *   q00 = f*adj(f) + g*adj(g)
+ *   q01 = F*adj(f) + G*adj(g)
+ *   q11 = F*adj(f) + G*adj(G)
+ * The three polynomials are stored at the start of tmp[], in that order:
+ *    q00   int16_t[n]
+ *    q01   int16_t[n]
+ *    q11   int32_t[n]
+ * Note: q00 and q11 are auto-adjoint.
+ *
+ * Return value is 1 on success, 0 on error. An error is reported if
+ * any of the coefficients does not comply with the following limits:
+ *    -32768 <= q00[0] < 32768
+ *    -lim00 <= q00[u] < +lim00    for u = 1 to n  
+ *    -lim01 <= q01[u] < +lim01    for u = 0 to n  
+ *    -lim11 <= q11[u] < +lim11    for u = 1 to n
+ * An error is also reported if q00 turns out not to be invertible modulo
+ * X^n+1 and modulo 2147473409.
+ *
+ * RAM USAGE: 5*n words
+ */
+static int
+make_q001(unsigned logn,
+	int lim00, int lim01, int32_t lim11,
+	const int8_t *restrict f, const int8_t *restrict g,
+	const int8_t *restrict F, const int8_t *restrict G,
+	uint32_t *restrict tmp)
+{
+	size_t n = (size_t)1 << logn;
+	size_t hn = n >> 1;
+	uint32_t p = PRIMES[0].p;
+	uint32_t p0i = PRIMES[0].p0i;
+	uint32_t R2 = PRIMES[0].R2;
+
+	uint32_t *t1 = tmp;
+	uint32_t *t2 = t1 + n;
+	uint32_t *t3 = t2 + n;
+	uint32_t *t4 = t3 + n;
+	uint32_t *t5 = t4 + n;
+
+	mp_mkgm(logn, t1, PRIMES[0].g, p, p0i);
+	poly_mp_set_small(logn, t2, f, p);
+	poly_mp_set_small(logn, t3, g, p);
+	poly_mp_set_small(logn, t4, F, p);
+	poly_mp_set_small(logn, t5, G, p);
+	mp_NTT(logn, t2, t1, p, p0i);
+	mp_NTT(logn, t3, t1, p, p0i);
+	mp_NTT(logn, t4, t1, p, p0i);
+	mp_NTT(logn, t5, t1, p, p0i);
+	for (size_t u = 0; u < hn; u ++) {
+		uint32_t xf = t2[u];
+		uint32_t xfa = t2[(n - 1) - u];
+		uint32_t xg = t3[u];
+		uint32_t xga = t3[(n - 1) - u];
+		uint32_t xF = t4[u];
+		uint32_t xFa = t4[(n - 1) - u];
+		uint32_t xG = t5[u];
+		uint32_t xGa = t5[(n - 1) - u];
+
+		uint32_t xq00 = mp_montymul(R2, mp_add(
+			mp_montymul(xf, xfa, p, p0i),
+			mp_montymul(xg, xga, p, p0i), p), p, p0i);
+		uint32_t xq11 = mp_montymul(R2, mp_add(
+			mp_montymul(xF, xFa, p, p0i),
+			mp_montymul(xG, xGa, p, p0i), p), p, p0i);
+		uint32_t xq01_0 = mp_montymul(R2, mp_add(
+			mp_montymul(xF, xfa, p, p0i),
+			mp_montymul(xG, xga, p, p0i), p), p, p0i);
+		uint32_t xq01_1 = mp_montymul(R2, mp_add(
+			mp_montymul(xFa, xf, p, p0i),
+			mp_montymul(xGa, xg, p, p0i), p), p, p0i);
+		if (xq00 == 0) {
+			/* q00 is not invertible mod X^n+1 mod p,
+			   we report the error right away (key is invalid
+			   and will be discarded). */
+			return 0;
+		}
+		t3[u] = xq00;
+		t3[(n - 1) - u] = xq00;
+		t4[u] = xq01_0;
+		t4[(n - 1) - u] = xq01_1;
+		t5[u] = xq11;
+		t5[(n - 1) - u] = xq11;
+	}
+	mp_mkigm(logn, t1, PRIMES[0].ig, p, p0i);
+	mp_iNTT(logn, t3, t1, p, p0i);
+	mp_iNTT(logn, t4, t1, p, p0i);
+	mp_iNTT(logn, t5, t1, p, p0i);
+
+	/*
+	 * t1 and t2 are free, we reuse them for the output.
+	 */
+	int16_t *q00 = (int16_t *)tmp;
+	int16_t *q01 = q00 + n;
+	int32_t *q11 = (int32_t *)(q01 + n);
+	for (size_t u = 0; u < n; u ++) {
+		int32_t xq00 = mp_norm(t3[u], p);
+		int32_t xq01 = mp_norm(t4[u], p);
+		int32_t xq11 = mp_norm(t5[u], p);
+		if (u == 0) {
+			if (xq00 < -32768 || xq00 > +32767) {
+				return 0;
+			}
+		} else {
+			if (xq00 <= -lim00 || xq00 >= +lim00) {
+				return 0;
+			}
+			if (xq11 <= -lim11 || xq11 >= +lim11) {
+				return 0;
+			}
+		}
+		if (xq01 <= -lim01 || xq01 >= +lim01) {
+			return 0;
+		}
+		q00[u] = (int16_t)xq00;
+		q01[u] = (int16_t)xq01;
+		q11[u] = xq11;
+	}
+
+	return 1;
+}
+
+/* see ntrugen.h */
+int
+Hawk_keygen(unsigned logn,
+	int8_t *restrict f, int8_t *restrict g,
+	int8_t *restrict F, int8_t *restrict G,
+	int16_t *restrict q00, int16_t *restrict q01, int32_t *restrict q11,
+	void *seed, ntrugen_rng rng, void *restrict rng_context,
+	void *restrict tmp, size_t tmp_len)
+{
+	/*
+	 * Ensure that the tmp[] buffer has proper alignment for 64-bit
+	 * access.
+	 */
+	if (tmp_len < 7) {
+		return -1;
+	}
+	if (logn < 2 || logn > 10) {
+		return -1;
+	}
+	uintptr_t utmp1 = (uintptr_t)tmp;
+	uintptr_t utmp2 = (utmp1 + 7) & ~(uintptr_t)7;
+	tmp_len -= (size_t)(utmp2 - utmp1);
+	uint32_t *tt32 = (void *)utmp2;
+	if (tmp_len < ((size_t)24 << logn)) {
+		return -1;
+	}
+
+	uint32_t l2low;
+	fxr d0high;
+	const ntru_profile *prof;
+	switch (logn) {
+	case 8:
+		l2low = 556;
+		d0high = fxr_of_scaled32(17179869); /* 1/250 */
+		prof = &SOLVE_Hawk_256;
+		break;
+	case 9:
+		l2low = 2080;
+		d0high = fxr_of_scaled32(4294967); /* 1/1000 */
+		prof = &SOLVE_Hawk_512;
+		break;
+	case 10:
+		l2low = 7981;
+		d0high = fxr_of_scaled32(1431655); /* 1/3000 */
+		prof = &SOLVE_Hawk_1024;
+		break;
+	default:
+		/*
+		 * Other degrees are not supported.
+		 */
+		return -1;
+	}
+	int lim00 = 1 << bits_lim00[logn];
+	int lim01 = 1 << bits_lim01[logn];
+	int lim11 = (int32_t)1 << bits_lim11[logn];
+
+	uint8_t seed_buf[40];
+	size_t seed_len = 8 + ((size_t)1 << (logn - 5));
+
+	for (;;) {
+		/*
+		 * Generate f and g.
+		 */
+		rng(rng_context, seed_buf, seed_len);
+		Hawk_regen_fg(logn, f, g, seed_buf);
+
+		/*
+		 * Start again if f and g are not both odd.
+		 */
+		if (parity(logn, f) != 1 || parity(logn, g) != 1) {
+			continue;
+		}
+
+		/*
+		 * Check that (f,g) has an acceptable norm; this is a
+		 * _minimum_ bound (2*n*sigma_sec^2).
+		 */
+		uint32_t norm2_fg = poly_sqnorm(logn, f) + poly_sqnorm(logn, g);
+		if (norm2_fg < l2low) {
+			continue;
+		}
+
+		/*
+		 * Check that f*adj(f) + g*adj(g) is invertible modulo
+		 * X^n+1 mod p1 (with p1 = 2147473409 = PRIMES[0].p).
+		 * We also output f*adj(f) + g*adj(g) into t1.
+		 */
+		int invertible = 1;
+		size_t n = (size_t)1 << logn;
+		size_t hn = n >> 1;
+		uint32_t *t1 = tt32;
+		uint32_t *t2 = t1 + n;
+		uint32_t *t3 = t2 + n;
+		uint32_t *t4 = t3 + n;
+		uint32_t p = PRIMES[0].p;
+		uint32_t p0i = PRIMES[0].p0i;
+		uint32_t R2 = PRIMES[0].R2;
+		mp_mkgmigm(logn, t1, t2, PRIMES[0].g, PRIMES[0].ig, p, p0i);
+		for (size_t u = 0; u < n; u ++) {
+			t3[u] = mp_set(f[u], p);
+			t4[u] = mp_set(g[u], p);
+		}
+		mp_NTT(logn, t3, t1, p, p0i);
+		mp_NTT(logn, t4, t1, p, p0i);
+		for (size_t u = 0; u < n; u ++) {
+			uint32_t x = mp_add(
+				mp_montymul(t3[u], t3[(n - 1) - u], p, p0i),
+				mp_montymul(t4[u], t4[(n - 1) - u], p, p0i), p);
+			/*
+			 * Value x is in anti-Montgomery representation;
+			 * it is enough to test invertibility, but we need
+			 * the actual value for the next test on (f,g).
+			 */
+			if (x == 0) {
+				invertible = 0;
+				break;
+			}
+			x = mp_montymul(R2, x, p, p0i);
+			t1[u] = x;
+		}
+		if (!invertible) {
+			continue;
+		}
+		/* Get the plain f*adj(f) + g*adj(g) */
+		mp_iNTT(logn, t1, t2, p, p0i);
+		for (size_t u = 0; u < n; u ++) {
+			t1[u] = (uint32_t)mp_norm(t1[u], p);
+		}
+
+		/*
+		 * Also check that f*adj(f) + g*adj(g) is invertible modulo
+		 * X^n+1 mod p2 (with p2 = 2147389441 = PRIMES[1].p).
+		 */
+		p = PRIMES[1].p;
+		p0i = PRIMES[1].p0i;
+		for (size_t u = 0; u < n; u ++) {
+			t2[u] = mp_set(*(int32_t *)&t1[u], p);
+		}
+		mp_mkgm(logn, t3, PRIMES[1].g, p, p0i);
+		mp_NTT(logn, t2, t3, p, p0i);
+		for (size_t u = 0; u < n; u ++) {
+			if (t2[u] == 0) {
+				invertible = 0;
+				break;
+			}
+		}
+		if (!invertible) {
+			continue;
+		}
+
+		/*
+		 * Check that the constant term of 1/(f*adj(f) + g*adj(g))
+		 * is small enough.
+		 */
+#if NTRUGEN_STATS
+		stats_hawk_ctt_attempt ++;
+#endif
+		fxr *rt1 = (fxr *)t2;
+		for (size_t u = 0; u < n; u ++) {
+			rt1[u] = fxr_of(*(int32_t *)&t1[u]);
+		}
+		vect_FFT(logn, rt1);
+		for (size_t u = 0; u < hn; u ++) {
+			rt1[u] = fxr_inv(rt1[u]);
+		}
+		/* Normally the values are already zero, or close to zero
+		   in case of loss of precision. We force them to zero. */
+		for (size_t u = hn; u < n; u ++) {
+			rt1[u] = fxr_zero;
+		}
+		vect_iFFT(logn, rt1);
+
+		if (fxr_lt(d0high, rt1[0])) {
+#if NTRUGEN_STATS
+			stats_hawk_ctt_reject ++;
+#endif
+			continue;
+		}
+
+		/*
+		 * Solve the NTRU equation.
+		 */
+#if NTRUGEN_STATS
+		stats_solve_attempt ++;
+#endif
+		int err = solve_NTRU(prof, logn, f, g, tt32);
+		switch (err) {
+		case SOLVE_OK:
+#if NTRUGEN_STATS
+			stats_solve_success ++;
+#endif
+			break;
+#if NTRUGEN_STATS
+		case SOLVE_ERR_GCD:
+			stats_solve_err_gcd ++;
+			continue;
+		case SOLVE_ERR_REDUCE:
+			stats_solve_err_reduce ++;
+			continue;
+		case SOLVE_ERR_LIMIT:
+			stats_solve_err_limit ++;
+			continue;
+#endif
+		default:
+			continue;
+		}
+
+		/*
+		 * F and G are at the start of tt32[].
+		 */
+		int8_t *tF = (int8_t *)tt32;
+		int8_t *tG = tF + n;
+
+		/*
+		 * Compute q00, q01 and q1, and check that they are in the
+		 * expected range.
+		 *
+		 * F and G use the first 2*n bytes = hn words.
+		 */
+		if (!make_q001(logn, lim00, lim01, lim11,
+			f, g, tF, tG, (uint32_t *)(tG + n)))
+		{
+#if NTRUGEN_STATS
+			stats_solve_err_limit ++;
+#endif
+			continue;
+		}
+
+		int16_t *tq00 = (int16_t *)(tG + n);
+		int16_t *tq01 = tq00 + n;
+		int32_t *tq11 = (int32_t *)(tq01 + n);
+		uint8_t *tseed = (uint8_t *)(tq11 + n);
+		memmove(tseed, seed_buf, seed_len);
+
+		/*
+		 * Return the computed F, G, q00, q01, q11 and seed.
+		 */
+		if (F != NULL) {
+			memmove(F, tF, n);
+		}
+		if (G != NULL) {
+			memmove(G, tG, n);
+		}
+		if (q00 != NULL) {
+			memmove(q00, tq00, n * sizeof *tq00);
+		}
+		if (q01 != NULL) {
+			memmove(q01, tq01, n * sizeof *tq01);
+		}
+		if (q11 != NULL) {
+			memmove(q11, tq11, n * sizeof *tq11);
+		}
+		if (seed != NULL) {
+			memmove(seed, tseed, seed_len);
+		}
+		if (tt32 != tmp) {
+			memmove(tmp, tt32, 10 * n + seed_len);
+		}
+
+		return 0;
+	}
+}
+
+/* see ntrugen.h */
+int
+Hawk_recover_G(unsigned logn,
+	int8_t *restrict G,
+	const int8_t *restrict f, const int8_t *restrict g,
+	const int8_t *restrict F, void *restrict tmp, size_t tmp_len)
+{
+	const ntru_profile *prof;
+	switch (logn) {
+	case 8:   prof = &SOLVE_Hawk_256;   break;
+	case 9:   prof = &SOLVE_Hawk_512;   break;
+	case 10:  prof = &SOLVE_Hawk_1024;  break;
+	default:
+		return -1;
+	}
+
+	/*
+	 * Ensure that the tmp[] buffer has proper alignment for 64-bit
+	 * access.
+	 */
+	if (tmp_len < 7) {
+		return -1;
+	}
+	uintptr_t utmp1 = (uintptr_t)tmp;
+	uintptr_t utmp2 = (utmp1 + 7) & ~(uintptr_t)7;
+	tmp_len -= (size_t)(utmp2 - utmp1);
+	uint32_t *tt32 = (void *)utmp2;
+	if (tmp_len < ((size_t)12 << logn)) {
+		return -1;
+	}
+
+	int r = recover_G(logn,
+		(int32_t)prof->q, prof->coeff_FG_limit[logn],
+		f, g, F, tt32);
+	size_t n = (size_t)1 << logn;
+	if (G != NULL) {
+		memmove(G, tt32, n);
+	}
+	if ((void *)tt32 != tmp) {
+		memmove(tmp, tt32, n);
+	}
+
+	return r - 1;
+}
+
+/* see ntrugen.h */
+int
+Hawk_recover_qq(unsigned logn,
+	int16_t *restrict q00, int16_t *restrict q01, int32_t *restrict q11,
+	const int8_t *restrict f, const int8_t *restrict g,
+	const int8_t *restrict F, const int8_t *restrict G,
+	void *restrict tmp, size_t tmp_len)
+{
+	int lim00, lim01;
+	int32_t lim11;
+	switch (logn) {
+	case 8:
+	case 9:
+	case 10:
+		lim00 = 1 << bits_lim00[logn];
+		lim01 = 1 << bits_lim01[logn];
+		lim11 = (int32_t)1 << bits_lim11[logn];
+		break;
+	default:
+		return -1;
+	}
+
+	/*
+	 * Ensure that the tmp[] buffer has proper alignment for 64-bit
+	 * access.
+	 */
+	if (tmp_len < 7) {
+		return -1;
+	}
+	uintptr_t utmp1 = (uintptr_t)tmp;
+	uintptr_t utmp2 = (utmp1 + 7) & ~(uintptr_t)7;
+	tmp_len -= (size_t)(utmp2 - utmp1);
+	uint32_t *tt32 = (void *)utmp2;
+	if (tmp_len < ((size_t)20 << logn)) {
+		return -1;
+	}
+
+	if (!make_q001(logn, lim00, lim01, lim11, f, g, F, G, tt32)) {
+		return -1;
+	}
+	size_t n = (size_t)1 << logn;
+	int16_t *tq00 = (int16_t *)tt32;
+	int16_t *tq01 = tq00 + n;
+	int32_t *tq11 = (int32_t *)(tq01 + n);
+	if (q00 != NULL) {
+		memmove(q00, tq00, n * sizeof *tq00);
+	}
+	if (q01 != NULL) {
+		memmove(q01, tq01, n * sizeof *tq01);
+	}
+	if (q11 != NULL) {
+		memmove(q11, tq11, n * sizeof *tq11);
+	}
+	if ((void *)tt32 != tmp) {
+		memmove(tmp, tt32, n * 8);
+	}
+	return 0;
+}
diff --git a/lib/dns/hawk/ng_inner.h b/lib/dns/hawk/ng_inner.h
new file mode 100644
index 0000000000..fc1e4c30d3
--- /dev/null
+++ b/lib/dns/hawk/ng_inner.h
@@ -0,0 +1,1796 @@
+#ifndef NG_INNER_H__
+#define NG_INNER_H__
+
+/* ==================================================================== */
+
+#include <stddef.h>
+#include <stdint.h>
+#include <string.h>
+
+#include "ng_config.h"
+
+#ifndef NTRUGEN_PREFIX
+#define NTRUGEN_PREFIX   ntrugen
+#endif
+#define Zn(name)             Zn_(NTRUGEN_PREFIX, name)
+#define Zn_(prefix, name)    Zn__(prefix, name)
+#define Zn__(prefix, name)   prefix ## _ ## name
+
+#include "sha3.h"
+
+/* ==================================================================== */
+
+#ifndef NTRUGEN_AVX2
+/*
+ * Auto-detection of AVX2 support, if not overridden by configuration.
+ */
+#if defined __AVX2__ && __AVX2__
+#define NTRUGEN_AVX2   1
+#else
+#define NTRUGEN_AVX2   0
+#endif
+#endif // NTRUGEN_AVX2
+
+#if NTRUGEN_AVX2
+/*
+ * This implementation uses AVX2 intrinsics.
+ */
+#include <immintrin.h>
+#if defined __GNUC__ || defined __clang__
+#include <x86intrin.h>
+#endif
+#ifndef NTRUGEN_LE
+#define NTRUGEN_LE   1
+#endif
+#ifndef NTRUGEN_UNALIGNED
+#define NTRUGEN_UNALIGNED   1
+#endif
+#if defined __GNUC__
+#define TARGET_AVX2    __attribute__((target("avx2,lzcnt,pclmul")))
+#define ALIGNED_AVX2   __attribute__((aligned(32)))
+#elif defined _MSC_VER && _MSC_VER
+#pragma warning( disable : 4752 )
+#endif
+#endif // NTRUGEN_AVX2
+
+#ifndef TARGET_AVX2
+#define TARGET_AVX2
+#endif
+#ifndef ALIGNED_AVX2
+#define ALIGNED_AVX2
+#endif
+
+/*
+ * Auto-detect ARM Cortex-M4 platforms.
+ */
+#ifndef NTRUGEN_ASM_CORTEXM4
+#if (defined __ARM_ARCH_7EM__ && __ARM_ARCH_7EM__) \
+	&& (defined __ARM_FEATURE_DSP && __ARM_FEATURE_DSP)
+#define NTRUGEN_ASM_CORTEXM4   1
+#else
+#define NTRUGEN_ASM_CORTEXM4   0
+#endif
+#endif
+
+/*
+ * Disable warning on applying unary minus on an unsigned type.
+ */
+#if defined _MSC_VER && _MSC_VER
+#pragma warning( disable : 4146 )
+#pragma warning( disable : 4244 )
+#pragma warning( disable : 4267 )
+#pragma warning( disable : 4334 )
+#endif
+
+/*
+ * Auto-detect 64-bit architectures.
+ */
+#ifndef NTRUGEN_64
+#if defined __x86_64__ || defined _M_X64 \
+        || defined __ia64 || defined __itanium__ || defined _M_IA64 \
+        || defined __powerpc64__ || defined __ppc64__ || defined __PPC64__ \
+        || defined __64BIT__ || defined _LP64 || defined __LP64__ \
+        || defined __sparc64__ \
+        || defined __aarch64__ || defined _M_ARM64 \
+        || defined __mips64
+#define NTRUGEN_64   1
+#else
+#define NTRUGEN_64   0
+#endif
+#endif
+
+/*
+ * Auto-detect endianness and support of unaligned accesses.
+ */
+#if defined __i386__ || defined _M_IX86 \
+        || defined __x86_64__ || defined _M_X64 \
+	|| defined __aarch64__ || defined _M_ARM64 || defined _M_ARM64EC \
+        || (defined _ARCH_PWR8 \
+                && (defined __LITTLE_ENDIAN || defined __LITTLE_ENDIAN__))
+
+#ifndef NTRUGEN_LE
+#define NTRUGEN_LE   1
+#endif
+#ifndef NTRUGEN_UNALIGNED
+#define NTRUGEN_UNALIGNED   1
+#endif
+
+#elif (defined __LITTLE_ENDIAN || defined __LITTLE_ENDIAN__) \
+        || (defined __BYTE_ORDER__ && defined __ORDER_LITTLE_ENDIAN__ \
+                && __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__)
+
+#ifndef NTRUGEN_LE
+#define NTRUGEN_LE   1
+#endif
+#ifndef NTRUGEN_UNALIGNED
+#define NTRUGEN_UNALIGNED   0
+#endif
+
+#else
+
+#ifndef NTRUGEN_LE
+#define NTRUGEN_LE   0
+#endif
+#ifndef NTRUGEN_UNALIGNED
+#define NTRUGEN_UNALIGNED   0
+#endif
+
+#endif
+
+/*
+ * For seed generation:
+ *
+ *  - On Linux (glibc-2.25+), FreeBSD 12+ and OpenBSD, use getentropy().
+ *  - On other Unix-like systems, use /dev/urandom (also a fallback for
+ *    failed getentropy() calls).
+ *  - On Windows, use CryptGenRandom().
+ */
+
+#ifndef NTRUGEN_RAND_GETENTROPY
+#if (defined __linux && defined __GLIBC__ \
+        && (__GLIBC__ > 2 || (__GLIBC__ == 2 && __GLIBC_MINOR__ >= 25))) \
+        || (defined __FreeBSD__ && __FreeBSD__ >= 12) \
+        || defined __OpenBSD__
+#define NTRUGEN_RAND_GETENTROPY   1
+#else
+#define NTRUGEN_RAND_GETENTROPY   0
+#endif
+#endif
+#ifndef NTRUGEN_RAND_URANDOM
+#if defined _AIX \
+        || defined __ANDROID__ \
+        || defined __FreeBSD__ \
+        || defined __NetBSD__ \
+        || defined __OpenBSD__ \
+        || defined __DragonFly__ \
+        || defined __linux__ \
+        || (defined __sun && (defined __SVR4 || defined __svr4__)) \
+        || (defined __APPLE__ && defined __MACH__)
+#define NTRUGEN_RAND_URANDOM   1
+#else
+#define NTRUGEN_RAND_URANDOM   0
+#endif
+#endif
+
+#ifndef NTRUGEN_RAND_WIN32
+#if defined _WIN32 || defined _WIN64
+#define NTRUGEN_RAND_WIN32   1
+#else
+#define NTRUGEN_RAND_WIN32   0
+#endif
+#endif
+
+/*
+ * MSVC 2015 does not known the C99 keyword 'restrict'.
+ */
+#if defined _MSC_VER && _MSC_VER
+#ifndef restrict
+#define restrict   __restrict
+#endif
+#endif
+
+/*
+ * Enable stats if not defined explicitly (stats should be disabled by
+ * default, since they are not thread-safe).
+ */
+#ifndef NTRUGEN_STATS
+#define NTRUGEN_STATS   0
+#endif
+
+/* ==================================================================== */
+/*
+ * The ntrugen public API is redeclared here, but with the proper name
+ * prefixes and macros.
+ */
+
+typedef void (*ntrugen_rng)(void *ctx, void *dst, size_t len);
+
+#define Hawk_keygen   Zn(Hawk_keygen)
+int Hawk_keygen(unsigned logn,
+	int8_t *f, int8_t *g, int8_t *F, int8_t *G,
+	int16_t *q00, int16_t *q01, int32_t *q11,
+	void *seed, ntrugen_rng rng, void *rng_context,
+	void *tmp, size_t tmp_len);
+
+#define Hawk_regen_fg   Zn(Hawk_regen_fg)
+void Hawk_regen_fg(unsigned logn,
+	int8_t *f, int8_t *g, const void *seed);
+
+#define Hawk_recover_G   Zn(Hawk_recover_G)
+int Hawk_recover_G(unsigned logn,
+	int8_t *G,
+	const int8_t *f, const int8_t *g, const int8_t *F,
+	void *tmp, size_t tmp_len);
+
+#define Hawk_recover_qq   Zn(Hawk_recover_qq)
+int Hawk_recover_qq(unsigned logn,
+	int16_t *q00, int16_t *q01, int32_t *q11,
+	const int8_t *f, const int8_t *g, const int8_t *F, const int8_t *G,
+	void *tmp, size_t tmp_len);
+
+/* ==================================================================== */
+
+static inline unsigned
+dec16le(const void *src)
+{
+#if NTRUGEN_LE && NTRUGEN_UNALIGNED
+	return *(const uint16_t *)src;
+#else
+	const uint8_t *buf = src;
+	return (unsigned)buf[0]
+		| ((unsigned)buf[1] << 8);
+#endif
+}
+
+static inline void
+enc16le(void *dst, unsigned x)
+{
+#if NTRUGEN_LE && NTRUGEN_UNALIGNED
+	*(uint16_t *)dst = x;
+#else
+	uint8_t *buf = dst;
+	buf[0] = (uint8_t)x;
+	buf[1] = (uint8_t)(x >> 8);
+#endif
+}
+
+static inline uint32_t
+dec32le(const void *src)
+{
+#if NTRUGEN_LE && NTRUGEN_UNALIGNED
+	return *(const uint32_t *)src;
+#else
+	const uint8_t *buf = src;
+	return (uint32_t)buf[0]
+		| ((uint32_t)buf[1] << 8)
+		| ((uint32_t)buf[2] << 16)
+		| ((uint32_t)buf[3] << 24);
+#endif
+}
+
+static inline void
+enc32le(void *dst, uint32_t x)
+{
+#if NTRUGEN_LE && NTRUGEN_UNALIGNED
+	*(uint32_t *)dst = x;
+#else
+	uint8_t *buf = dst;
+	buf[0] = (uint8_t)x;
+	buf[1] = (uint8_t)(x >> 8);
+	buf[2] = (uint8_t)(x >> 16);
+	buf[3] = (uint8_t)(x >> 24);
+#endif
+}
+
+static inline uint64_t
+dec64le(const void *src)
+{
+#if NTRUGEN_LE && NTRUGEN_UNALIGNED
+	return *(const uint64_t *)src;
+#else
+	const uint8_t *buf = src;
+	return (uint64_t)buf[0]
+		| ((uint64_t)buf[1] << 8)
+		| ((uint64_t)buf[2] << 16)
+		| ((uint64_t)buf[3] << 24)
+		| ((uint64_t)buf[4] << 32)
+		| ((uint64_t)buf[5] << 40)
+		| ((uint64_t)buf[6] << 48)
+		| ((uint64_t)buf[7] << 56);
+#endif
+}
+
+static inline void
+enc64le(void *dst, uint64_t x)
+{
+#if NTRUGEN_LE && NTRUGEN_UNALIGNED
+	*(uint64_t *)dst = x;
+#else
+	uint8_t *buf = dst;
+	buf[0] = (uint8_t)x;
+	buf[1] = (uint8_t)(x >> 8);
+	buf[2] = (uint8_t)(x >> 16);
+	buf[3] = (uint8_t)(x >> 24);
+	buf[4] = (uint8_t)(x >> 32);
+	buf[5] = (uint8_t)(x >> 40);
+	buf[6] = (uint8_t)(x >> 48);
+	buf[7] = (uint8_t)(x >> 56);
+#endif
+}
+
+/* ==================================================================== */
+/*
+ * Modular arithmetics.
+ *
+ * We implement computations modulo some small integer p with the following
+ * characteristics:
+ *
+ *   (4/3)*2^30 < p < 2^31       (this implies that 2*p < 2^32 < 3*p)
+ *   p-1 is a multiple of 2048
+ *
+ * Operands are held in 32-bit values (uint32_t). We define R = 2^32 mod p.
+ *
+ * Values modulo p are 32-bit integers (uint32_t type) in the 0 to p-1 range.
+ * Montgomery representation of an element x of Z_p is the value x*R mod p
+ * (also in the 0 to p-1 range). Montgomery multiplication of x and y
+ * computes x*y/R mod p (thus, Montgomery multiplication of the Montgomery
+ * representations of x and y outputs the Montgomery representation of the
+ * product x*y, since (x*R)*(y*R)/R = (x*y)*R). In general, values are not
+ * kept in Montgomery representation, unless explicitly specified.
+ *
+ * The "signed normalized" value of x modulo p is the unique integer v
+ * in the -(p-1)/2 to +(p-1)/2 range such that x = v mod p.
+ *
+ * The PRIMES[] array contains the largest of such primes, in
+ * descending order. Six values are provided for each prime p:
+ *    p     modulus
+ *    p0i   -1/p mod 2^32
+ *    R2    2^64 mod p
+ *    g     a primitive 2048-th root of 1 modulo p (i.e. g^1024 = -1 mod p)
+ *    ig    1/g mod p
+ *    s     inverse mod p of the product of the previous primes
+ * Values g, ig and s are in Montgomery representation.
+ * R2 is used to convert values to Montgomery representations
+ * (with montymul(x, R2) = x*R2/R = x*R mod p). g and ig are used to
+ * generate the tables used for NTT and inverse NTT. Value s supports
+ * reconstruction of a big integer in RNS representation with the CRT.
+ * The product of all the primes in the PRIMES[] array is about 2^10012.25
+ * and thus appropriate for big integers (in RNS) of up to 10000 bits.
+ *
+ * Polynomials over Z_p are considered modulo X^n+1 for n a power of two
+ * between 2 and 1024 (inclusive). The degree n is provided as parameter
+ * 'logn' in the 1 to 10 range (with n = 2^logn). The polynomial
+ * coefficients are consecutive in RAM, in ascending degree order. The
+ * NTT representation of such a polynomial is the evaluation of
+ * the polynomial over the roots of X^n+1 (which are (g^(1024/n))^(2*i+1)
+ * for integers i = 0 to n-1).
+ *
+ * Non-prime modulus
+ * -----------------
+ *
+ * This code also works for some non-prime moduli p. If p is a product
+ * p = p_1*p_2*...*p_k, such that each p_i is an odd prime, and p is in
+ * the allowed range ((4/3)*2^30 to 2^31), then all operations work and
+ * really compute things modulo each p_i simultaneously (through the
+ * CRT). In particular, we can compute modulo q = 12289 by using (for
+ * instance) p = 2013442049 = 163841*q (value 163841 is itself prime,
+ * and 163840 is a multiple of 2048, which is not actually needed if we
+ * are only interested in correct computations modulo q).
+ */
+
+/*
+ * Expand the top bit of value x into a full 32-bit mask (i.e. return
+ * 0xFFFFFFFF if x >= 0x80000000, or 0x00000000 otherwise).
+ */
+static inline uint32_t
+tbmask(uint32_t x)
+{
+	return (uint32_t)(*(int32_t *)&x >> 31);
+}
+
+/*
+ * Get v mod p in the 0 to p-1 range; input v must be in the -(p-1) to +(p-1)
+ * range.
+ */
+static inline uint32_t
+mp_set(int32_t v, uint32_t p)
+{
+	uint32_t w = (uint32_t)v;
+	return w + (p & tbmask(w));
+}
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static inline __m256i
+mp_set_x8(__m256i yv, __m256i yp)
+{
+	return _mm256_add_epi32(yv, _mm256_and_si256(yp,
+		_mm256_srai_epi32(yv, 31)));
+}
+#endif // NTRUGEN_AVX2
+
+/*
+ * Get the signed normalized value of x mod p.
+ */
+static inline int32_t
+mp_norm(uint32_t x, uint32_t p)
+{
+	uint32_t w = x - (p & tbmask((p >> 1) - x));
+	return *(int32_t *)&w;
+}
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static inline __m256i
+mp_norm_x8(__m256i yv, __m256i yp, __m256i yhp)
+{
+	return _mm256_sub_epi32(yv, _mm256_and_si256(yp,
+		_mm256_cmpgt_epi32(yv, yhp)));
+}
+#endif // NTRUGEN_AVX2
+
+#if 0 /* unused */
+/*
+ * Compute p0i = -1/p mod 2^32.
+ */
+static inline uint32_t
+mp_ninv32(uint32_t p)
+{
+	uint32_t y = 2 - p;
+	y *= 2 - p * y;
+	y *= 2 - p * y;
+	y *= 2 - p * y;
+	y *= 2 - p * y;
+	return -y;
+}
+#endif
+
+/*
+ * Compute R = 2^32 mod p.
+ */
+static inline uint32_t
+mp_R(uint32_t p)
+{
+	/*
+	 * Since 2*p < 2^32 < 3*p, we just subtract 2*p from 2^32.
+	 */
+	return -(p << 1);
+}
+
+/*
+ * Compute R/2 = 2^31 mod p.
+ */
+static inline uint32_t
+mp_hR(uint32_t p)
+{
+	/*
+	 * Since p < 2^31 < (3/2)*p, we just subtract p from 2^31.
+	 */
+	return ((uint32_t)1 << 31) - p;
+}
+
+/*
+ * Addition modulo p.
+ */
+static inline uint32_t
+mp_add(uint32_t a, uint32_t b, uint32_t p)
+{
+	uint32_t d = a + b - p;
+	return d + (p & tbmask(d));
+}
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static inline __m256i
+mp_add_x8(__m256i ya, __m256i yb, __m256i yp)
+{
+	__m256i yd = _mm256_sub_epi32(_mm256_add_epi32(ya, yb), yp);
+	return _mm256_add_epi32(yd, _mm256_and_si256(yp,
+		_mm256_srai_epi32(yd, 31)));
+}
+#endif // NTRUGEN_AVX2
+
+/*
+ * Subtraction modulo p.
+ */
+static inline uint32_t
+mp_sub(uint32_t a, uint32_t b, uint32_t p)
+{
+	uint32_t d = a - b;
+	return d + (p & tbmask(d));
+}
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static inline __m256i
+mp_sub_x8(__m256i ya, __m256i yb, __m256i yp)
+{
+	__m256i yd = _mm256_sub_epi32(ya, yb);
+	return _mm256_add_epi32(yd, _mm256_and_si256(yp,
+		_mm256_srai_epi32(yd, 31)));
+}
+#endif // NTRUGEN_AVX2
+
+/*
+ * Halving modulo p.
+ */
+static inline uint32_t
+mp_half(uint32_t a, uint32_t p)
+{
+	return (a + (p & -(a & 1))) >> 1;
+}
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static inline __m256i
+mp_half_x8(__m256i ya, __m256i yp)
+{
+	return _mm256_srli_epi32(
+		_mm256_add_epi32(ya, _mm256_and_si256(yp,
+			_mm256_sub_epi32(_mm256_setzero_si256(),
+			_mm256_and_si256(ya, _mm256_set1_epi32(1))))), 1);
+}
+#endif // NTRUGEN_AVX2
+
+/*
+ * Montgomery multiplication modulo p.
+ *
+ * Reduction computes (a*b + w*p)/(2^32) for some w <= 2^(32-1);
+ * then p is conditionally subtracted. This process works as long as:
+ *    (a*b + p*(2^32-1))/(2^32) <= 2*p-1
+ * which holds if:
+ *    a*b <= p*2^32 - 2^32 + p
+ * This works if both a and b are proper integers modulo p (in the 0 to p-1
+ * range), but also if, for instance, a is an integer modulo p, and b is an
+ * arbitrary 32-bit integer.
+ */
+static inline uint32_t
+mp_montymul(uint32_t a, uint32_t b, uint32_t p, uint32_t p0i)
+{
+	uint64_t z = (uint64_t)a * (uint64_t)b;
+	uint32_t w = (uint32_t)z * p0i;
+	uint32_t d = (uint32_t)((z + (uint64_t)w * (uint64_t)p) >> 32) - p;
+	return d + (p & tbmask(d));
+}
+
+#if NTRUGEN_AVX2
+/*
+ * Input:
+ *    ya = a0 : XX : a1 : XX : a2 : XX : a3 : XX
+ *    yb = b0 : XX : b1 : XX : b2 : XX : b3 : XX
+ * Output:
+ *    mm(a0,b0) : 00 : mm(a1,b1) : 00 : mm(a2,b2) : 00 : mm(a3,b3) : 00
+ */
+TARGET_AVX2
+static inline __m256i
+mp_montymul_x4(__m256i ya, __m256i yb, __m256i yp, __m256i yp0i)
+{
+	__m256i yd = _mm256_mul_epu32(ya, yb);
+	__m256i ye = _mm256_mul_epu32(yd, yp0i);
+	ye = _mm256_mul_epu32(ye, yp);
+	yd = _mm256_srli_epi64(_mm256_add_epi64(yd, ye), 32);
+	yd = _mm256_sub_epi32(yd, yp);
+	return _mm256_add_epi32(yd, _mm256_and_si256(yp,
+		_mm256_srai_epi32(yd, 31)));
+}
+
+TARGET_AVX2
+static inline __m256i
+mp_montymul_x8(__m256i ya, __m256i yb, __m256i yp, __m256i yp0i)
+{
+	/* yd0 <- a0*b0 : a2*b2 (+high lane) */
+	__m256i yd0 = _mm256_mul_epu32(ya, yb);
+	/* yd1 <- a1*b1 : a3*b3 (+high lane) */
+	__m256i yd1 = _mm256_mul_epu32(
+		_mm256_srli_epi64(ya, 32),
+		_mm256_srli_epi64(yb, 32));
+
+	__m256i ye0 = _mm256_mul_epu32(yd0, yp0i);
+	__m256i ye1 = _mm256_mul_epu32(yd1, yp0i);
+	ye0 = _mm256_mul_epu32(ye0, yp);
+	ye1 = _mm256_mul_epu32(ye1, yp);
+	yd0 = _mm256_add_epi64(yd0, ye0);
+	yd1 = _mm256_add_epi64(yd1, ye1);
+
+	/* yf0 <- lo(d0) : lo(d1) : hi(d0) : hi(d1) (+high lane) */
+	__m256i yf0 = _mm256_unpacklo_epi32(yd0, yd1);
+	/* yf1 <- lo(d2) : lo(d3) : hi(d2) : hi(d3) (+high lane) */
+	__m256i yf1 = _mm256_unpackhi_epi32(yd0, yd1);
+	/* yg <- hi(d0) : hi(d1) : hi(d2) : hi(d3) (+high lane) */
+	__m256i yg = _mm256_unpackhi_epi64(yf0, yf1);
+	/*
+	 * Alternate version (instead of the three unpack above) but it
+	 * seems to be slightly slower.
+	__m256i yg = _mm256_blend_epi32(_mm256_srli_epi64(yd0, 32), yd1, 0xAA);
+	 */
+
+	yg = _mm256_sub_epi32(yg, yp);
+	return _mm256_add_epi32(yg, _mm256_and_si256(yp,
+		_mm256_srai_epi32(yg, 31)));
+}
+#endif // NTRUGEN_AVX2
+
+/*
+ * Compute 2^(31*e) mod p.
+ */
+static inline uint32_t
+mp_Rx31(unsigned e, uint32_t p, uint32_t p0i, uint32_t R2)
+{
+	/* x <- 2^63 mod p = Montgomery representation of 2^31 */
+	uint32_t x = mp_half(R2, p);
+	uint32_t d = 1;
+	for (;;) {
+		if ((e & 1) != 0) {
+			d = mp_montymul(d, x, p, p0i);
+		}
+		e >>= 1;
+		if (e == 0) {
+			return d;
+		}
+		x = mp_montymul(x, x, p, p0i);
+	}
+}
+
+/*
+ * Division modulo p (x = dividend, y = divisor).
+ * This code uses a constant-time binary GCD, which also works for a
+ * non-prime modulus p (contrary to Fermat's Little Theorem). If the
+ * divisor is not invertible modulo p, then 0 is returned.
+ */
+#define mp_div   Zn(mp_div)
+uint32_t mp_div(uint32_t x, uint32_t y, uint32_t p);
+
+/*
+ * Compute the roots for NTT; given g (primitive 2048-th root of 1 modulo p),
+ * this fills gm[] and igm[] with powers of g and 1/g:
+ *    gm[rev(i)] = g^i mod p              (in Montgomery representation)
+ *    igm[rev(i)] = (1/2)*(1/g)^i mod p   (in Montgomery representation)
+ * rev() is the bit-reversal function over 10 bits. The arrays gm[] and igm[]
+ * are filled only up to n = 2^logn values. Roots g and ig must be provided
+ * in Montgomery representation.
+ */
+#define mp_mkgmigm   Zn(mp_mkgmigm)
+void mp_mkgmigm(unsigned logn, uint32_t *restrict gm, uint32_t *restrict igm,
+	uint32_t g, uint32_t ig, uint32_t p, uint32_t p0i);
+
+/*
+ * Like mp_mkgmigm(), but computing only gm[].
+ */
+#define mp_mkgm   Zn(mp_mkgm)
+void mp_mkgm(unsigned logn, uint32_t *restrict gm,
+	uint32_t g, uint32_t p, uint32_t p0i);
+
+/*
+ * A variant of mp_mkgm(), specialized for logn = 7, and g being a
+ * 256-th root of 1, not a 2048-th root of 1.
+ */
+#define mp_mkgm7   Zn(mp_mkgm7)
+void mp_mkgm7(uint32_t *restrict gm, uint32_t g, uint32_t p, uint32_t p0i);
+
+/*
+ * Like mp_mkgmigm(), but computing only igm[].
+ */
+#define mp_mkigm   Zn(mp_mkigm)
+void mp_mkigm(unsigned logn, uint32_t *restrict igm,
+	uint32_t ig, uint32_t p, uint32_t p0i);
+
+/*
+ * Compute the NTT over a polynomial. The polynomial a[] is modified in-place.
+ */
+#define mp_NTT   Zn(mp_NTT)
+void mp_NTT(unsigned logn, uint32_t *restrict a, const uint32_t *restrict gm,
+	uint32_t p, uint32_t p0i);
+
+/*
+ * Compute the inverse NTT over a polynomial. The polynomial a[] is modified
+ * in-place.
+ */
+#define mp_iNTT   Zn(mp_iNTT)
+void mp_iNTT(unsigned logn, uint32_t *restrict a, const uint32_t *restrict igm,
+	uint32_t p, uint32_t p0i);
+
+/*
+ * Precomputed small primes. Enough values are provided to allow
+ * computations in RNS representation over big integers up to 10000 bits.
+ */
+typedef struct {
+	uint32_t p;
+	uint32_t p0i;
+	uint32_t R2;
+	uint32_t g;
+	uint32_t ig;
+	uint32_t s;
+} small_prime;
+#define PRIMES   Zn(PRIMES)
+extern const small_prime PRIMES[];
+
+/* ==================================================================== */
+/*
+ * Custom bignum implementation.
+ *
+ * Big integers are represented as sequences of 32-bit integers; the
+ * integer values are not necessarily consecutive in RAM (a dynamically
+ * provided "stride" value is added to the current word pointer, to get
+ * to the next word). The "len" parameter qualifies the number of words.
+ *
+ * Normal representation uses 31-bit limbs; each limb is stored in a
+ * 32-bit word, with the top bit (31) always cleared. Limbs are in
+ * low-to-high order. Signed integers use two's complement (hence, bit 30
+ * of the last limb is the sign bit).
+ *
+ * RNS representation of a big integer x is the sequence of values
+ * x modulo p, for the primes p defined in the PRIMES[] array.
+ */
+
+/*
+ * Mutiply the provided big integer m with a small value x. The big
+ * integer must have stride 1.
+ * This function assumes that x < 2^31. The carry word is returned.
+ */
+#define zint_mul_small   Zn(zint_mul_small)
+uint32_t zint_mul_small(uint32_t *m, size_t len, uint32_t x);
+
+/*
+ * Reduce a big integer d modulo a small integer p.
+ * Rules:
+ *  d is unsigned
+ *  p is prime
+ *  2^30 < p < 2^31
+ *  p0i = -(1/p) mod 2^31
+ *  R2 = 2^64 mod p
+ */
+#define zint_mod_small_unsigned   Zn(zint_mod_small_unsigned)
+uint32_t zint_mod_small_unsigned(const uint32_t *d, size_t len, size_t stride,
+	uint32_t p, uint32_t p0i, uint32_t R2);
+
+#if NTRUGEN_AVX2
+#define zint_mod_small_unsigned_x8 Zn(zint_mod_small_unsigned_x8)
+TARGET_AVX2
+__m256i zint_mod_small_unsigned_x8(
+	const uint32_t *d, size_t len, size_t stride,
+	__m256i yp, __m256i yp0i, __m256i yR2);
+#endif // NTRUGEN_AVX2
+
+/*
+ * Similar to zint_mod_small_unsigned(), except that d may be signed.
+ * Extra parameter is Rx = 2^(31*len) mod p.
+ */
+static inline uint32_t
+zint_mod_small_signed(const uint32_t *d, size_t len, size_t stride,
+	uint32_t p, uint32_t p0i, uint32_t R2, uint32_t Rx)
+{
+	if (len == 0) {
+		return 0;
+	}
+	uint32_t z = zint_mod_small_unsigned(d, len, stride, p, p0i, R2);
+	z = mp_sub(z, Rx & -(d[(len - 1) * stride] >> 30), p);
+	return z;
+}
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static inline __m256i
+zint_mod_small_signed_x8(const uint32_t *d, size_t len, size_t stride,
+	__m256i yp, __m256i yp0i, __m256i yR2, __m256i yRx)
+{
+	if (len == 0) {
+		return _mm256_setzero_si256();
+	}
+	__m256i yz = zint_mod_small_unsigned_x8(d, len, stride, yp, yp0i, yR2);
+	__m256i yl = _mm256_loadu_si256((__m256i *)(d + (len - 1) * stride));
+	__m256i ym = _mm256_sub_epi32(_mm256_setzero_si256(),
+		_mm256_srli_epi32(yl, 30));
+	yz = mp_sub_x8(yz, _mm256_and_si256(yRx, ym), yp);
+	return yz;
+}
+#endif // NTRUGEN_AVX2
+
+/*
+ * Add s*a to d. d and a initially have length 'len' words; the new d
+ * has length 'len+1' words. 's' must fit on 31 bits. d[] and a[] must
+ * not overlap. d uses stride dstride, while a has stride 1.
+ */
+#define zint_add_mul_small   Zn(zint_add_mul_small)
+void zint_add_mul_small(uint32_t *restrict d, size_t len, size_t dstride,
+	const uint32_t *restrict a, uint32_t s);
+
+#if NTRUGEN_AVX2
+/*
+ * Like zint_add_mul_small(), except that it handles eight integers in
+ * parallel:
+ *    d0 <- d0 + s0*a
+ *    d1 <- d1 + s1*a
+ *     ...
+ *    d7 <- d7 + s7*a
+ */
+#define zint_add_mul_small_x8   Zn(zint_add_mul_small_x8)
+TARGET_AVX2
+void zint_add_mul_small_x8(uint32_t *restrict d, size_t len, size_t dstride,
+	const uint32_t *restrict a, __m256i ys);
+#endif // NTRUGEN_AVX2
+
+/*
+ * Normalize a modular integer around 0: if x > p/2, then x is replaced
+ * with x - p (signed encoding with two's complement); otherwise, x is
+ * untouched. The two integers x and p are encoded over the same length;
+ * x has stride xstride, while p has stride 1.
+ */
+#define zint_norm_zero   Zn(zint_norm_zero)
+void zint_norm_zero(uint32_t *restrict x, size_t len, size_t xstride,
+	const uint32_t *restrict p);
+
+/*
+ * Rebuild integers from their RNS representation. There are 'num_sets' sets
+ * of 'n' integers. Within each set, the n integers are interleaved,
+ * so that words of a given integer occur every n slots in RAM (i.e. each
+ * integer has stride 'n'). The sets are consecutive in RAM.
+ *
+ * If "normalize_signed" is non-zero, then the output values are
+ * normalized to the -m/2..m/2 interval (where m is the product of all
+ * small prime moduli); two's complement is used for negative values.
+ * If "normalize_signed" is zero, then the output values are all
+ * in the 0..m-1 range.
+ *
+ * tmp[] must have room for xlen words.
+ */
+#define zint_rebuild_CRT   Zn(rebuild_CRT)
+void zint_rebuild_CRT(uint32_t *restrict xx, size_t xlen, size_t n,
+	size_t num_sets, int normalize_signed, uint32_t *restrict tmp);
+
+/*
+ * Negate a big integer conditionally: value a is replaced with -a if
+ * and only if ctl = 1. Control value ctl must be 0 or 1. The integer
+ * has stride 1.
+ */
+#define zint_negate   Zn(zint_negate)
+void zint_negate(uint32_t *a, size_t len, uint32_t ctl);
+
+/*
+ * Get the number of leading zeros in a 32-bit value.
+ */
+TARGET_AVX2
+static inline unsigned
+lzcnt(uint32_t x)
+{
+#if NTRUGEN_AVX2
+	/*
+	 * All AVX2-capable CPUs have lzcnt.
+	 */
+	return _lzcnt_u32(x);
+#else // NTRUGEN_AVX2
+	uint32_t m = tbmask((x >> 16) - 1);
+	uint32_t s = m & 16;
+	x = (x >> 16) ^ (m & (x ^ (x >> 16)));
+	m = tbmask((x >>  8) - 1);
+	s |= m &  8;
+	x = (x >>  8) ^ (m & (x ^ (x >>  8)));
+	m = tbmask((x >>  4) - 1);
+	s |= m &  4;
+	x = (x >>  4) ^ (m & (x ^ (x >>  4)));
+	m = tbmask((x >>  2) - 1);
+	s |= m &  2;
+	x = (x >>  2) ^ (m & (x ^ (x >>  2)));
+
+	/*
+	 * At this point, x fits on 2 bits. Number of leading zeros is
+	 * then:
+	 *    x = 0   -> 2
+	 *    x = 1   -> 1
+	 *    x = 2   -> 0
+	 *    x = 3   -> 0
+	 */
+	return (unsigned)(s + ((2 - x) & tbmask(x - 3)));
+#endif // NTRUGEN_AVX2
+}
+
+/*
+ * Identical to lzcnt(), except that the caller makes sure that the
+ * operand is non-zero. On (old-ish) x86 systems, this function could be
+ * specialized with the bsr opcode (which does not support a zero input).
+ */
+#define lzcnt_nonzero   lzcnt
+
+/*
+ * Compute a GCD between two positive big integers x and y. The two
+ * integers must be odd. Returned value is 1 if the GCD is 1, 0
+ * otherwise. When 1 is returned, arrays u and v are filled with values
+ * such that:
+ *   0 <= u <= y
+ *   0 <= v <= x
+ *   x*u - y*v = 1
+ * x[] and y[] are unmodified. Both input values must have the same
+ * encoded length. Temporary array must be large enough to accommodate 4
+ * extra values of that length. Arrays u, v and tmp may not overlap with
+ * each other, or with either x or y. All integers use stride 1.
+ */
+#define zint_bezout   Zn(zint_bezout)
+int zint_bezout(uint32_t *restrict u, uint32_t *restrict v,
+	const uint32_t *restrict x, const uint32_t *restrict y,
+	size_t len, uint32_t *restrict tmp);
+
+/*
+ * Add k*(2^sc)*y to x. The result is assumed to fit in the array of
+ * size xlen (truncation is applied if necessary).
+ * Scale factor sc is provided as sch and scl, such that:
+ *    sch = sc / 31
+ *    scl = sc % 31  (in the 0..30 range)
+ * xlen MUST NOT be lower than ylen; however, it is allowed that
+ * xlen is greater than ylen.
+ *
+ * x[] and y[] are both signed integers, using two's complement for
+ * negative values. They both use the same stride ('stride' parameter).
+ */
+#define zint_add_scaled_mul_small   Zn(zint_add_scaled_mul_small)
+void zint_add_scaled_mul_small(uint32_t *restrict x, size_t xlen,
+	const uint32_t *restrict y, size_t ylen, size_t stride,
+	int32_t k, uint32_t sch, uint32_t scl);
+
+/*
+ * Subtract y*2^sc from x. This is a specialized version of
+ * zint_add_scaled_mul_small(), with multiplier k = -1.
+ */
+#define zint_sub_scaled   Zn(zint_sub_scaled)
+void zint_sub_scaled(uint32_t *restrict x, size_t xlen,
+	const uint32_t *restrict y, size_t ylen, size_t stride,
+	uint32_t sch, uint32_t scl);
+
+/* ====================================================================== */
+/*
+ * Fixed-point numbers.
+ *
+ * For FFT and other computations with approximations, we use a fixed-point
+ * format over 64 bits; the top 32 bits are the integral part, and the low
+ * 32 bits are the fractional part.
+ */
+
+/*
+ * We wrap the type into a struct in order to detect any attempt at using
+ * arithmetic operators on values directly. Since all functions are inline,
+ * the compiler will be able to remove the wrapper, which will then have
+ * no runtime cost.
+ */
+typedef struct {
+	uint64_t v;
+} fxr;
+
+#define FXR(x)   { (x) }
+
+static inline fxr
+fxr_of(int32_t j)
+{
+	fxr x;
+
+	x.v = (uint64_t)j << 32;
+	return x;
+}
+
+static inline fxr
+fxr_of_scaled32(uint64_t t)
+{
+	fxr x;
+
+	x.v = t;
+	return x;
+}
+
+static inline fxr
+fxr_add(fxr x, fxr y)
+{
+	x.v += y.v;
+	return x;
+}
+
+static inline fxr
+fxr_sub(fxr x, fxr y)
+{
+	x.v -= y.v;
+	return x;
+}
+
+static inline fxr
+fxr_double(fxr x)
+{
+	x.v <<= 1;
+	return x;
+}
+
+static inline fxr
+fxr_neg(fxr x)
+{
+	x.v = -x.v;
+	return x;
+}
+
+static inline fxr
+fxr_abs(fxr x)
+{
+	x.v -= (x.v << 1) & (uint64_t)(*(int64_t *)&x.v >> 63);
+	return x;
+}
+
+static inline fxr
+fxr_mul(fxr x, fxr y)
+{
+#if defined __GNUC__ && defined __SIZEOF_INT128__
+	__int128 z;
+
+	z = (__int128)*(int64_t *)&x.v * (__int128)*(int64_t *)&y.v;
+	x.v = (uint64_t)(z >> 32);
+	return x;
+#else
+	int32_t xh, yh;
+	uint32_t xl, yl;
+	uint64_t z0, z1, z2, z3;
+
+	xl = (uint32_t)x.v;
+	yl = (uint32_t)y.v;
+	xh = (int32_t)(*(int64_t *)&x.v >> 32);
+	yh = (int32_t)(*(int64_t *)&y.v >> 32);
+	z0 = ((uint64_t)xl * (uint64_t)yl) >> 32;
+	z1 = (uint64_t)((int64_t)xl * (int64_t)yh);
+	z2 = (uint64_t)((int64_t)yl * (int64_t)xh);
+	z3 = (uint64_t)((int64_t)xh * (int64_t)yh) << 32;
+	x.v = z0 + z1 + z2 + z3;
+	return x;
+#endif
+}
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static inline __m256i
+fxr_mul_x4(__m256i ya, __m256i yb)
+{
+	__m256i ya_hi = _mm256_srli_epi64(ya, 32);
+	__m256i yb_hi = _mm256_srli_epi64(yb, 32);
+	__m256i y1 = _mm256_mul_epu32(ya, yb);
+	__m256i y2 = _mm256_mul_epu32(ya, yb_hi);
+	__m256i y3 = _mm256_mul_epu32(ya_hi, yb);
+	__m256i y4 = _mm256_mul_epu32(ya_hi, yb_hi);
+	y1 = _mm256_srli_epi64(y1, 32);
+	y4 = _mm256_slli_epi64(y4, 32);
+	__m256i y5 = _mm256_add_epi64(
+		_mm256_add_epi64(y1, y2),
+		_mm256_add_epi64(y3, y4));
+	__m256i yna = _mm256_srai_epi32(ya, 31);
+	__m256i ynb = _mm256_srai_epi32(yb, 31);
+	return _mm256_sub_epi64(y5,
+		_mm256_add_epi64(
+			_mm256_and_si256(_mm256_slli_epi64(yb, 32), yna),
+			_mm256_and_si256(_mm256_slli_epi64(ya, 32), ynb)));
+}
+#endif // NTRUGEN_AVX2
+
+static inline fxr
+fxr_sqr(fxr x)
+{
+#if defined __GNUC__ && defined __SIZEOF_INT128__
+	int64_t t;
+	__int128 z;
+
+	t = *(int64_t *)&x.v;
+	z = (__int128)t * (__int128)t;
+	x.v = (uint64_t)(z >> 32);
+	return x;
+#else
+	int32_t xh;
+	uint32_t xl;
+	uint64_t z0, z1, z3;
+
+	xl = (uint32_t)x.v;
+	xh = (int32_t)(*(int64_t *)&x.v >> 32);
+	z0 = ((uint64_t)xl * (uint64_t)xl) >> 32;
+	z1 = (uint64_t)((int64_t)xl * (int64_t)xh);
+	z3 = (uint64_t)((int64_t)xh * (int64_t)xh) << 32;
+	x.v = z0 + (z1 << 1) + z3;
+	return x;
+#endif
+}
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static inline __m256i
+fxr_sqr_x4(__m256i ya)
+{
+	__m256i ya_hi = _mm256_srli_epi64(ya, 32);
+	__m256i y1 = _mm256_mul_epu32(ya, ya);
+	__m256i y2 = _mm256_mul_epu32(ya, ya_hi);
+	__m256i y3 = _mm256_mul_epu32(ya_hi, ya_hi);
+	y1 = _mm256_srli_epi64(y1, 32);
+	y2 = _mm256_add_epi64(y2, y2);
+	y3 = _mm256_slli_epi64(y3, 32);
+	__m256i y4 = _mm256_add_epi64(_mm256_add_epi64(y1, y2), y3);
+	return _mm256_sub_epi64(y4,
+		_mm256_and_si256(_mm256_slli_epi64(ya, 33),
+		_mm256_srai_epi32(ya, 31)));
+}
+#endif // NTRUGEN_AVX2
+
+static inline int32_t
+fxr_round(fxr x)
+{
+	x.v += 0x80000000ul;
+	return (int32_t)(*(int64_t *)&x.v >> 32);
+}
+
+static inline fxr
+fxr_div2e(fxr x, unsigned n)
+{
+	int64_t v;
+
+	v = *(int64_t *)&x.v;
+	x.v = (uint64_t)((v + (((int64_t)1 << n) >> 1)) >> n);
+	return x;
+}
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static inline __m256i
+fxr_half_x4(__m256i ya)
+{
+	const __m256i y1 = _mm256_set1_epi64x(1);
+	const __m256i yh = _mm256_set1_epi64x((uint64_t)1 << 63);
+	ya = _mm256_add_epi64(ya, y1);
+	return _mm256_or_si256(
+		_mm256_srli_epi64(ya, 1),
+		_mm256_and_si256(ya, yh));
+}
+#endif // NTRUGEN_AVX2
+
+static inline fxr
+fxr_mul2e(fxr x, unsigned n)
+{
+	x.v <<= n;
+	return x;
+}
+
+#define inner_fxr_div   Zn(inner_fxr_div)
+uint64_t inner_fxr_div(uint64_t x, uint64_t y);
+
+static inline fxr
+fxr_inv(fxr x)
+{
+	x.v = inner_fxr_div((uint64_t)1 << 32, x.v);
+	return x;
+}
+
+static inline fxr
+fxr_div(fxr x, fxr y)
+{
+	x.v = inner_fxr_div(x.v, y.v);
+	return x;
+}
+
+#if NTRUGEN_AVX2
+#define fxr_div_x4   Zn(fxr_div_x4)
+TARGET_AVX2 __m256i fxr_div_x4(__m256i yn, __m256i yd);
+
+/*
+ * Divide four values (n0..n3) by the same divisor (d).
+ */
+TARGET_AVX2
+static inline void
+fxr_div_x4_1(fxr *n0, fxr *n1, fxr *n2, fxr *n3, fxr d)
+{
+	__m256i yn = _mm256_setr_epi64x(n0->v, n1->v, n2->v, n3->v);
+	__m256i yd = _mm256_set1_epi64x(d.v);
+	union {
+		__m256i y;
+		uint64_t q[4];
+	} z;
+	z.y = fxr_div_x4(yn, yd);
+	n0->v = z.q[0];
+	n1->v = z.q[1];
+	n2->v = z.q[2];
+	n3->v = z.q[3];
+}
+#endif // NTRUGEN_AVX2
+
+static inline int
+fxr_lt(fxr x, fxr y)
+{
+	return *(int64_t *)&x.v < *(int64_t *)&y.v;
+}
+
+static const fxr fxr_zero = { 0 };
+static const fxr fxr_sqrt2 = { 6074001000ull };
+
+/*
+ * A complex value.
+ */
+typedef struct {
+	fxr re, im;
+} fxc;
+
+#define FXC(re, im)   { FXR(re), FXR(im) }
+
+static inline fxc
+fxc_add(fxc x, fxc y)
+{
+	x.re = fxr_add(x.re, y.re);
+	x.im = fxr_add(x.im, y.im);
+	return x;
+}
+
+static inline fxc
+fxc_sub(fxc x, fxc y)
+{
+	x.re = fxr_sub(x.re, y.re);
+	x.im = fxr_sub(x.im, y.im);
+	return x;
+}
+
+static inline fxc
+fxc_half(fxc x)
+{
+	x.re = fxr_div2e(x.re, 1);
+	x.im = fxr_div2e(x.im, 1);
+	return x;
+}
+
+static inline fxc
+fxc_mul(fxc x, fxc y)
+{
+	/*
+	 * We are computing r = (a + i*b)*(c + i*d) with:
+	 *   z0 = a*c
+	 *   z1 = b*d
+	 *   z2 = (a + b)*(c + d)
+	 *   r = (z0 - z1) + i*(z2 - (z0 + z1))
+	 * Since the intermediate values are truncated to our precision,
+	 * the imaginary value of r _may_ be slightly different from
+	 * a*d + b*c (if we had calculated it directly). For full
+	 * reproducibility, all implementations should use the formulas
+	 * above.
+	 */
+	fxr z0 = fxr_mul(x.re, y.re);
+	fxr z1 = fxr_mul(x.im, y.im);
+	fxr z2 = fxr_mul(fxr_add(x.re, x.im), fxr_add(y.re, y.im));
+	fxc z;
+	z.re = fxr_sub(z0, z1);
+	z.im = fxr_sub(z2, fxr_add(z0, z1));
+	return z;
+}
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static inline void
+fxc_mul_x4(__m256i *yd_re, __m256i *yd_im,
+	__m256i ya_re, __m256i ya_im, __m256i yb_re, __m256i yb_im)
+{
+	__m256i y0 = fxr_mul_x4(ya_re, yb_re);
+	__m256i y1 = fxr_mul_x4(ya_im, yb_im);
+	__m256i y2 = fxr_mul_x4(
+		_mm256_add_epi64(ya_re, ya_im),
+		_mm256_add_epi64(yb_re, yb_im));
+	*yd_re = _mm256_sub_epi64(y0, y1);
+	*yd_im = _mm256_sub_epi64(y2, _mm256_add_epi64(y0, y1));
+}
+#endif // NTRUGEN_AVX2
+
+static inline fxc
+fxc_conj(fxc x)
+{
+	x.im = fxr_neg(x.im);
+	return x;
+}
+
+/*
+ * In FFT representation, we keep only half of the coefficients, because
+ * all our vectors are real in non-FFT representation; thus, the FFT
+ * representation is redundant. For 0 <= k < n/2, f[k] contains the
+ * real part of FFT coefficient k, and f[k + n/2] contains the imaginary
+ * part of FFT coefficient k.
+ */
+
+/*
+ * Convert a (real) vector to its FFT representation.
+ */
+#define vect_FFT   Zn(vect_FFT)
+void vect_FFT(unsigned logn, fxr *f);
+
+/*
+ * Convert back from FFT representation into a real vector.
+ */
+#define vect_iFFT   Zn(vect_iFFT)
+void vect_iFFT(unsigned logn, fxr *f);
+
+/*
+ * Set a vector d to the value of the small polynomial f.
+ */
+#define vect_set   Zn(vect_set)
+void vect_set(unsigned logn, fxr *d, const int8_t *f);
+
+/*
+ * Add vector b to vector a. This works in both real and FFT representations.
+ * Vectors a and b MUST NOT overlap.
+ */
+#define vect_add   Zn(vect_add)
+void vect_add(unsigned logn, fxr *restrict a, const fxr *restrict b);
+
+/*
+ * Multiply vector a by the real constant c. This works in both real
+ * and FFT representations.
+ */
+#define vect_mul_realconst   Zn(vect_mul_realconst)
+void vect_mul_realconst(unsigned logn, fxr *a, fxr c);
+
+/*
+ * Multiply a vector by 2^e. This works in both real and FFT representations.
+ */
+#define vect_mul2e   Zn(vect_mul2e)
+void vect_mul2e(unsigned logn, fxr *a, unsigned e);
+
+/*
+ * Multiply vector a by vector b. The vectors must be in FFT representation.
+ * Vectors a and b MUST NOT overlap.
+ */
+#define vect_mul_fft   Zn(vect_mul_fft)
+void vect_mul_fft(unsigned logn, fxr *restrict a, const fxr *restrict b);
+
+/*
+ * Convert a vector into its adjoint (in FFT representation).
+ */
+#define vect_adj_fft   Zn(vect_adj_fft)
+void vect_adj_fft(unsigned logn, fxr *a);
+
+/*
+ * Multiply vector a by auto-adjoint vector b. The vectors must be in FFT
+ * representation. Since the FFT representation of an auto-adjoint vector
+ * contains only real number, the second half of b contains only zeros and
+ * is not accessed by this function. Vectors a and b MUST NOT overlap.
+ */
+#define vect_mul_autoadj_fft   Zn(vect_mul_autoadj_fft)
+void vect_mul_autoadj_fft(unsigned logn,
+	fxr *restrict a, const fxr *restrict b);
+
+/*
+ * Divide vector a by auto-adjoint vector b. The vectors must be in FFT
+ * representation. Since the FFT representation of an auto-adjoint vector
+ * contains only real number, the second half of b contains only zeros and
+ * is not accessed by this function. Vectors a and b MUST NOT overlap.
+ */
+#define vect_div_autoadj_fft   Zn(vect_div_autoadj_fft)
+void vect_div_autoadj_fft(unsigned logn,
+	fxr *restrict a, const fxr *restrict b);
+
+/*
+ * Compute d = a*adj(a) + b*adj(b). Polynomials are in FFT representation.
+ * Since d is auto-adjoint, only its first half is set; the second half
+ * is _implicitly_ zero (this function does not access the second half of d).
+ * Vectors a, b and d MUST NOT overlap.
+ */
+#define vect_norm_fft   Zn(vect_norm_fft)
+void vect_norm_fft(unsigned logn, fxr *restrict d,
+	const fxr *restrict a, const fxr *restrict b);
+
+/*
+ * Compute d = (2^e)/(a*adj(a) + b*adj(b)). Polynomials are in FFT
+ * representation. Since d is auto-adjoint, only its first half is set; the
+ * second half is _implicitly_ zero (this function does not access the
+ * second half of d). Vectors a, b and d MUST NOT overlap.
+ */
+#define vect_invnorm_fft   Zn(vect_invnorm_fft)
+void vect_invnorm_fft(unsigned logn, fxr *restrict d,
+	const fxr *restrict a, const fxr *restrict b, unsigned e);
+
+/* ==================================================================== */
+/*
+ * Code for polynomials with integer coefficients.
+ *
+ * Polynomials use an interleaved in-memory representation:
+ *
+ *   There are n = 2^logn coefficients (degrees 0 to n-1).
+ *   Each coefficient contains 'len' words (zint limbs or RNS).
+ *   Each coefficient has a stride of 'n'.
+ *   The first (lowest) words of the respective coefficients are consecutive.
+ */
+
+/*
+ * Load a one-byte polynomial with reduction modulo p.
+ */
+#define poly_mp_set_small   Zn(poly_mp_set_small)
+void poly_mp_set_small(unsigned logn, uint32_t *restrict d,
+	const int8_t *restrict f, uint32_t p);
+
+/*
+ * Convert a polynomial in one-word normal representation (signed) into RNS
+ * modulo the single prime p.
+ */
+#define poly_mp_set   Zn(poly_mp_set)
+void poly_mp_set(unsigned logn, uint32_t *f, uint32_t p);
+
+/*
+ * Convert a polynomial in RNS (modulo a single prime p) into one-word
+ * normal representation (signed).
+ */
+#define poly_mp_norm   Zn(poly_mp_norm)
+void poly_mp_norm(unsigned logn, uint32_t *f, uint32_t p);
+
+/*
+ * Convert a polynomial to small integers. Source values are supposed
+ * to be normalized (signed). Returned value is 0 if any of the
+ * coefficients exceeds the provided limit (in absolute value); on
+ * success, 1 is returned.
+ *
+ * In case of failure, the function returns earlier; this does not
+ * break constant-time discipline as long as a failure implies that the
+ * (f,g) polynomials are discarded.
+ */
+#define poly_big_to_small   Zn(poly_big_to_small)
+int poly_big_to_small(unsigned logn, int8_t *restrict d,
+	const uint32_t *restrict s, int lim);
+
+/*
+ * Get the maximum bit length of all coefficients of a polynomial. Each
+ * coefficient has size flen words.
+ *
+ * The bit length of a big integer is defined to be the length of the
+ * minimal binary representation, using two's complement for negative
+ * values, and excluding the sign bit. This definition implies that
+ * if x = 2^k, then x has bit length k but -x has bit length k-1. For
+ * non powers of two, x and -x have the same bit length.
+ *
+ * This function is constant-time with regard to coefficient values and
+ * the returned bit length.
+ */
+#define poly_max_bitlength   Zn(poly_max_bitlength)
+uint32_t poly_max_bitlength(unsigned logn, const uint32_t *f, size_t flen);
+
+/*
+ * Compute q = x / 31 and r = x % 31 for an unsigned integer x. This
+ * macro is constant-time and works for values x up to 63487 (inclusive).
+ */
+#define DIVREM31(q, r, x)  { \
+		uint32_t divrem31_q, divrem31_x; \
+		divrem31_x = (x); \
+		divrem31_q = (uint32_t)(divrem31_x * (uint32_t)67651) >> 21; \
+		(q) = divrem31_q; \
+		(r) = divrem31_x - 31 * divrem31_q; \
+	} while (0)
+
+/*
+ * Convert a polynomial to fixed-point approximations, with scaling.
+ * For each coefficient x, the computed approximation is x/2^sc.
+ * This function assumes that |x| < 2^(30+sc). The length of each
+ * coefficient must be less than 2^24 words.
+ *
+ * This function is constant-time with regard to the coefficient values
+ * and to the scaling factor.
+ */
+#define poly_big_to_fixed   Zn(poly_big_to_fixed)
+void poly_big_to_fixed(unsigned logn, fxr *restrict d,
+	const uint32_t *restrict f, size_t len, uint32_t sc);
+
+/*
+ * Subtract k*f from F, where F, f and k are polynomials modulo X^n+1.
+ * Coefficients of polynomial k are small integers (signed values in the
+ * -2^31..+2^31 range) scaled by 2^sc.
+ *
+ * This function implements the basic quadratic multiplication algorithm,
+ * which is efficient in space (no extra buffer needed) but slow at
+ * high degree.
+ */
+#define poly_sub_scaled   Zn(poly_sub_scaled)
+void poly_sub_scaled(unsigned logn,
+	uint32_t *restrict F, size_t Flen,
+	const uint32_t *restrict f, size_t flen,
+	const int32_t *restrict k, uint32_t sc);
+
+/*
+ * Subtract k*f from F. Coefficients of polynomial k are small integers
+ * (signed values in the -2^31..+2^31 range) scaled by 2^sc. Polynomial f
+ * MUST be in RNS+NTT over flen+1 words (even though f itself would fit on
+ * flen words); polynomial F MUST be in plain representation.
+ */
+#define poly_sub_scaled_ntt   Zn(poly_sub_scaled_ntt)
+void
+poly_sub_scaled_ntt(unsigned logn, uint32_t *restrict F, size_t Flen,
+	const uint32_t *restrict f, size_t flen,
+	const int32_t *restrict k, uint32_t sc, uint32_t *restrict tmp);
+
+/*
+ * depth = 1
+ * logn = logn_top - depth
+ * Inputs:
+ *    F, G    polynomials of degree 2^logn, plain integer representation (FGlen)
+ *    FGlen   size of each coefficient of F and G (must be 1 or 2)
+ *    f, g    polynomials of degree 2^logn_top, small coefficients
+ *    k       polynomial of degree 2^logn (plain, 32-bit)
+ *    sc      scaling logarithm (public value)
+ *    tmp     temporary with room at least max(FGlen, 2^logn_top) words
+ * Operation:
+ *    F <- F - (2^sc)*k*ft
+ *    G <- G - (2^sc)*k*gt
+ * with (ft,gt) being the degree-n polynomials corresponding to (f,g)
+ * It is assumed that the result fits.
+ *
+ * WARNING: polynomial k is consumed in the process.
+ *
+ * This function uses 3*n words in tmp[].
+ */
+#define poly_sub_kfg_scaled_depth1   Zn(poly_sub_kfg_scaled_depth1)
+void poly_sub_kfg_scaled_depth1(unsigned logn_top,
+	uint32_t *restrict F, uint32_t *restrict G, size_t FGlen,
+	uint32_t *restrict k, uint32_t sc,
+	const int8_t *restrict f, const int8_t *restrict g,
+	uint32_t *restrict tmp);
+
+/*
+ * Check whether the provided polynomial is invertible modulo X^n+1
+ * and modulo some small prime r which is such that r-1 is a multiple
+ * of 2048. Parameters:
+ *    logn     degree logarithm
+ *    f        polynomial to test
+ *    r        small prime to use
+ *    p        p = r*t, such that t is prime and (4/3)*2^30 < p < 2^31
+ *    p0i      -1/p mod 2^32
+ *    s        s'*2^32 mod p, for some s' such that s'^1024 = -1 mod r
+ *    rm, rs   division by r parameters
+ *    tmp      temporary buffer
+ * rm and rs must be such that floor(x*rm/(2^rs)) == floor(x/r) for all
+ * x in [0..p-1]; such values always exist.
+ *
+ * Return value: 1 if f is invertible, 0 otherwise.
+ *
+ * RAM USAGE: 2*n words
+ */
+#define poly_is_invertible   Zn(poly_is_invertible)
+int
+poly_is_invertible(unsigned logn, const int8_t *restrict f,
+	uint32_t p, uint32_t p0i, uint32_t s,
+	uint32_t r, uint32_t rm, unsigned rs, uint32_t *restrict tmp);
+
+/*
+ * Similar to poly_is_invertible(), except that we test two prime moduli
+ * at the same time. We have p = r1*r2*t, with both r1-1 and r2-1 being
+ * multiples of 2048, and s is a 2048-th root of 1 modulo both r1 and r2
+ * (s is in Montgomery representation).
+ */
+#define poly_is_invertible_ext   Zn(poly_is_invertible_ext)
+int
+poly_is_invertible_ext(unsigned logn, const int8_t *restrict f,
+	uint32_t r1, uint32_t r2, uint32_t p, uint32_t p0i, uint32_t s,
+	uint32_t r1m, unsigned r1s, uint32_t r2m, unsigned r2s,
+	uint32_t *restrict tmp);
+
+/* ==================================================================== */
+/*
+ * For a Gaussian distribution centred on zero and with standard deviation
+ * sigma, we define:
+ *    D(x) = exp(-(x^2)/(2*sigma^2))
+ * The probability of integer k to be selected is then D(k)/(sum_j D(j)).
+ *
+ * We then define probabilities:
+ *    P(k) = (\sum_{j<=k} D(j))/(\sum_{all j} D(j))
+ * i.e. P(k) is the probability of the selected integer to be at most k.
+ * We scale and round these values into:
+ *    Q(k) = round((2^16-1)*P(k))
+ * (Note: 2^16-1, not 2^16, so that the max value is 65535)
+ * We then store these values Q(k). The head values are all equal to 0,
+ * the tail values are all equal to 65535; each table contains the values
+ * Q(k) for k = -kmax to +(kmax-1), for the integer kmax such that
+ * Q(-kmax) != 0 but Q(-kmax-1) = 0.
+ *
+ * Actual selection consists in the following:
+ *    generate a random 16-bit integer x (with 0 <= x <= 65535)
+ *    k = -kmax
+ *    while (k < +kmax) and (Q(k) < x):
+ *        k = k + 1
+ *    return k
+ * This process returns a value in the [-kmax; +kmax] range.
+ *
+ * The following deviations (sigma) are used, for various algorithms:
+ *
+ *    name          degree n       sigma
+ *    Falcon-n      2 to 1024      1.17*sqrt(12289/n)
+ *    BAT-128-256      256         sqrt(sqrt(2)*128/512)
+ *    BAT-257-512      512         sqrt(sqrt(2)*257/1024)
+ *    BAT-769-1024    1024         sqrt(sqrt(5*log(5)/24)*769/1024)
+ *    Hawk-256         256         1.1
+ *    Hawk-512         512         1.5
+ *    Hawk-1024       1024         2.0
+ *
+ * All algorithms target f*G - g*F = q for some integer q; Falcon uses
+ * q = 12289; BAT uses q = 128, 257 or 769 (depending on the degree n);
+ * Hawk uses q = 1.
+ *
+ * Table format:
+ * -------------
+ *
+ * The in-memory format for each table is the value of kmax, followed
+ * by the 2*kmax values of Q(k) for k = -kmax to +kmax-1.
+ *
+ * Special case for Falcon reduced versions:
+ * -----------------------------------------
+ * Standard Falcon uses n = 512 or 1024. We include a table for n = 256
+ * ("small" version, maybe useful for some use cases with very small
+ * embedded systems?). We also support reduced versions (useful for
+ * tests) for degrees n = 2 to 128, by using the n = 256 table
+ * repeatedly. Namely, we use the table 256/n times, and add the values
+ * together. This still follows the proper distribution, without
+ * requiring any extra table. We have to enforce an arbitrary limit on
+ * values, so that the resulting f and g are encodable in a way
+ * compatible with the reference Falcon code; namely, for n <= 32, we
+ * reject values which are not in [-127..+127]. This conditional
+ * rejection means that the source RNG may have to be invoked a few more
+ * times to get enough random material. This does not happen for the
+ * degrees 64 to 1024, where the maximum output value (kmax) always fits
+ * within the encoding format limit.
+ */
+
+/* BAT, q = 128, n = 256 -> kmax = 2 */
+#define gauss_BAT_128_256   Zn(gauss_BAT_128_256)
+extern const uint16_t gauss_BAT_128_256[];
+
+/* BAT, q = 257, n = 512 -> kmax = 2 */
+#define gauss_BAT_257_512   Zn(gauss_BAT_257_512)
+extern const uint16_t gauss_BAT_257_512[];
+
+/* BAT, q = 769, n = 1024 -> kmax = 3 */
+#define gauss_BAT_769_1024   Zn(gauss_BAT_769_1024)
+extern const uint16_t gauss_BAT_769_1024[];
+
+/* Falcon, q = 12289, n = 256 -> kmax = 24 */
+#define gauss_Falcon_256   Zn(gauss_Falcon_256)
+extern const uint16_t gauss_Falcon_256[];
+
+/* Falcon, q = 12289, n = 512 -> kmax = 17 */
+#define gauss_Falcon_512   Zn(gauss_Falcon_512)
+extern const uint16_t gauss_Falcon_512[];
+
+/* Falcon, q = 12289, n = 1024 -> kmax = 12 */
+#define gauss_Falcon_1024   Zn(gauss_Falcon_1024)
+extern const uint16_t gauss_Falcon_1024[];
+
+/*
+ * For a given random source and selection table, generate polynomial f
+ * This function ensures that the returned f has odd parity; if the
+ * produced polynomial has even parity, it is discarded and the process
+ * loops.
+ */
+#define gauss_sample_poly   Zn(gauss_sample_poly)
+void gauss_sample_poly(unsigned logn, int8_t *f,
+	const uint16_t *tab, ntrugen_rng rng, void *rng_context);
+
+/*
+ * Special sampling function for reduced versions: provided table should
+ * be for degree n=256, and 256/(2^n) samples are added together for each
+ * of the 2^n output values (re-sampling is performed when a value does
+ * not fit in [-127..+127], so the amount of consumed randomness may vary;
+ * minimum usage is 512 bytes).
+ */
+#define gauss_sample_poly_reduced   Zn(gauss_sample_poly_reduced)
+void gauss_sample_poly_reduced(unsigned logn, int8_t *f,
+	const uint16_t *tab, ntrugen_rng rng, void *rng_context);
+
+/* see ng_inner.h */
+#define poly_sqnorm   Zn(poly_sqnorm)
+uint32_t poly_sqnorm(unsigned logn, const int8_t *f);
+
+/* ==================================================================== */
+
+/*
+ * Lengths of values (big integers) in 31-bit limbs:
+ *   max_bl_small[d]      max length of coeffs of (f,g) at depth d
+ *   max_bl_small[logn]   max length of Res(f,phi) and Res(g,phi)
+ *   max_bl_large[d]      max length of coeffs of unreduced (F,G) at depth d
+ *   word_win[d]          number of top limbs to consider in (f,g) (Babai's NP)
+ * Rules:
+ *   max_bl_small[0] = 1
+ *   max_bl_large[d] >= max_bl_small[d + 1]
+ *   1 <= word_win[d] <= max_bl_small[d]
+ * Additional rules to use the optimized depth0 function:
+ *   max_bl_large[0] = 1
+ *   max_bl_small[1] = 1
+ *
+ * q                target integer (f*G - g*F = q)
+ * min_logn         minimum logn supported
+ * max_logn         maximum logn supported
+ * reduce_bits      assumed reduction per iteration (in bits)
+ * coeff_FG_limit   maximum allowed value for coefficients of F and G
+ * min_save_fg      minimum depth at which (f,g) can be saved temporarily
+ */
+typedef struct {
+	uint32_t q;
+	unsigned min_logn, max_logn;
+	uint16_t max_bl_small[11];
+	uint16_t max_bl_large[10];
+	uint16_t word_win[10];
+	uint32_t reduce_bits;
+	uint8_t coeff_FG_limit[11];
+	uint16_t min_save_fg[11];
+} ntru_profile;
+
+/* Error code: no error (so far) */
+#define SOLVE_OK           0
+
+/* Error code: GCD(Res(f,X^n+1), Res(g,X^n+1)) != 1 */
+#define SOLVE_ERR_GCD      -1
+
+/* Error code: reduction error (NTRU equation no longer fulfilled) */
+#define SOLVE_ERR_REDUCE   -2
+
+/* Error code: output (F,G) coefficients are off-limits */
+#define SOLVE_ERR_LIMIT    -3
+
+/*
+ * Solve the NTRU equation for the provided (f,g).
+ * The (F,G) solution (if found) is returned at the start of the tmp[]
+ * array, as two consecutive int8_t[] values. Returned value is
+ * SOLVE_OK (0) on success, a negative error code on failure.
+ *
+ * Note: if f is not invertible modulo X^n+1 and modulo p = 2147473409,
+ * then an error (SOLVE_ERR_GCD) is reported. This test is not necessary
+ * for the computation itself, but fulfilling it implies that G will
+ * be recoverable later on from f, g and F. Only a very small proportion
+ * of possible polynomials f are not invertible modulo X^n+1 and p.
+ *
+ * RAM USAGE: 6*n words
+ */
+#define solve_NTRU   Zn(solve_NTRU)
+int solve_NTRU(const ntru_profile *prof, unsigned logn,
+	const int8_t *restrict f, const int8_t *restrict g, uint32_t *tmp);
+
+/*
+ * Recompute G from f, g and F (using the NTRU equation f*G - g*F = q).
+ * This may fail if f is not invertible modulo X^n+1 and modulo
+ * p = 2147473409. However, the rest of this key pair generator code takes
+ * care never to generate keys with such polynomials f.
+ *
+ * G is returned as the first n bytes of tmp.
+ *
+ * Returned value is 1 on success, 0 on error. An error is reported if
+ * f is not invertible, or if any of the reconstructed G coefficients is
+ * not in the [-lim..+lim] range.
+ *
+ * RAM USAGE: 3*n words
+ */
+#define recover_G   Zn(recover_G)
+int recover_G(unsigned logn, int32_t q, uint32_t lim,
+	const int8_t *restrict f, const int8_t *restrict g,
+	const int8_t *restrict F, uint32_t *restrict tmp);
+
+/* ==================================================================== */
+
+#if NTRUGEN_STATS
+/*
+ * Statistics are gathered in some global variables. These variables are
+ * not thread-safe; thus, this should be disabled by default.
+ */
+#define stats_hawk_ctt_attempt   Zn(stats_hawk_ctt_attempt)
+extern uint32_t stats_hawk_ctt_attempt;
+#define stats_hawk_ctt_reject   Zn(stats_hawk_ctt_reject)
+extern uint32_t stats_hawk_ctt_reject;
+#define stats_solve_attempt   Zn(stats_solve_attempt)
+extern uint32_t stats_solve_attempt;
+#define stats_solve_err_gcd   Zn(stats_solve_err_gcd)
+extern uint32_t stats_solve_err_gcd;
+#define stats_solve_err_reduce   Zn(stats_solve_err_reduce)
+extern uint32_t stats_solve_err_reduce;
+#define stats_solve_err_limit   Zn(stats_solve_err_limit)
+extern uint32_t stats_solve_err_limit;
+#define stats_solve_success   Zn(stats_solve_success)
+extern uint32_t stats_solve_success;
+#define stats_compute_w_attempt   Zn(stats_compute_w_attempt)
+extern uint32_t stats_compute_w_attempt;
+#define stats_compute_w_err_lim1   Zn(stats_compute_w_err_lim1)
+extern uint32_t stats_compute_w_err_lim1;
+#define stats_compute_w_err_lim2   Zn(stats_compute_w_err_lim2)
+extern uint32_t stats_compute_w_err_lim2;
+#define stats_compute_w_err_lim3   Zn(stats_compute_w_err_lim3)
+extern uint32_t stats_compute_w_err_lim3;
+#define stats_compute_w_err_norm   Zn(stats_compute_w_err_norm)
+extern uint32_t stats_compute_w_err_norm;
+#define stats_compute_w_success   Zn(stats_compute_w_success)
+extern uint32_t stats_compute_w_success;
+
+/*
+ * Initialize (or reset) the statistics-gathering counters.
+ */
+#define stats_init   Zn(stats_init)
+void stats_init(void);
+#endif
+
+/* ==================================================================== */
+
+#endif
diff --git a/lib/dns/hawk/ng_mp31.c b/lib/dns/hawk/ng_mp31.c
new file mode 100644
index 0000000000..290655b968
--- /dev/null
+++ b/lib/dns/hawk/ng_mp31.c
@@ -0,0 +1,1032 @@
+#include "ng_inner.h"
+
+/* see ng_inner.h */
+uint32_t
+mp_div(uint32_t x, uint32_t y, uint32_t p)
+{
+	uint32_t a = y;
+	uint32_t b = p;
+	uint32_t u = x;
+	uint32_t v = 0;
+	for (int i = 0; i < 62; i ++) {
+		uint32_t a_odd = -(a & 1);
+		uint32_t swap = tbmask(a - b) & a_odd;
+		uint32_t t1 = swap & (a ^ b);
+		a ^= t1;
+		b ^= t1;
+		uint32_t t2 = swap & (u ^ v);
+		u ^= t2;
+		v ^= t2;
+		a -= a_odd & b;
+		u = mp_sub(u, a_odd & v, p);
+		a >>= 1;
+		u = mp_half(u, p);
+	}
+
+	/* GCD is in b; it is 1 if and only if y was invertible.
+	   Otherwise, the GCD is greater than 1. */
+	return v & tbmask(b - 2);
+}
+
+/*
+ * Bit-reversal index table (over 10 bits).
+ */
+static const uint16_t REV10[] = {
+	   0,  512,  256,  768,  128,  640,  384,  896,   64,  576,  320,  832,
+	 192,  704,  448,  960,   32,  544,  288,  800,  160,  672,  416,  928,
+	  96,  608,  352,  864,  224,  736,  480,  992,   16,  528,  272,  784,
+	 144,  656,  400,  912,   80,  592,  336,  848,  208,  720,  464,  976,
+	  48,  560,  304,  816,  176,  688,  432,  944,  112,  624,  368,  880,
+	 240,  752,  496, 1008,    8,  520,  264,  776,  136,  648,  392,  904,
+	  72,  584,  328,  840,  200,  712,  456,  968,   40,  552,  296,  808,
+	 168,  680,  424,  936,  104,  616,  360,  872,  232,  744,  488, 1000,
+	  24,  536,  280,  792,  152,  664,  408,  920,   88,  600,  344,  856,
+	 216,  728,  472,  984,   56,  568,  312,  824,  184,  696,  440,  952,
+	 120,  632,  376,  888,  248,  760,  504, 1016,    4,  516,  260,  772,
+	 132,  644,  388,  900,   68,  580,  324,  836,  196,  708,  452,  964,
+	  36,  548,  292,  804,  164,  676,  420,  932,  100,  612,  356,  868,
+	 228,  740,  484,  996,   20,  532,  276,  788,  148,  660,  404,  916,
+	  84,  596,  340,  852,  212,  724,  468,  980,   52,  564,  308,  820,
+	 180,  692,  436,  948,  116,  628,  372,  884,  244,  756,  500, 1012,
+	  12,  524,  268,  780,  140,  652,  396,  908,   76,  588,  332,  844,
+	 204,  716,  460,  972,   44,  556,  300,  812,  172,  684,  428,  940,
+	 108,  620,  364,  876,  236,  748,  492, 1004,   28,  540,  284,  796,
+	 156,  668,  412,  924,   92,  604,  348,  860,  220,  732,  476,  988,
+	  60,  572,  316,  828,  188,  700,  444,  956,  124,  636,  380,  892,
+	 252,  764,  508, 1020,    2,  514,  258,  770,  130,  642,  386,  898,
+	  66,  578,  322,  834,  194,  706,  450,  962,   34,  546,  290,  802,
+	 162,  674,  418,  930,   98,  610,  354,  866,  226,  738,  482,  994,
+	  18,  530,  274,  786,  146,  658,  402,  914,   82,  594,  338,  850,
+	 210,  722,  466,  978,   50,  562,  306,  818,  178,  690,  434,  946,
+	 114,  626,  370,  882,  242,  754,  498, 1010,   10,  522,  266,  778,
+	 138,  650,  394,  906,   74,  586,  330,  842,  202,  714,  458,  970,
+	  42,  554,  298,  810,  170,  682,  426,  938,  106,  618,  362,  874,
+	 234,  746,  490, 1002,   26,  538,  282,  794,  154,  666,  410,  922,
+	  90,  602,  346,  858,  218,  730,  474,  986,   58,  570,  314,  826,
+	 186,  698,  442,  954,  122,  634,  378,  890,  250,  762,  506, 1018,
+	   6,  518,  262,  774,  134,  646,  390,  902,   70,  582,  326,  838,
+	 198,  710,  454,  966,   38,  550,  294,  806,  166,  678,  422,  934,
+	 102,  614,  358,  870,  230,  742,  486,  998,   22,  534,  278,  790,
+	 150,  662,  406,  918,   86,  598,  342,  854,  214,  726,  470,  982,
+	  54,  566,  310,  822,  182,  694,  438,  950,  118,  630,  374,  886,
+	 246,  758,  502, 1014,   14,  526,  270,  782,  142,  654,  398,  910,
+	  78,  590,  334,  846,  206,  718,  462,  974,   46,  558,  302,  814,
+	 174,  686,  430,  942,  110,  622,  366,  878,  238,  750,  494, 1006,
+	  30,  542,  286,  798,  158,  670,  414,  926,   94,  606,  350,  862,
+	 222,  734,  478,  990,   62,  574,  318,  830,  190,  702,  446,  958,
+	 126,  638,  382,  894,  254,  766,  510, 1022,    1,  513,  257,  769,
+	 129,  641,  385,  897,   65,  577,  321,  833,  193,  705,  449,  961,
+	  33,  545,  289,  801,  161,  673,  417,  929,   97,  609,  353,  865,
+	 225,  737,  481,  993,   17,  529,  273,  785,  145,  657,  401,  913,
+	  81,  593,  337,  849,  209,  721,  465,  977,   49,  561,  305,  817,
+	 177,  689,  433,  945,  113,  625,  369,  881,  241,  753,  497, 1009,
+	   9,  521,  265,  777,  137,  649,  393,  905,   73,  585,  329,  841,
+	 201,  713,  457,  969,   41,  553,  297,  809,  169,  681,  425,  937,
+	 105,  617,  361,  873,  233,  745,  489, 1001,   25,  537,  281,  793,
+	 153,  665,  409,  921,   89,  601,  345,  857,  217,  729,  473,  985,
+	  57,  569,  313,  825,  185,  697,  441,  953,  121,  633,  377,  889,
+	 249,  761,  505, 1017,    5,  517,  261,  773,  133,  645,  389,  901,
+	  69,  581,  325,  837,  197,  709,  453,  965,   37,  549,  293,  805,
+	 165,  677,  421,  933,  101,  613,  357,  869,  229,  741,  485,  997,
+	  21,  533,  277,  789,  149,  661,  405,  917,   85,  597,  341,  853,
+	 213,  725,  469,  981,   53,  565,  309,  821,  181,  693,  437,  949,
+	 117,  629,  373,  885,  245,  757,  501, 1013,   13,  525,  269,  781,
+	 141,  653,  397,  909,   77,  589,  333,  845,  205,  717,  461,  973,
+	  45,  557,  301,  813,  173,  685,  429,  941,  109,  621,  365,  877,
+	 237,  749,  493, 1005,   29,  541,  285,  797,  157,  669,  413,  925,
+	  93,  605,  349,  861,  221,  733,  477,  989,   61,  573,  317,  829,
+	 189,  701,  445,  957,  125,  637,  381,  893,  253,  765,  509, 1021,
+	   3,  515,  259,  771,  131,  643,  387,  899,   67,  579,  323,  835,
+	 195,  707,  451,  963,   35,  547,  291,  803,  163,  675,  419,  931,
+	  99,  611,  355,  867,  227,  739,  483,  995,   19,  531,  275,  787,
+	 147,  659,  403,  915,   83,  595,  339,  851,  211,  723,  467,  979,
+	  51,  563,  307,  819,  179,  691,  435,  947,  115,  627,  371,  883,
+	 243,  755,  499, 1011,   11,  523,  267,  779,  139,  651,  395,  907,
+	  75,  587,  331,  843,  203,  715,  459,  971,   43,  555,  299,  811,
+	 171,  683,  427,  939,  107,  619,  363,  875,  235,  747,  491, 1003,
+	  27,  539,  283,  795,  155,  667,  411,  923,   91,  603,  347,  859,
+	 219,  731,  475,  987,   59,  571,  315,  827,  187,  699,  443,  955,
+	 123,  635,  379,  891,  251,  763,  507, 1019,    7,  519,  263,  775,
+	 135,  647,  391,  903,   71,  583,  327,  839,  199,  711,  455,  967,
+	  39,  551,  295,  807,  167,  679,  423,  935,  103,  615,  359,  871,
+	 231,  743,  487,  999,   23,  535,  279,  791,  151,  663,  407,  919,
+	  87,  599,  343,  855,  215,  727,  471,  983,   55,  567,  311,  823,
+	 183,  695,  439,  951,  119,  631,  375,  887,  247,  759,  503, 1015,
+	  15,  527,  271,  783,  143,  655,  399,  911,   79,  591,  335,  847,
+	 207,  719,  463,  975,   47,  559,  303,  815,  175,  687,  431,  943,
+	 111,  623,  367,  879,  239,  751,  495, 1007,   31,  543,  287,  799,
+	 159,  671,  415,  927,   95,  607,  351,  863,  223,  735,  479,  991,
+	  63,  575,  319,  831,  191,  703,  447,  959,  127,  639,  383,  895,
+	 255,  767,  511, 1023
+};
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+mp_mkgmigm(unsigned logn, uint32_t *restrict gm, uint32_t *restrict igm,
+	uint32_t g, uint32_t ig, uint32_t p, uint32_t p0i)
+{
+	size_t n = (size_t)1 << logn;
+
+	/*
+	 * We want g^n = -1 mod p, so we may need to square the provided
+	 * g and ig a few times if logn < 10.
+	 */
+	for (unsigned j = logn; j < 10; j ++) {
+		g = mp_montymul(g, g, p, p0i);
+		ig = mp_montymul(ig, ig, p, p0i);
+	}
+
+#if NTRUGEN_AVX2
+	if (logn >= 3) {
+		/*
+		 * If u % 8 == 0, then gm[u + v] = gm[u]*(gx^v) for v = 1..7,
+		 * with gx = g^(1024/8) mod p.
+		 * (idem for igm[] and igx = ig^(1024/8) mod p)
+		 */
+		uint32_t gx = g;
+		for (unsigned j = 3; j < logn; j ++) {
+			gx = mp_montymul(gx, gx, p, p0i);
+		}
+		uint32_t ya0 = mp_R(p);
+		uint32_t ya4 = gx;
+		uint32_t ya2 = mp_montymul(ya4, gx, p, p0i);;
+		uint32_t ya6 = mp_montymul(ya2, gx, p, p0i);;
+		uint32_t ya1 = mp_montymul(ya6, gx, p, p0i);;
+		uint32_t ya5 = mp_montymul(ya1, gx, p, p0i);;
+		uint32_t ya3 = mp_montymul(ya5, gx, p, p0i);;
+		uint32_t ya7 = mp_montymul(ya3, gx, p, p0i);;
+		__m256i ya = _mm256_setr_epi32(
+			ya0, ya1, ya2, ya3, ya4, ya5, ya6, ya7);
+		__m256i yg = _mm256_set1_epi32(g);
+
+		uint32_t igx = ig;
+		for (unsigned j = 3; j < logn; j ++) {
+			igx = mp_montymul(igx, igx, p, p0i);
+		}
+		uint32_t yb0 = mp_hR(p);
+		uint32_t yb4 = mp_half(igx, p);
+		uint32_t yb2 = mp_montymul(yb4, igx, p, p0i);;
+		uint32_t yb6 = mp_montymul(yb2, igx, p, p0i);;
+		uint32_t yb1 = mp_montymul(yb6, igx, p, p0i);;
+		uint32_t yb5 = mp_montymul(yb1, igx, p, p0i);;
+		uint32_t yb3 = mp_montymul(yb5, igx, p, p0i);;
+		uint32_t yb7 = mp_montymul(yb3, igx, p, p0i);;
+		__m256i yb = _mm256_setr_epi32(
+			yb0, yb1, yb2, yb3, yb4, yb5, yb6, yb7);
+		__m256i yig = _mm256_set1_epi32(ig);
+
+		unsigned k = 10 - logn;
+		__m256i yp = _mm256_set1_epi32(p);
+		__m256i yp0i = _mm256_set1_epi32(p0i);
+		size_t u = 0;
+		for (;;) {
+			size_t v = REV10[u << k];
+			_mm256_storeu_si256(
+				(__m256i *)(gm + v), ya);
+			_mm256_storeu_si256(
+				(__m256i *)(igm + v), yb);
+			u ++;
+			if (u >= (n >> 3)) {
+				break;
+			}
+			ya = mp_montymul_x8(ya, yg, yp, yp0i);
+			yb = mp_montymul_x8(yb, yig, yp, yp0i);
+		}
+	} else {
+		unsigned k = 10 - logn;
+		uint32_t x1 = mp_R(p);
+		uint32_t x2 = mp_hR(p);
+		size_t u = 0;
+		for (;;) {
+			size_t v = REV10[u << k];
+			gm[v] = x1;
+			igm[v] = x2;
+			u ++;
+			if (u >= n) {
+				break;
+			}
+			x1 = mp_montymul(x1, g, p, p0i);
+			x2 = mp_montymul(x2, ig, p, p0i);
+		}
+	}
+#else // NTRUGEN_AVX2
+	unsigned k = 10 - logn;
+	uint32_t x1 = mp_R(p);
+	uint32_t x2 = mp_hR(p);
+	size_t u = 0;
+	for (;;) {
+		size_t v = REV10[u << k];
+		gm[v] = x1;
+		igm[v] = x2;
+		u ++;
+		if (u >= n) {
+			break;
+		}
+		x1 = mp_montymul(x1, g, p, p0i);
+		x2 = mp_montymul(x2, ig, p, p0i);
+	}
+#endif // NTRUGEN_AVX2
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+mp_mkgm(unsigned logn, uint32_t *restrict gm,
+	uint32_t g, uint32_t p, uint32_t p0i)
+{
+	size_t n = (size_t)1 << logn;
+
+	for (unsigned j = logn; j < 10; j ++) {
+		g = mp_montymul(g, g, p, p0i);
+	}
+
+#if NTRUGEN_AVX2
+	if (logn >= 3) {
+		uint32_t gx = g;
+		for (unsigned j = 3; j < logn; j ++) {
+			gx = mp_montymul(gx, gx, p, p0i);
+		}
+		uint32_t ya0 = mp_R(p);
+		uint32_t ya4 = gx;
+		uint32_t ya2 = mp_montymul(ya4, gx, p, p0i);;
+		uint32_t ya6 = mp_montymul(ya2, gx, p, p0i);;
+		uint32_t ya1 = mp_montymul(ya6, gx, p, p0i);;
+		uint32_t ya5 = mp_montymul(ya1, gx, p, p0i);;
+		uint32_t ya3 = mp_montymul(ya5, gx, p, p0i);;
+		uint32_t ya7 = mp_montymul(ya3, gx, p, p0i);;
+		__m256i ya = _mm256_setr_epi32(
+			ya0, ya1, ya2, ya3, ya4, ya5, ya6, ya7);
+		__m256i yg = _mm256_set1_epi32(g);
+
+		unsigned k = 10 - logn;
+		__m256i yp = _mm256_set1_epi32(p);
+		__m256i yp0i = _mm256_set1_epi32(p0i);
+		size_t u = 0;
+		for (;;) {
+			size_t v = REV10[u << k];
+			_mm256_storeu_si256(
+				(__m256i *)(gm + v), ya);
+			u ++;
+			if (u >= (n >> 3)) {
+				break;
+			}
+			ya = mp_montymul_x8(ya, yg, yp, yp0i);
+		}
+	} else {
+		unsigned k = 10 - logn;
+		uint32_t x1 = mp_R(p);
+		size_t u = 0;
+		for (;;) {
+			size_t v = REV10[u << k];
+			gm[v] = x1;
+			u ++;
+			if (u >= n) {
+				break;
+			}
+			x1 = mp_montymul(x1, g, p, p0i);
+		}
+	}
+#else // NTRUGEN_AVX2
+	unsigned k = 10 - logn;
+	uint32_t x1 = mp_R(p);
+	size_t u = 0;
+	for (;;) {
+		size_t v = REV10[u << k];
+		gm[v] = x1;
+		u ++;
+		if (u >= n) {
+			break;
+		}
+		x1 = mp_montymul(x1, g, p, p0i);
+	}
+#endif // NTRUGEN_AVX2
+}
+
+/* see ng_inner.h */
+void
+mp_mkgm7(uint32_t *restrict gm, uint32_t g, uint32_t p, uint32_t p0i)
+{
+	size_t n = 128;
+	unsigned k = 3;
+	uint32_t x1 = mp_R(p);
+	for (size_t u = 0; u < n; u ++) {
+		size_t v = REV10[u << k];
+		gm[v] = x1;
+		x1 = mp_montymul(x1, g, p, p0i);
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+mp_mkigm(unsigned logn, uint32_t *restrict igm,
+	uint32_t ig, uint32_t p, uint32_t p0i)
+{
+	size_t n = (size_t)1 << logn;
+
+	for (unsigned j = logn; j < 10; j ++) {
+		ig = mp_montymul(ig, ig, p, p0i);
+	}
+
+#if NTRUGEN_AVX2
+	if (logn >= 3) {
+		uint32_t igx = ig;
+		for (unsigned j = 3; j < logn; j ++) {
+			igx = mp_montymul(igx, igx, p, p0i);
+		}
+		uint32_t yb0 = mp_hR(p);
+		uint32_t yb4 = mp_half(igx, p);
+		uint32_t yb2 = mp_montymul(yb4, igx, p, p0i);;
+		uint32_t yb6 = mp_montymul(yb2, igx, p, p0i);;
+		uint32_t yb1 = mp_montymul(yb6, igx, p, p0i);;
+		uint32_t yb5 = mp_montymul(yb1, igx, p, p0i);;
+		uint32_t yb3 = mp_montymul(yb5, igx, p, p0i);;
+		uint32_t yb7 = mp_montymul(yb3, igx, p, p0i);;
+		__m256i yb = _mm256_setr_epi32(
+			yb0, yb1, yb2, yb3, yb4, yb5, yb6, yb7);
+		__m256i yig = _mm256_set1_epi32(ig);
+
+		unsigned k = 10 - logn;
+		__m256i yp = _mm256_set1_epi32(p);
+		__m256i yp0i = _mm256_set1_epi32(p0i);
+		size_t u = 0;
+		for (;;) {
+			size_t v = REV10[u << k];
+			_mm256_storeu_si256(
+				(__m256i *)(igm + v), yb);
+			u ++;
+			if (u >= (n >> 3)) {
+				break;
+			}
+			yb = mp_montymul_x8(yb, yig, yp, yp0i);
+		}
+	} else {
+		unsigned k = 10 - logn;
+		uint32_t x2 = mp_hR(p);
+		size_t u = 0;
+		for (;;) {
+			size_t v = REV10[u << k];
+			igm[v] = x2;
+			u ++;
+			if (u >= n) {
+				break;
+			}
+			x2 = mp_montymul(x2, ig, p, p0i);
+		}
+	}
+#else // NTRUGEN_AVX2
+	unsigned k = 10 - logn;
+	uint32_t x2 = mp_hR(p);
+	size_t u = 0;
+	for (;;) {
+		size_t v = REV10[u << k];
+		igm[v] = x2;
+		u ++;
+		if (u >= n) {
+			break;
+		}
+		x2 = mp_montymul(x2, ig, p, p0i);
+	}
+#endif // NTRUGEN_AVX2
+}
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static inline __m256i
+mp_NTT8(__m256i ya, const uint32_t *restrict gm, size_t k,
+	__m256i yp, __m256i yp0i)
+{
+	__m256i yt1, yt2, ya0, ya1;
+
+	/* 0/4, 1/5, 2/6, 3/7 with gm[1] */
+	/* ya <- a0:a1:a4:a5:a2:a3:a6:a7 */
+	ya = _mm256_permute4x64_epi64(ya, 0xD8);
+	/* yt1 <- a0:a4:a1:a5:a2:a6:a3:a7 */
+	yt1 = _mm256_shuffle_epi32(ya, 0xD8);
+	/* yt2 <- a4:a0:a5:a1:a6:a2:a7:a3 */
+	yt2 = _mm256_shuffle_epi32(ya, 0x72);
+	/* yg0 <- g1:g1:g1:g1:g1:g1:g1:g1 */
+	__m256i yg0 = _mm256_set1_epi32(gm[k]);
+	yt2 = mp_montymul_x4(yt2, yg0, yp, yp0i);
+	ya0 = mp_add_x8(yt1, yt2, yp);
+	ya1 = mp_sub_x8(yt1, yt2, yp);
+
+	/* ya0 = a0:--:a1:--:a2:--:a3:--
+	   ya1 = a4:--:a5:--:a6:--:a7:-- */
+
+	/* 0/2, 1/3 with gm[2]; 4/6, 5/7 with gm[3] */
+	/* yt1 <- a0:--:a1:--:a4:--:a5:--
+	   yt2 <- a2:--:a3:--:a6:--:a7:-- */
+	yt1 = _mm256_permute2x128_si256(ya0, ya1, 0x20);
+	yt2 = _mm256_permute2x128_si256(ya0, ya1, 0x31);
+	__m256i yg1 = _mm256_setr_epi32(
+		gm[(k << 1) + 0], gm[(k << 1) + 0],
+		gm[(k << 1) + 0], gm[(k << 1) + 0],
+		gm[(k << 1) + 1], gm[(k << 1) + 1],
+		gm[(k << 1) + 1], gm[(k << 1) + 1]);
+	yt2 = mp_montymul_x4(yt2, yg1, yp, yp0i);
+	ya0 = mp_add_x8(yt1, yt2, yp);
+	ya1 = mp_sub_x8(yt1, yt2, yp);
+
+	/* ya0 = a0:--:a1:--:a4:--:a5:--
+	   ya1 = a2:--:a3:--:a6:--:a7:-- */
+
+	/* 0/1 with gm[4], 2/3 with gm[5], 4/5 with gm[6], 6/7 with gm[7] */
+	/* yt1 <- a0:--:a2:--:a4:--:a6:--
+	   yt2 <- a1:--:a3:--:a5:--:a7:-- */
+	yt1 = _mm256_unpacklo_epi64(ya0, ya1);
+	yt2 = _mm256_unpackhi_epi64(ya0, ya1);
+	__m256i yg2 = _mm256_setr_epi32(
+		gm[(k << 2) + 0], gm[(k << 2) + 0],
+		gm[(k << 2) + 1], gm[(k << 2) + 1],
+		gm[(k << 2) + 2], gm[(k << 2) + 2],
+		gm[(k << 2) + 3], gm[(k << 2) + 3]);
+	yt2 = mp_montymul_x4(yt2, yg2, yp, yp0i);
+	ya0 = mp_add_x8(yt1, yt2, yp);
+	ya1 = mp_sub_x8(yt1, yt2, yp);
+
+	/* ya0 = a0:--:a2:--:a4:--:a6:--
+	   ya1 = a1:--:a3:--:a5:--:a7:-- */
+	ya = _mm256_blend_epi32(ya0, _mm256_slli_epi64(ya1, 32), 0xAA);
+	return ya;
+}
+
+TARGET_AVX2
+static inline __m256i
+mp_iNTT8(__m256i ya, const uint32_t *restrict igm, size_t k,
+	__m256i yp, __m256i yp0i)
+{
+	__m256i yt1, yt2, ya0, ya1;
+
+	/* yt1 <- a0:--:a2:--:a4:--:a6:--
+	   yt2 <- a1:--:a3:--:a5:--:a7:-- */
+	yt1 = ya;
+	yt2 = _mm256_srli_epi64(ya, 32);
+	__m256i yg2 = _mm256_setr_epi32(
+		igm[(k << 2) + 0], igm[(k << 2) + 0],
+		igm[(k << 2) + 1], igm[(k << 2) + 1],
+		igm[(k << 2) + 2], igm[(k << 2) + 2],
+		igm[(k << 2) + 3], igm[(k << 2) + 3]);
+	ya0 = mp_half_x8(mp_add_x8(yt1, yt2, yp), yp);
+	ya1 = mp_montymul_x4(mp_sub_x8(yt1, yt2, yp), yg2, yp, yp0i);
+
+	/* yt1 <- a0:--:a1:--:a4:--:a5:--
+	   yt2 <- a2:--:a3:--:a6:--:a7:-- */
+	yt1 = _mm256_unpacklo_epi64(ya0, ya1);
+	yt2 = _mm256_unpackhi_epi64(ya0, ya1);
+	__m256i yg1 = _mm256_setr_epi32(
+		igm[(k << 1) + 0], igm[(k << 1) + 0],
+		igm[(k << 1) + 0], igm[(k << 1) + 0],
+		igm[(k << 1) + 1], igm[(k << 1) + 1],
+		igm[(k << 1) + 1], igm[(k << 1) + 1]);
+	ya0 = mp_half_x8(mp_add_x8(yt1, yt2, yp), yp);
+	ya1 = mp_montymul_x4(mp_sub_x8(yt1, yt2, yp), yg1, yp, yp0i);
+
+	/* yt1 <- a0:--:a1:--:a2:--:a3:--
+	   yt2 <- a4:--:a5:--:a6:--:a7:-- */
+	yt1 = _mm256_permute2x128_si256(ya0, ya1, 0x20);
+	yt2 = _mm256_permute2x128_si256(ya0, ya1, 0x31);
+	__m256i yg0 = _mm256_set1_epi32(igm[k]);
+	ya0 = mp_half_x8(mp_add_x8(yt1, yt2, yp), yp);
+	ya1 = mp_montymul_x4(mp_sub_x8(yt1, yt2, yp), yg0, yp, yp0i);
+
+	/* yt1 <- a0:--:a1:--:a4:--:a5:--
+	   yt2 <- a2:--:a3:--:a6:--:a7:-- */
+	yt1 = _mm256_permute2x128_si256(ya0, ya1, 0x20);
+	yt2 = _mm256_permute2x128_si256(ya0, ya1, 0x31);
+	/* yt1 <- a0:a2:a1:a3:a4:a6:a5:a7 */
+	yt1 = _mm256_blend_epi32(yt1, _mm256_slli_epi64(yt2, 32), 0xAA);
+	ya = _mm256_shuffle_epi32(yt1, 0xD8);
+	return ya;
+}
+#endif // NTRUGEN_AVX2
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+mp_NTT(unsigned logn, uint32_t *restrict a, const uint32_t *restrict gm,
+	uint32_t p, uint32_t p0i)
+{
+	if (logn == 0) {
+		return;
+	}
+#if NTRUGEN_AVX2
+	switch (logn) {
+		uint32_t x1, x2, s;
+		size_t t, n;
+		__m256i yp, yp0i;
+
+	case 1:
+		x1 = a[0];
+		x2 = mp_montymul(a[1], gm[1], p, p0i);
+		a[0] = mp_add(x1, x2, p);
+		a[1] = mp_sub(x1, x2, p);
+		return;
+	case 2:
+		s = gm[1];
+		x1 = a[0];
+		x2 = mp_montymul(a[2], s, p, p0i);
+		a[0] = mp_add(x1, x2, p);
+		a[2] = mp_sub(x1, x2, p);
+		x1 = a[1];
+		x2 = mp_montymul(a[3], s, p, p0i);
+		a[1] = mp_add(x1, x2, p);
+		a[3] = mp_sub(x1, x2, p);
+		x1 = a[0];
+		x2 = mp_montymul(a[1], gm[2], p, p0i);
+		a[0] = mp_add(x1, x2, p);
+		a[1] = mp_sub(x1, x2, p);
+		x1 = a[2];
+		x2 = mp_montymul(a[3], gm[3], p, p0i);
+		a[2] = mp_add(x1, x2, p);
+		a[3] = mp_sub(x1, x2, p);
+		return;
+	default:
+		yp = _mm256_set1_epi32(p);
+		yp0i = _mm256_set1_epi32(p0i);
+		n = (size_t)1 << logn;
+		t = n;
+		for (unsigned lm = 0; lm < (logn - 3); lm ++) {
+			size_t m = (size_t)1 << lm;
+			size_t ht = t >> 1;
+			size_t v0 = 0;
+			for (size_t u = 0; u < m; u ++) {
+				__m256i ys = _mm256_set1_epi32(gm[u + m]);
+				for (size_t v = 0; v < ht; v += 8) {
+					size_t k1 = v0 + v;
+					size_t k2 = k1 + ht;
+					__m256i *a1 = (__m256i *)(a + k1);
+					__m256i *a2 = (__m256i *)(a + k2);
+					__m256i y1 = _mm256_loadu_si256(a1);
+					__m256i y2 = _mm256_loadu_si256(a2);
+					y2 = mp_montymul_x8(y2, ys, yp, yp0i);
+					_mm256_storeu_si256(a1,
+						mp_add_x8(y1, y2, yp));
+					_mm256_storeu_si256(a2,
+						mp_sub_x8(y1, y2, yp));
+				}
+				v0 += t;
+			}
+			t = ht;
+		}
+		size_t m = n >> 3;
+		for (size_t u = 0; u < m; u ++) {
+			uint32_t *za = a + (u << 3);
+			__m256i ya = _mm256_loadu_si256((__m256i *)za);
+			ya = mp_NTT8(ya, gm, u + m, yp, yp0i);
+			_mm256_storeu_si256((__m256i *)za, ya);
+		}
+		return;
+	}
+#else // NTRUGEN_AVX2
+	size_t t = (size_t)1 << logn;
+	for (unsigned lm = 0; lm < logn; lm ++) {
+		size_t m = (size_t)1 << lm;
+		size_t ht = t >> 1;
+		size_t v0 = 0;
+		for (size_t u = 0; u < m; u ++) {
+			uint32_t s = gm[u + m];
+			for (size_t v = 0; v < ht; v ++) {
+				size_t k1 = v0 + v;
+				size_t k2 = k1 + ht;
+				uint32_t x1 = a[k1];
+				uint32_t x2 = mp_montymul(a[k2], s, p, p0i);
+				a[k1] = mp_add(x1, x2, p);
+				a[k2] = mp_sub(x1, x2, p);
+			}
+			v0 += t;
+		}
+		t = ht;
+	}
+#endif // NTRUGEN_AVX2
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+mp_iNTT(unsigned logn, uint32_t *restrict a, const uint32_t *restrict igm,
+	uint32_t p, uint32_t p0i)
+{
+	if (logn == 0) {
+		return;
+	}
+#if NTRUGEN_AVX2
+	switch (logn) {
+		uint32_t x1, x2, s;
+		size_t t, n;
+		__m256i yp, yp0i;
+
+	case 1:
+		x1 = a[0];
+		x2 = a[1];
+		a[0] = mp_half(mp_add(x1, x2, p), p);
+		a[1] = mp_montymul(mp_sub(x1, x2, p), igm[1], p, p0i);
+		return;
+	case 2:
+		x1 = a[0];
+		x2 = a[1];
+		a[0] = mp_half(mp_add(x1, x2, p), p);
+		a[1] = mp_montymul(mp_sub(x1, x2, p), igm[2], p, p0i);
+		x1 = a[2];
+		x2 = a[3];
+		a[2] = mp_half(mp_add(x1, x2, p), p);
+		a[3] = mp_montymul(mp_sub(x1, x2, p), igm[3], p, p0i);
+		s = igm[1];
+		x1 = a[0];
+		x2 = a[2];
+		a[0] = mp_half(mp_add(x1, x2, p), p);
+		a[2] = mp_montymul(mp_sub(x1, x2, p), s, p, p0i);
+		x1 = a[1];
+		x2 = a[3];
+		a[1] = mp_half(mp_add(x1, x2, p), p);
+		a[3] = mp_montymul(mp_sub(x1, x2, p), s, p, p0i);
+		return;
+	default:
+		yp = _mm256_set1_epi32(p);
+		yp0i = _mm256_set1_epi32(p0i);
+		n = (size_t)1 << logn;
+		size_t m = n >> 3;
+		for (size_t u = 0; u < m; u ++) {
+			uint32_t *za = a + (u << 3);
+			__m256i ya = _mm256_loadu_si256((__m256i *)za);
+			ya = mp_iNTT8(ya, igm, u + m, yp, yp0i);
+			_mm256_storeu_si256((__m256i *)za, ya);
+		}
+		t = 8;
+		for (unsigned lm = 3; lm < logn; lm ++) {
+			size_t hm = (size_t)1 << (logn - 1 - lm);
+			size_t dt = t << 1;
+			size_t v0 = 0;
+			for (size_t u = 0; u < hm; u ++) {
+				__m256i ys = _mm256_set1_epi32(igm[u + hm]);
+				for (size_t v = 0; v < t; v += 8) {
+					size_t k1 = v0 + v;
+					size_t k2 = k1 + t;
+					__m256i *a1 = (__m256i *)(a + k1);
+					__m256i *a2 = (__m256i *)(a + k2);
+					__m256i y1 = _mm256_loadu_si256(a1);
+					__m256i y2 = _mm256_loadu_si256(a2);
+					_mm256_storeu_si256(a1, mp_half_x8(
+						mp_add_x8(y1, y2, yp), yp));
+					_mm256_storeu_si256(a2, mp_montymul_x8(
+						mp_sub_x8(y1, y2, yp),
+						ys, yp, yp0i));
+				}
+				v0 += dt;
+			}
+			t = dt;
+		}
+		return;
+	}
+#else // NTRUGEN_AVX2
+	size_t t = 1;
+	for (unsigned lm = 0; lm < logn; lm ++) {
+		size_t hm = (size_t)1 << (logn - 1 - lm);
+		size_t dt = t << 1;
+		size_t v0 = 0;
+		for (size_t u = 0; u < hm; u ++) {
+			uint32_t s = igm[u + hm];
+			for (size_t v = 0; v < t; v ++) {
+				size_t k1 = v0 + v;
+				size_t k2 = k1 + t;
+				uint32_t x1 = a[k1];
+				uint32_t x2 = a[k2];
+				a[k1] = mp_half(mp_add(x1, x2, p), p);
+				a[k2] = mp_montymul(
+					mp_sub(x1, x2, p), s, p, p0i);
+			}
+			v0 += dt;
+		}
+		t = dt;
+	}
+#endif // NTRUGEN_AVX2
+}
+
+/* see ng_inner.h */
+const small_prime PRIMES[] = {
+    { 2147473409, 2042615807,  419348484, 1790111537,  786166065,      20478 },
+    { 2147389441, 1862176767, 1141604340,  677655126, 2024968256,  942807490 },
+    { 2147387393, 1472104447,  554514419,  563781659, 1438853699,  511282737 },
+    { 2147377153, 3690881023,  269819887,  978644358, 1971237828, 1936446844 },
+    { 2147358721, 3720222719,  153618407, 1882929796,  289507384,  264920030 },
+    { 2147352577, 2147352575,    3145700,  875644459, 1993867586, 1197387618 },
+    { 2147346433,  498984959,  154699745, 1268990641, 1559885885, 2112514368 },
+    { 2147338241, 2478688255,  826591197,  304701980,  207126964,  842573420 },
+    { 2147309569, 1908234239,  964657100, 1953449942,  309167499,   75092579 },
+    { 2147297281, 1774004223, 1503608772,  353848817, 1726802198, 2084007226 },
+    { 2147295233, 1006444543,  279363522,  632955619,  419515149,   38880066 },
+    { 2147239937, 2881243135, 1383813014,  710717957,  756383674,  706593520 },
+    { 2147235841,  867973119,  848351122, 1627458017, 1867341538, 1260600213 },
+    { 2147217409, 4277923839,  100122496, 1700895952, 1076153660,  370621817 },
+    { 2147205121, 1878769663, 1111621492,   44667394, 1556218865,   32248737 },
+    { 2147196929, 1543217151,  310009707, 1302747296, 1775932980, 1745280407 },
+    { 2147178497, 3518715903, 1006651223,  995615578, 1904695444,  546877831 },
+    { 2147100673, 1505372159,  520918771,   32821017,  801131396, 1402329446 },
+    { 2147082241, 4227457023,  385646296,  671813454,  349022278, 1235641740 },
+    { 2147074049, 1878638591, 1198259916,   22548867,  381743482,  316764378 },
+    { 2147051521,   96036863, 1908098729, 1744576193,  129309952, 1045517086 },
+    { 2147043329, 1538869247,  440645275,  848789527,  259769898,   74455690 },
+    { 2147039233, 2209953791, 2055370389, 1117428534, 1471147502,  119673623 },
+    { 2146988033, 1324904447, 1363381819, 2041738204,  478044407,  147722658 },
+    { 2146963457, 2130186239,  325123596, 2068278376,  875801492, 1306038870 },
+    { 2146959361, 2146959359,  264240644,  872271011,  507971579, 1990286186 },
+    { 2146938881, 1727508479, 1974074844,  198886428,  561977686, 1269843026 },
+    { 2146908161, 1672951807,   98813339, 1770745661, 1393783074, 1823212183 },
+    { 2146885633, 1006034943,  661929322, 1068953529,  641063610,  596476372 },
+    { 2146871297,  834054143, 1378823498, 1788896577,  959521530,  583621658 },
+    { 2146846721,  196495359, 1834738961,  827307343, 2002269931, 1831804644 },
+    { 2146834433, 1572214783,  655434995, 1825957452, 2005158872,   95719048 },
+    { 2146818049, 1505089535,  963224779, 1698458806, 1152339536, 1292562110 },
+    { 2146775041, 2532651007, 1260858461, 1897922218, 1980311657, 1048890741 },
+    { 2146756609, 1840572415, 1934326828, 1319016594,  303348307,  309425167 },
+    { 2146744321, 1001699327, 1031932938,  166920536,  125412453,  206010953 },
+    { 2146738177,  469016575, 1034034169, 2049530115,  240883589, 2109942428 },
+    { 2146736129, 1706334207,  386311155, 1983901927,  399193796, 1866844306 },
+    { 2146713601, 1878278143, 1917713332,  795783646, 2145730902,  666529419 },
+    { 2146695169, 3216242687,  923528062,  410549424,  826761640,  881662233 },
+    { 2146656257,  468934655, 1316739849,  819155176, 1775281069,  769010182 },
+    { 2146650113, 3149088767, 1357138678, 1776223808, 1272363648, 1075150578 },
+    { 2146646017,  598947839,  836424425,  861255062,  862632924, 1452282057 },
+    { 2146643969, 1458778111, 1699760867, 1128852454, 1250440960,  551264863 },
+    { 2146603009, 4008873983,  258845279, 1693343820, 1370713748,  896492399 },
+    { 2146572289,  498210815, 1553576441, 1357574155, 2042997788,  475091953 },
+    { 2146547713, 2343679999,  731429284,  874518801,  933977652, 2038082698 },
+    { 2146508801, 1005658111, 1986115866,  994764483, 1486111048,   77164992 },
+    { 2146492417, 3153125375, 2074458334, 1607832505, 1814950582, 1064849124 },
+    { 2146490369, 3383810047, 1163389142, 1860189331,  766301042, 1792895956 },
+    { 2146459649, 1542479871,  113653858, 1774578485,  436467466,  509354151 },
+    { 2146447361, 1995452415,  521816115,  984131366,  920695297, 1917290506 },
+    { 2146441217, 2108692479,  106094619, 1700403904,   55841604,  574542256 },
+    { 2146437121, 2142242815, 2143320076, 1575554072, 1350403963, 1873207947 },
+    { 2146430977, 2129653759,   84969459,  244551713,  118889817,  571880191 },
+    { 2146418689, 1877983231, 1167996867, 1775456464, 1211626453, 1434078655 },
+    { 2146406401, 1324322815, 1313757074, 1622299552,   60879896, 1023260741 },
+    { 2146404353, 1202685951, 1813342090, 1162222118,  193969494, 1214240996 },
+    { 2146379777, 3383699455, 1842644774,  237716594, 1996303667, 1647718365 },
+    { 2146363393, 1303308287, 1687447266, 1850889128,  545315431, 1827479741 },
+    { 2146355201,   61786111,  269635263,  691559024,  756508884, 1699936240 },
+    { 2146336769, 1072594943,  654341745, 1871443110,  107703592, 1164751303 },
+    { 2146312193, 4226686975, 1106990599,  230828908, 2118815775,  547343662 },
+    { 2146293761, 3652048895, 1401349558, 1527866384, 1939695340,  934813966 },
+    { 2146283521,  653111295,  594294152, 1585987504,  113384110,  901617467 },
+    { 2146203649,  128743423, 1230019607, 2070978911, 1602264262,  195465499 },
+    { 2146154497,  732674047, 1428919080,   29335864, 1593288655, 2129713526 },
+    { 2146142209, 2276165631, 1819536107,  649122195,  996268341,  667538093 },
+    { 2146127873, 2263568383, 2015437475,  207440004, 1904659130, 2030659201 },
+    { 2146099201, 1005248511,  907637264,  559331750,  152292400, 1381346740 },
+    { 2146093057, 1135265791,  453939696, 1717051717,  781899671,  630273220 },
+    { 2146091009, 4025139199, 1800630758, 1086157903, 1771609892, 1947078850 },
+    { 2146078721, 4008349695, 2001965478, 1399918158,  329087331, 2118472151 },
+    { 2146060289, 3416934399,  279720260, 1683079797, 1885781100,  720846962 },
+    { 2146048001, 2645170175, 1358862595,  455982466, 1271329796, 1757914587 },
+    { 2146041857, 2146041855, 1278996706,   70834413, 1910051359, 2005325121 },
+    { 2146019329, 1101637631, 1427296360,  223814614,  804128830, 1360420507 },
+    { 2145986561, 2846435327, 1292076979, 1571468773, 1576376806, 1922069675 },
+    { 2145976321, 2145976319, 2074996602,  755073721,  414238476,  200778967 },
+    { 2145964033, 3605581823,  689794868, 1452485437,  856502235,  416725139 },
+    { 2145906689, 2129129471,  921247209,  676947204,  654865993, 1812356432 },
+    { 2145875969, 3081205759, 1842525491, 1326145670,  309498173, 2087042424 },
+    { 2145871873, 2779211775,  962993434, 1019421319, 1501089909,  653721111 },
+    { 2145841153, 1592193023, 1869982816,  214096862, 1556972726,  503214451 },
+    { 2145832961,  384225279,  384678957,  669316727,  783241415, 1108169518 },
+    { 2145816577, 1860603903, 1173007304, 1845637035,  826920248,  854681726 },
+    { 2145785857, 1571166207,  669709063, 1939498551, 1558887263,  633181476 },
+    { 2145755137, 3689259007, 1290750531,  954377268,   91681545, 1222833471 },
+    { 2145742849, 4008013823,  223279603,  943919281,  916877791, 1449746232 },
+    { 2145728513, 1134901247, 1222351254, 2118240515, 1429489422,  863342952 },
+    { 2145699841, 2745485311, 1723896025, 1632678465, 1030096814, 1093745233 },
+    { 2145691649, 3517229055,  931453090,   54415538,  530452177, 1965948870 },
+    { 2145687553, 1705285631, 1809739911, 1581677400, 1990115608,  440621047 },
+    { 2145673217, 1541693439,  578267174, 1396659387, 1723941977, 1763556058 },
+    { 2145630209, 2879633407,  328648448,   40361658, 1539467270, 1674729976 },
+    { 2145595393, 1457729535,  255202829, 1400378563, 1431207189, 1703296854 },
+    { 2145587201,  518197247, 2028299732, 1486218552,  644522534, 1270684848 },
+    { 2145525761, 4225900543, 1358930970, 1145038363,  878769963, 1208930401 },
+    { 2145495041, 3248596991, 1641759544,  431141042, 1250759371,  401371792 },
+    { 2145466369,   60897279,    4913761, 1346487656, 1003126503, 1472826275 },
+    { 2145445889, 2913003519, 1950341575,  209604178, 2010474299,  190633762 },
+    { 2145390593, 2128613375, 2111959069, 1977319511,  253671817,  711951677 },
+    { 2145372161, 1939851263, 1176002444,  230466302, 1564380182,  318701388 },
+    { 2145361921, 1541382143,  879247163, 1458754908,  789794840, 1032907577 },
+    { 2145359873, 1436522495, 1349830443,  160850239,  842688712,  977726147 },
+    { 2145355777, 1201637375,  246501130, 1714048525, 1460503641,  167731422 },
+    { 2145343489,  295655423, 2034128553,   85533909,   89392725, 1282860004 },
+    { 2145325057, 2665418751, 1621650965, 1422485923,  288923379,  911614733 },
+    { 2145318913, 1872689151,  659138019,  507124624, 1410089651,  131619480 },
+    { 2145312769, 1004462079, 2144542130,  601028071,  309708965, 1694238751 },
+    { 2145300481, 3336482815, 1732525390, 1940872306, 2008521418,  313780675 },
+    { 2145282049, 4120799231, 1238750391, 1797682851,  557030104, 1782843353 },
+    { 2145232897,   27109375, 1795543839, 1037744642, 1557646622,  900304126 },
+    { 2145218561, 4007489535, 1315715750, 1070968735,  736559602, 1443800839 },
+    { 2145187841, 1335687167,   13023648, 1309044639, 1866831222,  399747323 },
+    { 2145181697, 3151814655, 1518459244, 1993483355,  691838010,  615546001 },
+    { 2145175553,  597477375, 1181348151,  703506449, 1514472518,  488565539 },
+    { 2145079297, 2262519807, 1805182441, 1948507193,  198785189,  638515383 },
+    { 2145021953, 2396680191, 1021023200,  896525833,  286005075, 1441021596 },
+    { 2145015809, 2174375935, 2117792680, 1601002661,  347118679, 1157904101 },
+    { 2145003521, 1503275007,  929358646, 1668692954,   37145528,  641759047 },
+    { 2144960513, 1071218687, 2014663077,  813829821,  819596313,   63874550 },
+    { 2144944129, 4023992319, 1528910090, 1405891381, 2038603849, 1893894911 },
+    { 2144935937, 1004085247, 1020776636, 1264145057,  131908564, 1538042171 },
+    { 2144894977, 1071153151,  117657395, 1658068046, 1863791040,  315971662 },
+    { 2144888833, 3583535103, 1035349752,  631245709,  221833619, 2112295623 },
+    { 2144880641, 3952625663, 2014869161, 1728523150, 1503761981, 2007376047 },
+    { 2144864257, 4288153599, 1298675209,  544582669, 1272986021, 1915075671 },
+    { 2144827393, 3080157183, 1123663006,  160045786,  473495242,  813254440 },
+    { 2144806913, 1234642943,  731069394, 2014053847, 1766552348,  857353667 },
+    { 2144796673, 2144796671, 1796197228,   54648661,  347997993, 2024676365 },
+    { 2144778241, 1536604159,  685152946, 1271425633,  321887990, 1348054165 },
+    { 2144759809,  248934399,  154114551, 1526637592, 1903326258, 1432928978 },
+    { 2144757761, 1972791295, 1931452899,  132970580, 1653105390, 1502698593 },
+    { 2144729089, 3751147519,  319799485,  856415206, 2139235838, 1528560673 },
+    { 2144727041, 1054207999,  456984744,  648538322, 1211291222, 1040592824 },
+    { 2144696321, 2543155199,  117528426,  396088388, 1351318511, 1705928562 },
+    { 2144667649,  798296063, 1911610943,  338234827, 1761076141, 1009985854 },
+    { 2144573441, 2509477887, 1537057360,  570563298,  647497611, 1089716523 },
+    { 2144555009, 2261995519, 1259090311, 1645617705,   28737134,  311642518 },
+    { 2144550913, 4023599103,  852574554,  787018986, 2112202798, 1495324375 },
+    { 2144536577, 1335035903, 1561905341, 2144070617, 1829236274, 1925884510 },
+    { 2144524289, 1771231231,  418502709, 1096979205, 1715236225,    4942642 },
+    { 2144512001, 1905436671,  486401965,  371227774,  621999585,  248381878 },
+    { 2144468993, 2144468991, 1685175757, 1750820831, 1117048683,  375676619 },
+    { 2144458753,  428988415,  481506649,  208627545, 1669824636,  994022373 },
+    { 2144421889, 3683731455,  111545270,  490157982,  598291463, 1506819637 },
+    { 2144409601, 1301354495, 1696205610, 1773221211, 1190821653,  703296234 },
+    { 2144370689, 1070628863,  410368360, 1852155417, 1241424587, 1916372058 },
+    { 2144348161, 2039490559, 2058332258,  380925864, 1763931741, 1080707148 },
+    { 2144335873, 2140141567,  189861841, 1779816687, 1155526432, 1839197694 },
+    { 2144329729, 2077220863,  782112649,  526684710, 1471877186,  752775607 },
+    { 2144327681, 2039470079, 1046835057, 1247119374, 1480573420, 1180445680 },
+    { 2144309249, 1322225663,  655091351, 1601388861,  546007835,  774279999 },
+    { 2144296961,  466575359,  478811653, 1358954307,  277575578, 1153736907 },
+    { 2144290817, 4220471295,  845002172, 1883197481,   20750133,  894854412 },
+    { 2144243713, 1859031039,  134828934,   37546773, 2020974628, 1786100430 },
+    { 2144237569,  663648255,  981840700,  819447626,  828884995, 1801721724 },
+    { 2144161793, 1187860479, 1312055195,  492584100,  284208050,  991575551 },
+    { 2144155649, 3280812031, 1909512015, 1038118142,  557855457,  351830619 },
+    { 2144137217,  516747263, 1231150697, 1145329724, 1222551687, 1031682140 },
+    { 2144120833, 2261561343, 1725803932,   85962159, 1987475484, 1541754604 },
+    { 2144071681, 4274778111, 1126468398,  956241311, 2057260329, 1928592240 },
+    { 2144065537,  965466111, 1878788832, 1053853819,  587985232,  409128944 },
+    { 2144028673,  998983679, 2034483463,   85221004,  692082439, 2038010847 },
+    { 2144010241, 2144010239,  842246168,  579253174, 1365083779,  893394979 },
+    { 2143952897, 4224327679,  754017578,  344456872,  574149142,  951344542 },
+    { 2143940609, 3200905215, 1412359304,  691093159, 1240091757,  209561083 },
+    { 2143918081, 1971951615, 1479162717, 1879554798, 1607927459, 1573571606 },
+    { 2143899649, 3335081983, 1640239719, 1873544500,  512920036,  980684517 },
+    { 2143891457,  382283775, 1157894649, 1152913815, 2082762113,  174332902 },
+    { 2143885313, 3448313855,   77697446, 1272010615,  639087423,  788921708 },
+    { 2143854593,  466132991, 1365712904,  234096830, 1624926716,  267754392 },
+    { 2143836161, 3783809023,  200636173,  802456521,  972626486, 1579347589 },
+    { 2143762433, 1669806079, 1374323479,  122119316,  523389252,  528105857 },
+    { 2143756289, 3150389247,  226179777, 2014672070,  977903451,  205998472 },
+    { 2143713281, 2810607615,  674480231,  499335673, 1923111979,  170087335 },
+    { 2143690753, 4224065535, 1119041321,  293184621,  548955895, 1709245285 },
+    { 2143670273, 3066417151,  966374918, 1855603277, 1939385627, 1369708529 },
+    { 2143666177, 1875230719, 1768658380, 1592043954,  626312185, 1973259130 },
+    { 2143645697, 4005916671, 1369531559, 1496045094,  868812782,  743685730 },
+    { 2143641601, 2613403647,  836342892,  137205166, 1106494107, 1904307099 },
+    { 2143635457, 2609203199, 1360942100,  611274460, 1430230424,  634419595 },
+    { 2143621121, 3737456639,  190915397,  456284400, 1315071924, 1321642891 },
+    { 2143598593,  998553599,  155040255, 1748188596, 1338143543, 1507236419 },
+    { 2143567873, 1875132415, 1411420224,  961363405,  100809682,    4006593 },
+    { 2143561729,  964962303, 1285824486, 1745824462,   52145752, 1898334618 },
+    { 2143553537, 1065617407, 1589845870,  642609604,  876094006, 1160521531 },
+    { 2143541249, 3112425471,  912656057,  832462820, 1196335625,   50513987 },
+    { 2143531009, 3871584255,  917220898,  733999433, 1060086084,  957280110 },
+    { 2143522817, 3468922879, 1098374569, 2134640637, 1812047677,  957540519 },
+    { 2143506433, 2260946943,  933735606,  453429574, 1461625836, 1505133897 },
+    { 2143488001, 1333987327, 1985268644, 2074073649, 1479394204, 1064535019 },
+    { 2143469569, 4022517759, 1477872272, 1513292205, 1862896587, 1004190392 },
+    { 2143426561,  495065087,  186028039, 1375811092,  173677829, 2089274222 },
+    { 2143383553, 1858170879,  887211384, 2134877926, 1608809321, 1661543915 },
+    { 2143377409, 2978043903, 1281348890, 1288031453,  483115862, 1970346133 },
+    { 2143363073, 1002512383, 1950991422, 1122143235, 1166724603, 1788468869 },
+    { 2143260673, 1321177087, 2069683714, 1805244102, 1261092720,  903254409 },
+    { 2143246337,  293558271, 1176536351, 1950051640,  309589975, 1529083832 },
+    { 2143209473,   58640383, 1560740565,  218139536, 1255708568, 1698136215 },
+    { 2143203329, 3334385663, 1971538547, 1999593400, 1859827267, 1956345089 },
+    { 2143160321, 2675836927,  477597631,  884313283, 2113130436, 1457071330 },
+    { 2143129601, 2394787839,  546440653, 1295503643, 2080414454, 1206105984 },
+    { 2143123457,  394098687,  612601193, 1693130333,   43008062,   16697358 },
+    { 2143100929, 1002250239,  591926265, 1101885974,  315537419, 1388418046 },
+    { 2143092737, 2143092735, 1205498739, 1071117401,  276869279, 1816776329 },
+    { 2143082497, 2306660351, 1658985163, 2126748415, 1783080325, 1909268843 },
+    { 2143062017, 2004649983,  806738297, 1026831688, 2035800522,  754902198 },
+    { 2143051777, 1539071999, 1644097744, 1553535853, 2052185358, 1958510946 },
+    { 2143025153, 1065089023,   48408341, 1525464941, 1852830390,  755368639 },
+    { 2143006721, 3871059967, 1620020706, 2138083553,   30951415, 1336294009 },
+    { 2142996481, 2273019903,  576994614,    1383533, 1219848252, 1992320100 },
+    { 2142976001, 2742761471, 1018092510, 1458681506,  608641646,  190719147 },
+    { 2142965761,  515575807,  359250737, 1354388786, 2131235811, 1323949132 },
+    { 2142916609,  649744383, 1208571887, 1878268755, 1560639011, 1431405645 },
+    { 2142892033, 3199856639,  340166218,  771047236, 1294387139, 1350670764 },
+    { 2142885889, 2574899199, 1416994273,  869163275, 1050539694,  503308105 },
+    { 2142871553, 3686375423,  823272682, 1330177723, 1618603945,  680882765 },
+    { 2142861313, 3615062015,  185662521,   59302870, 1086768468,  733960061 },
+    { 2142830593, 2142830591, 1184916005, 1079082138,  801265712, 1392890675 },
+    { 2142803969, 2776143871, 1898723413,  612877597,  748398044,  719565838 },
+    { 2142785537, 1723355135, 1938177810,  901717203, 1921085502, 1027865724 },
+    { 2142779393, 4084742143, 1843695270,  956271994, 1027905184,  437886242 },
+    { 2142724097,  465002495, 1786272468, 2057509300,  596569659,  833374098 },
+    { 2142707713, 3149340671, 1726077360,  110521970,  490838647,  570919333 },
+    { 2142658561, 3686162431, 1631852094,  261726828,  931917077, 2005364883 },
+    { 2142638081, 3199602687, 1965839564,  408833423, 1753224304, 1152041797 },
+    { 2142564353,  515174399, 1080848266, 1517077457, 1548531958,  823650319 },
+    { 2142533633, 2272557055,  392959315,  729816849,  921579743,  129344001 },
+    { 2142529537,  359950335,   17286407, 1033141274, 1935370361,  921394177 },
+    { 2142527489, 1538547711, 2022542562,  591503450,  522147359, 1308604835 },
+    { 2142502913, 2142502911,  862145305, 1943481144,  733213664,  364106514 },
+    { 2142498817, 4273205247, 1497774797,  362230737, 2058280749, 1402886894 },
+    { 2142416897, 1186115583, 1820910794,  869347984, 1125890573,  823863638 },
+    { 2142363649, 4222738431, 1712603348,  839133619,  770847044,  892019817 },
+    { 2142351361, 2796662783,  639467496,  472998882,  259948593, 1009074045 },
+    { 2142330881, 2612092927,  119930462,  898272391, 1611927337, 1395844887 },
+    { 2142314497, 1001463807,  417987874,  952188691, 1472816610, 1525176551 },
+    { 2142289921, 4021338111,  626441030, 1087932551, 1833827076, 1287261065 },
+    { 2142283777,  292595711, 1437184719, 1990366532,  718034518, 2141169066 },
+    { 2142277633,  783323135,  409102935, 1109091844, 1748524698, 1404314290 },
+    { 2142263297, 1634752511, 1332648256, 1458675808, 1235027887,  862728790 },
+    { 2142208001, 1068466175, 1397470467, 1098539573, 1097351447, 1965727963 },
+    { 2142164993,  695130111, 1066940847,    3394841,  337813155, 2063673696 },
+    { 2142097409, 3064844287,  297225318,  190118328,  120193342, 1425544062 },
+    { 2142087169,   23963647, 1242139544,  508378060, 2119898775, 1644552134 },
+    { 2142078977, 1735231487,   34593522,  662801663, 1837465297, 1388457155 },
+    { 2142074881,  393050111, 1775241888,  546076620,  753976398, 1286409850 },
+    { 2142044161, 2142044159, 1993353265,   20421055, 1749390716,  453736187 },
+    { 2142025729, 3144464383, 1054015161,  216592874,  174350842, 2000851460 },
+    { 2142011393, 1068269567, 1022043540,  154322400, 1364900586, 1216790673 },
+    { 2141974529, 4272680959,  878244511,  450223676,  840019780,  936239114 },
+    { 2141943809, 4151015423,  181002276, 1766661673,  824377997,  898611110 },
+    { 2141933569, 2259374079,  921001808,  138101311, 1597307312, 1683735850 },
+    { 2141931521,  996886527, 2026958630,  786548950,  491009818, 1487647832 },
+    { 2141902849, 3916093439, 1774507217, 1503936548, 2011562721,  803375820 },
+    { 2141890561, 4050298879, 1242184656, 1606622264,  973689178, 1298783383 },
+    { 2141857793, 1500129279,   47495456, 1818547837, 1933124842, 1327760978 },
+    { 2141833217, 3546925055,  534546202,  500926182, 2118308644, 1496583315 },
+    { 2141820929, 1969854463,  457737750,  690473287, 1631943651,  664359263 },
+    { 2141786113, 1588137983, 1837204275, 2031397019, 1967667817,  905298889 },
+    { 2141771777, 1231607807, 1585138177,   54300848,  314627109,  927112984 },
+    { 2141759489, 2439555071, 1154859258, 1383763879, 1971680935, 2108097075 },
+    { 2141749249, 1068007423, 2080935967,  859296785, 1197038946, 1880678306 },
+    { 2141685761, 3345451007,  517526213, 1550564945, 1632439809,  954282998 },
+    { 2141673473, 2439469055,   21649850, 2046161117,  552392242,  103443123 },
+    { 2141669377, 2070366207,  126251361, 1742664644, 1482481206,  605966977 },
+    { 2141655041, 2661748735,  484235305, 1528710429,  354771509,  593774164 },
+    { 2141587457,  526780415,  574888543,  955760611,  687141330, 1030240293 },
+    { 2141583361, 3748001791, 1371847173, 1144908715,  184363050, 1463331359 },
+    { 2141575169, 1499846655, 1229054288, 1538452901,  324225902, 1425521960 },
+    { 2141546497, 1164273663, 1768171221,  430711736, 1956344257,  358993721 },
+    { 2141515777,  514125823, 1883057193, 1188334395,  254771003,  712002260 },
+    { 2141495297,  463773695, 1894308447, 2130610251,   69814245, 2121489732 },
+    { 2141483009, 3466883071,  950896971, 1258381554,  825951706,  195111043 },
+    { 2141458433, 4272164863,  566843170,   64378978, 1469333580, 1466451638 },
+    { 2141360129, 4003631103,  510897768, 2054940459, 1603364306, 1896684552 },
+    { 2141325313, 1600260095,  309414728,  783698702,  845623609, 2118606683 },
+    { 2141317121, 3714181119,   41910923, 1219136678, 1953984968, 1546372147 },
+    { 2141286401, 1856073727, 1171249093,  986447182, 1530161498, 1359644991 },
+    { 2141267969,  694233087, 1211133465,  844862374, 1144145329,  372321440 },
+    { 2141255681, 2405496831, 1328934139,  735916721, 1296250995, 1612100237 },
+    { 2141243393, 3814770687,  520479708,  525351865, 2026115355,  538775863 },
+    { 2141214721, 1633703935,  504183101,  663335303,  715380266,   34424233 },
+    { 2141212673, 1721782271,  603205901, 1433302473, 1227056696, 1431443425 },
+    { 2141202433, 2036344831, 1604585501, 1229299775,  752118923, 1881364338 },
+    { 2141175809, 1872740351, 1733516714, 1807146636,  363795364, 1986179172 },
+    { 2141165569, 1432328191, 1490189496, 1320489732, 1641816108,  578038958 },
+    { 2141073409,  916336639,  873759781, 1556749288,  651084845, 1015729028 },
+    { 2141052929, 4221427711,  979186233, 1762104735,  715530680,  622504930 },
+    { 2141040641,  647868415,  949895441, 2025808022,  160106133,  561883724 },
+    { 2141028353, 1067286527, 2135838697,  916257035, 1402241650,  750070220 },
+    { 2141011969, 4020060159,  611765852, 1170281544, 1552266705,  329393240 },
+    { 2140999681, 3734835199,  351284530,  998193986, 2031434746, 1932350134 },
+    { 2140997633, 2942109695, 1853355265, 1185563841,  237281087,  440781243 },
+    { 2140993537, 1331492863,  676775069,  495308340, 1540448795,  750732506 },
+    { 2140942337, 4137431039, 1645148093, 2102068815, 2126166725, 1570735656 },
+    { 2140925953, 4271632383,  759158315,  871789395,  712612398, 1673104634 },
+    { 2140917761, 4137406463, 1126409570,  487401697,  790501062, 2003679138 },
+    { 2140887041, 3512424447,  356555373,  692136311, 1873117117, 2056578551 },
+    { 2140837889, 1163565055,  367306155,  549695576, 1539499132, 1848189620 },
+    { 2140788737, 2572802047,  558045408, 1566269988, 1578573646, 1949949243 },
+    { 2140766209, 1067024383, 1525495467,  488941581, 1332090451, 1459824799 },
+    { 2140764161, 3612964863,  453729911, 1063914887,  792300614, 2072819909 },
+    { 2140696577, 1318612991, 1154564043, 1249648119, 1281812338, 1631278485 },
+    { 2140684289, 1872248831, 2118938259,  262551938, 1786968202,   55549365 },
+    { 2140653569, 4082616319, 1285628803,   80889460,  220865202, 1452802925 },
+    { 2140594177,  999743487, 1382704526, 1592957652,  139669975,  754621876 },
+    { 2140579841,  827762687, 1601414172,    1043280, 1283241479,  868432049 },
+    { 2140569601, 1066827775, 1241785107, 1615837280, 1527749306,  716582032 },
+    { 2140567553, 2807461887, 1271173854,  308478868, 1555967545, 1677706733 },
+    { 2140557313, 2794868735, 1924690389,  870598748,   61433129,  210103227 },
+    { 2140549121,  915812351,  914841856, 1101232091, 1853042589, 1050014014 },
+    { 2140477441, 2371164159, 1750444466, 1671883855, 1913885020, 2062319628 },
+    { 2140469249, 3478452223, 1728225499,  134343014, 1041454719, 2059902640 },
+    { 2140426241, 3868479487, 1380594798, 1812066246, 2026797853, 1212120681 },
+    { 2140420097, 3008641023,  100654027,  100993678, 1430665666, 1692048952 },
+    { 2140413953, 2073305087, 1265120041,  624789355, 1980101954, 1492294345 },
+    { 2140383233,  559130623,  945242106,  782288940,  181435266,  453459249 },
+    { 2140366849, 3847448575, 1028300358,   75248903,  312878281,  294392408 },
+    { 2140354561,  592656383,  368901374, 2124984708, 1584238035,  557749653 },
+    { 2140348417, 3146981375,  495209562, 1555795152, 1821951283, 1580622851 },
+    { 0, 0, 0, 0, 0, 0 }
+};
diff --git a/lib/dns/hawk/ng_ntru.c b/lib/dns/hawk/ng_ntru.c
new file mode 100644
index 0000000000..6fbf6df9e4
--- /dev/null
+++ b/lib/dns/hawk/ng_ntru.c
@@ -0,0 +1,1555 @@
+#include "ng_inner.h"
+
+/*
+ * Convert source f and g into RNS+NTT, at the start of the provided tmp[]
+ * (one word per coefficient).
+ *
+ * RAM USAGE: 3*(2^logn_top)
+ */
+static void
+make_fg_zero(unsigned logn,
+	const int8_t *restrict f, const int8_t *restrict g,
+	uint32_t *restrict tmp)
+{
+	size_t n = (size_t)1 << logn;
+	uint32_t *ft = tmp;
+	uint32_t *gt = ft + n;
+	uint32_t *gm = gt + n;
+	uint32_t p = PRIMES[0].p;
+	uint32_t p0i = PRIMES[0].p0i;
+	poly_mp_set_small(logn, ft, f, p);
+	poly_mp_set_small(logn, gt, g, p);
+	mp_mkgm(logn, gm, PRIMES[0].g, p, p0i);
+	mp_NTT(logn, ft, gm, p, p0i);
+	mp_NTT(logn, gt, gm, p, p0i);
+}
+
+/*
+ * One step of computing (f,g) at a given depth.
+ * Input: (f,g) of degree 2^(logn_top-depth)
+ * Output: (f',g') of degree 2^(logn_top-(depth+1))
+ * Input and output values are at the start of tmp[], in RNS+NTT notation.
+ *
+ * RAM USAGE: 3*(2^logn_top) (at most)
+ * (assumptions: max_bl_small[0] = max_bl_small[1] = 1, max_bl_small[2] = 2)
+ */
+TARGET_AVX2
+static void
+make_fg_step(const ntru_profile *prof,
+	unsigned logn_top, unsigned depth, uint32_t *tmp)
+{
+	unsigned logn = logn_top - depth;
+	size_t n = (size_t)1 << logn;
+	size_t hn = n >> 1;
+	size_t slen = prof->max_bl_small[depth];
+	size_t tlen = prof->max_bl_small[depth + 1];
+
+	/*
+	 * Layout:
+	 *   fd    output f' (hn*tlen)
+	 *   gd    output g' (hn*tlen)
+	 *   fs    source (n*slen)
+	 *   gs    source (n*slen)
+	 *   t1    NTT support (n)
+	 *   t2    extra (max(n, slen - n))
+	 */
+	uint32_t *fd = tmp;
+	uint32_t *gd = fd + hn * tlen;
+	uint32_t *fs = gd + hn * tlen;
+	uint32_t *gs = fs + n * slen;
+	uint32_t *t1 = gs + n * slen;
+	uint32_t *t2 = t1 + n;
+	memmove(fs, tmp, 2 * n * slen * sizeof *tmp);
+
+	/*
+	 * First slen words: we use the input values directly, and apply
+	 * inverse NTT as we go, so that we get the sources in RNS (non-NTT).
+	 */
+	uint32_t *xf = fs;
+	uint32_t *xg = gs;
+	uint32_t *yf = fd;
+	uint32_t *yg = gd;
+	for (size_t u = 0; u < slen; u ++) {
+		uint32_t p = PRIMES[u].p;
+		uint32_t p0i = PRIMES[u].p0i;
+		uint32_t R2 = PRIMES[u].R2;
+		for (size_t v = 0; v < hn; v ++) {
+			yf[v] = mp_montymul(
+				mp_montymul(xf[2 * v], xf[2 * v + 1], p, p0i),
+				R2, p, p0i);
+			yg[v] = mp_montymul(
+				mp_montymul(xg[2 * v], xg[2 * v + 1], p, p0i),
+				R2, p, p0i);
+		}
+		mp_mkigm(logn, t1, PRIMES[u].ig, p, p0i);
+		mp_iNTT(logn, xf, t1, p, p0i);
+		mp_iNTT(logn, xg, t1, p, p0i);
+		xf += n;
+		xg += n;
+		yf += hn;
+		yg += hn;
+	}
+
+	/*
+	 * Now that fs and gs are in RNS, rebuild their plain integer
+	 * coefficients.
+	 */
+	zint_rebuild_CRT(fs, slen, n, 2, 1, t1);
+
+	/*
+	 * Remaining output words.
+	 */
+	for (size_t u = slen; u < tlen; u ++) {
+		uint32_t p = PRIMES[u].p;
+		uint32_t p0i = PRIMES[u].p0i;
+		uint32_t R2 = PRIMES[u].R2;
+		uint32_t Rx = mp_Rx31(slen, p, p0i, R2);
+		mp_mkgm(logn, t1, PRIMES[u].g, p, p0i);
+#if NTRUGEN_AVX2
+		if (logn >= 3) {
+			__m256i yp = _mm256_set1_epi32(p);
+			__m256i yp0i = _mm256_set1_epi32(p0i);
+			__m256i yR2 = _mm256_set1_epi32(R2);
+			__m256i yRx = _mm256_set1_epi32(Rx);
+			for (size_t v = 0; v < n; v += 8) {
+				__m256i yt = zint_mod_small_signed_x8(
+					fs + v, slen, n, yp, yp0i, yR2, yRx);
+				_mm256_storeu_si256((__m256i *)(t2 + v), yt);
+			}
+			mp_NTT(logn, t2, t1, p, p0i);
+			for (size_t v = 0; v < hn; v += 4) {
+				__m256i yt = _mm256_loadu_si256(
+					(__m256i *)(t2 + (2 * v)));
+				yt = mp_montymul_x4(yt,
+					_mm256_srli_epi64(yt, 32), yp, yp0i);
+				yt = mp_montymul_x4(yt, yR2, yp, yp0i);
+				yt = _mm256_shuffle_epi32(yt, 0xD8);
+				yt = _mm256_permute4x64_epi64(yt, 0xD8);
+				_mm_storeu_si128((__m128i *)(yf + v),
+					_mm256_castsi256_si128(yt));
+			}
+			yf += hn;
+			for (size_t v = 0; v < n; v += 8) {
+				__m256i yt = zint_mod_small_signed_x8(
+					gs + v, slen, n, yp, yp0i, yR2, yRx);
+				_mm256_storeu_si256((__m256i *)(t2 + v), yt);
+			}
+			mp_NTT(logn, t2, t1, p, p0i);
+			for (size_t v = 0; v < hn; v += 4) {
+				__m256i yt = _mm256_loadu_si256(
+					(__m256i *)(t2 + (2 * v)));
+				yt = mp_montymul_x4(yt,
+					_mm256_srli_epi64(yt, 32), yp, yp0i);
+				yt = mp_montymul_x4(yt, yR2, yp, yp0i);
+				yt = _mm256_shuffle_epi32(yt, 0xD8);
+				yt = _mm256_permute4x64_epi64(yt, 0xD8);
+				_mm_storeu_si128((__m128i *)(yg + v),
+					_mm256_castsi256_si128(yt));
+			}
+			yg += hn;
+			continue;
+		}
+#endif // NTRUGEN_AVX2
+		for (size_t v = 0; v < n; v ++) {
+			t2[v] = zint_mod_small_signed(
+				fs + v, slen, n, p, p0i, R2, Rx);
+		}
+		mp_NTT(logn, t2, t1, p, p0i);
+		for (size_t v = 0; v < hn; v ++) {
+			yf[v] = mp_montymul(
+				mp_montymul(t2[2 * v], t2[2 * v + 1], p, p0i),
+				R2, p, p0i);
+		}
+		yf += hn;
+		for (size_t v = 0; v < n; v ++) {
+			t2[v] = zint_mod_small_signed(
+				gs + v, slen, n, p, p0i, R2, Rx);
+		}
+		mp_NTT(logn, t2, t1, p, p0i);
+		for (size_t v = 0; v < hn; v ++) {
+			yg[v] = mp_montymul(
+				mp_montymul(t2[2 * v], t2[2 * v + 1], p, p0i),
+				R2, p, p0i);
+		}
+		yg += hn;
+	}
+}
+
+/*
+ * Compute (f,g) at a specified depth, in RNS+NTT notation.
+ * Computed values are stored at the start of the provided tmp[] (slen
+ * words per coefficient).
+ *
+ * This function is for depth < logn_top. For the deepest layer, use
+ * make_fg_deepest().
+ *
+ * RAM USAGE: 3*(2^logn_top)
+ */
+static void
+make_fg_intermediate(const ntru_profile *prof,
+	unsigned logn_top,
+	const int8_t *restrict f, const int8_t *restrict g,
+	unsigned depth, uint32_t *tmp)
+{
+	make_fg_zero(logn_top, f, g, tmp);
+	for (unsigned d = 0; d < depth; d ++) {
+		make_fg_step(prof, logn_top, d, tmp);
+	}
+}
+
+/*
+ * Compute (f,g) at the deepest level (i.e. get Res(f,X^n+1) and
+ * Res(g,X^n+1)). Intermediate (f,g) values (below the save threshold)
+ * are copied at the end of tmp (of size save_off words).
+ *
+ * If f is not invertible modulo X^n+1 and modulo p = 2147473409, then
+ * this function returns 0 (the resultants and intermediate values are
+ * still computed). Otherwise it returns 1.
+ */
+static int
+make_fg_deepest(const ntru_profile *prof,
+	unsigned logn_top,
+	const int8_t *restrict f, const int8_t *restrict g,
+	uint32_t *tmp, size_t sav_off)
+{
+	make_fg_zero(logn_top, f, g, tmp);
+	int r = 1;
+
+	/*
+	 * f is now in RNS+NTT, so we can test its invertibility (mod p)
+	 * by simply checking that all NTT coefficients are non-zero.
+	 * (If f is invertible then recover_G() always works.)
+	 */
+	size_t n = (size_t)1 << logn_top;
+	uint32_t b = 0;
+	for (size_t u = 0; u < n; u ++) {
+		b |= tmp[u] - 1;
+	}
+	r = (int)(1 - (b >> 31));
+
+	for (unsigned d = 0; d < logn_top; d ++) {
+		make_fg_step(prof, logn_top, d, tmp);
+
+		/*
+		 * make_fg_step() computes the (f,g) for depth d+1; we
+		 * save that value if d+1 is at least at the save
+		 * threshold, but is not the deepest level.
+		 */
+		unsigned d2 = d + 1;
+		if (d2 < logn_top && d2 >= prof->min_save_fg[logn_top]) {
+			size_t slen = prof->max_bl_small[d2];
+			size_t fglen = slen << (logn_top + 1 - d2);
+			sav_off -= fglen;
+			memmove(tmp + sav_off, tmp, fglen * sizeof *tmp);
+		}
+	}
+	return r;
+}
+
+/* Error code: no error (so far) */
+#define SOLVE_OK           0
+
+/* Error code: GCD(Res(f,X^n+1), Res(g,X^n+1)) != 1 */
+#define SOLVE_ERR_GCD      -1
+
+/* Error code: reduction error (NTRU equation no longer fulfilled) */
+#define SOLVE_ERR_REDUCE   -2
+
+/* Error code: output (F,G) coefficients are off-limits */
+#define SOLVE_ERR_LIMIT    -3
+
+/*
+ * Solve the NTRU equation at the deepest level. This computes the
+ * integers F and G such that Res(f,X^n+1)*G - Res(g,X^n+1)*F = q.
+ * The two integers are written into tmp[].
+ *
+ * Returned value: 0 on success, a negative error code otherwise.
+ *
+ * RAM USAGE: max(3*(2^logn_top), 8*max_bl_small[depth])
+ */
+static int
+solve_NTRU_deepest(const ntru_profile *prof,
+	unsigned logn_top, const int8_t *restrict f, const int8_t *restrict g,
+	uint32_t *tmp)
+{
+	/*
+	 * Get (f,g) at the deepest level (i.e. Res(f,X^n+1) and Res(g,X^n+1)).
+	 * Obtained (f,g) are in RNS+NTT (since degree n = 1, this is
+	 * equivalent to RNS).
+	 */
+	if (!make_fg_deepest(prof, logn_top, f, g,
+		tmp, (size_t)6 << logn_top))
+	{
+		return SOLVE_ERR_GCD;
+	}
+	/* obsolete
+	kg_stats_set_max_size(logn_top, logn_top, 3 * ((size_t)1 << logn_top));
+	*/
+
+	/*
+	 * Reorganize memory:
+	 *    Fp   output F (len)
+	 *    Gp   output G (len)
+	 *    fp   Res(f,X^n+1) (len)
+	 *    gp   Res(g,X^n+1) (len)
+	 *    t1   rest of temporary
+	 */
+	size_t len = prof->max_bl_small[logn_top];
+	uint32_t *Fp = tmp;
+	uint32_t *Gp = Fp + len;
+	uint32_t *fp = Gp + len;
+	uint32_t *gp = fp + len;
+	uint32_t *t1 = gp + len;
+	memmove(fp, tmp, 2 * len * sizeof *tmp);
+	/* obsolete
+	kg_stats_set_max_size(logn_top, logn_top, 8 * len);
+	*/
+
+	/*
+	 * Convert back the resultants into plain integers.
+	 */
+	zint_rebuild_CRT(fp, len, 1, 2, 0, t1);
+
+	/*
+	 * Apply the binary GCD to get a solution (F,G) such that
+	 * f*G - g*F = 1.
+	 */
+	if (!zint_bezout(Gp, Fp, fp, gp, len, t1)) {
+		return SOLVE_ERR_GCD;
+	}
+
+	/*
+	 * Multiply the obtained (F,G) by q to get a proper solution
+	 * f*G - g*F = q.
+	 */
+	if (prof->q != 1) {
+		if (zint_mul_small(Fp, len, prof->q) != 0
+			|| zint_mul_small(Gp, len, prof->q) != 0)
+		{
+			return SOLVE_ERR_REDUCE;
+		}
+	}
+
+	return SOLVE_OK;
+}
+
+/*
+ * We use poly_sub_scaled() when log(n) < MIN_LOGN_FGNTT, and
+ * poly_sub_scaled_ntt() when log(n) >= MIN_LOGN_FGNTT. The NTT variant
+ * is faster at large degrees, but not at small degrees.
+ */
+#define MIN_LOGN_FGNTT   4
+#if NTRUGEN_AVX2
+/*
+ * The AVX2 implementation requires MIN_LOGN_FGNTT >= 3
+ */
+#if MIN_LOGN_FGNTT < 3
+#error Incorrect MIN_LOGN_FGNTT value
+#endif
+#endif // NTRUGEN_AVX2
+
+/*
+ * Solving the NTRU equation, intermediate level.
+ * Input is (F,G) from one level deeper (half-degree), in plain
+ * representation, at the start of tmp[]; output is (F,G) from this
+ * level, written at the start of tmp[].
+ *
+ * Returned value: 0 on success, a negative error code otherwise.
+ */
+TARGET_AVX2
+static int
+solve_NTRU_intermediate(const ntru_profile *restrict prof,
+	unsigned logn_top,
+	const int8_t *restrict f, const int8_t *restrict g,
+	unsigned depth, uint32_t *restrict tmp)
+{
+	/*
+	 * MAX SIZE:
+	 *    input: 2 * hn * max_bl_small[depth + 1]
+	 */
+
+	unsigned logn = logn_top - depth;
+	size_t n = (size_t)1 << logn;
+	size_t hn = n >> 1;
+
+	/*
+	 * slen   size for (f,g) at this level (also output (F,G))
+	 * llen   size for unreduced (F,G) at this level
+	 * dlen   size for input (F,G) from deeper level
+	 * Note: we always have llen >= dlen (constraint enforced in profiles)
+	 */
+	size_t slen = prof->max_bl_small[depth];
+	size_t llen = prof->max_bl_large[depth];
+	size_t dlen = prof->max_bl_small[depth + 1];
+
+	/*
+	 * Fd   F from deeper level (dlen*hn)
+	 * Gd   G from deeper level (dlen*hn)
+	 * ft   f from this level (slen*n)
+	 * gt   g from this level (slen*n)
+	 */
+	uint32_t *Fd = tmp;
+	uint32_t *Gd = Fd + dlen * hn;
+	uint32_t *fgt = Gd + dlen * hn;
+
+	/*
+	 * Get (f,g) for this level (in RNS+NTT).
+	 */
+	if (depth < prof->min_save_fg[logn_top]) {
+		make_fg_intermediate(prof, logn_top, f, g, depth, fgt);
+	} else {
+		uint32_t *sav_fg = tmp + ((size_t)6 << logn_top);
+		for (unsigned d = prof->min_save_fg[logn_top];
+			d <= depth; d ++)
+		{
+			sav_fg -= prof->max_bl_small[d] << (logn_top + 1 - d);
+		}
+		memmove(fgt, sav_fg, 2 * slen * n * sizeof *fgt);
+	}
+	/* obsolete
+	kg_stats_set_max_size(logn_top, depth,
+		2 * dlen * hn + 3 * ((size_t)1 << logn_top));
+	*/
+
+	/*
+	 * Move buffers so that we have room for the unreduced (F,G) at
+	 * this level.
+	 *   Ft   F from this level (unreduced) (llen*n)
+	 *   Gt   G from this level (unreduced) (llen*n)
+	 *   ft   f from this level (slen*n)
+	 *   gt   g from this level (slen*n)
+	 *   Fd   F from deeper level (dlen*hn)
+	 *   Gd   G from deeper level (dlen*hn)
+	 */
+	uint32_t *Ft = tmp;
+	uint32_t *Gt = Ft + llen * n;
+	uint32_t *ft = Gt + llen * n;
+	uint32_t *gt = ft + slen * n;
+	Fd = gt + slen * n;
+	Gd = Fd + dlen * hn;
+	uint32_t *t1 = Gd + dlen * hn;
+	memmove(ft, fgt, 2 * n * slen * sizeof *ft);
+	memmove(Fd, tmp, 2 * hn * dlen * sizeof *tmp);
+	/* obsolete
+	kg_stats_set_max_size(logn_top, depth,
+		2 * llen * n + 2 * slen * n + 2 * dlen * hn);
+	*/
+
+	/*
+	 * Convert Fd and Gd to RNS, with output temporarily stored
+	 * in (Ft, Gt). Fd and Gd have degree hn only; we store the
+	 * values for each modulus p in the _last_ hn slots of the
+	 * n-word line for that modulus.
+	 */
+	for (size_t u = 0; u < llen; u ++) {
+		uint32_t p = PRIMES[u].p;
+		uint32_t p0i = PRIMES[u].p0i;
+		uint32_t R2 = PRIMES[u].R2;
+		uint32_t Rx = mp_Rx31((unsigned)dlen, p, p0i, R2);
+		uint32_t *xt = Ft + u * n + hn;
+		uint32_t *yt = Gt + u * n + hn;
+#if NTRUGEN_AVX2
+		if (logn >= 4) {
+			__m256i yp = _mm256_set1_epi32(p);
+			__m256i yp0i = _mm256_set1_epi32(p0i);
+			__m256i yR2 = _mm256_set1_epi32(R2);
+			__m256i yRx = _mm256_set1_epi32(Rx);
+			for (size_t v = 0; v < hn; v += 8) {
+				_mm256_storeu_si256((__m256i *)(xt + v),
+					zint_mod_small_signed_x8(Fd + v, dlen,
+						hn, yp, yp0i, yR2, yRx));
+				_mm256_storeu_si256((__m256i *)(yt + v),
+					zint_mod_small_signed_x8(Gd + v, dlen,
+						hn, yp, yp0i, yR2, yRx));
+			}
+		} else {
+			for (size_t v = 0; v < hn; v ++) {
+				xt[v] = zint_mod_small_signed(Fd + v, dlen, hn,
+					p, p0i, R2, Rx);
+				yt[v] = zint_mod_small_signed(Gd + v, dlen, hn,
+					p, p0i, R2, Rx);
+			}
+		}
+#else // NTRUGEN_AVX2
+		for (size_t v = 0; v < hn; v ++) {
+			xt[v] = zint_mod_small_signed(Fd + v, dlen, hn,
+				p, p0i, R2, Rx);
+			yt[v] = zint_mod_small_signed(Gd + v, dlen, hn,
+				p, p0i, R2, Rx);
+		}
+#endif // NTRUGEN_AVX2
+	}
+
+	/*
+	 * Fd and Gd are no longer needed.
+	 */
+	t1 = Fd;
+	/* obsolete
+	kg_stats_set_max_size(logn_top, depth,
+		2 * llen * n + 2 * slen * n + llen);
+	kg_stats_set_max_size(logn_top, depth,
+		2 * llen * n + 2 * slen * n + 4 * n);
+	*/
+
+	/*
+	 * Compute (F,G) (unreduced) modulo sufficiently many small primes.
+	 * We also un-NTT (f,g) as we go; when slen primes have been
+	 * processed, we obtain (f,g) in RNS, and we apply the CRT to
+	 * get (f,g) in plain representation.
+	 */
+	for (size_t u = 0; u < llen; u ++) {
+		/*
+		 * If we have processed exactly slen primes, then (f,g)
+		 * are in RNS, and we can rebuild them.
+		 */
+		if (u == slen) {
+			zint_rebuild_CRT(ft, slen, n, 2, 1, t1);
+		}
+
+		uint32_t p = PRIMES[u].p;
+		uint32_t p0i = PRIMES[u].p0i;
+		uint32_t R2 = PRIMES[u].R2;
+
+		/*
+		 * Memory layout: we keep Ft, Gt, ft and gt; we append:
+		 *   gm    NTT support (n)
+		 *   igm   iNTT support (n)
+		 *   fx    temporary f mod p (NTT) (n)
+		 *   gx    temporary g mod p (NTT) (n)
+		 */
+		uint32_t *gm = t1;
+		uint32_t *igm = gm + n;
+		uint32_t *fx = igm + n;
+		uint32_t *gx = fx + n;
+		mp_mkgmigm(logn, gm, igm, PRIMES[u].g, PRIMES[u].ig, p, p0i);
+		if (u < slen) {
+			memcpy(fx, ft + u * n, n * sizeof *fx);
+			memcpy(gx, gt + u * n, n * sizeof *gx);
+			mp_iNTT(logn, ft + u * n, igm, p, p0i);
+			mp_iNTT(logn, gt + u * n, igm, p, p0i);
+		} else {
+			uint32_t Rx = mp_Rx31((unsigned)slen, p, p0i, R2);
+			for (size_t v = 0; v < n; v ++) {
+				fx[v] = zint_mod_small_signed(ft + v, slen, n,
+					p, p0i, R2, Rx);
+				gx[v] = zint_mod_small_signed(gt + v, slen, n,
+					p, p0i, R2, Rx);
+			}
+			mp_NTT(logn, fx, gm, p, p0i);
+			mp_NTT(logn, gx, gm, p, p0i);
+		}
+
+		/*
+		 * We have (F,G) from deeper level in Ft and Gt, in
+		 * RNS. We apply the NTT modulo p.
+		 */
+		uint32_t *Fe = Ft + u * n;
+		uint32_t *Ge = Gt + u * n;
+		mp_NTT(logn - 1, Fe + hn, gm, p, p0i);
+		mp_NTT(logn - 1, Ge + hn, gm, p, p0i);
+
+		/*
+		 * Compute F and G (unreduced) modulo p.
+		 */
+#if NTRUGEN_AVX2
+		if (hn >= 4) {
+			__m256i yp = _mm256_set1_epi32(p);
+			__m256i yp0i = _mm256_set1_epi32(p0i);
+			__m256i yR2 = _mm256_set1_epi32(R2);
+			for (size_t v = 0; v < hn; v += 4) {
+				__m256i yfa = _mm256_loadu_si256(
+					(__m256i *)(fx + (v << 1)));
+				__m256i yga = _mm256_loadu_si256(
+					(__m256i *)(gx + (v << 1)));
+				__m256i yfb = _mm256_srli_epi64(yfa, 32);
+				__m256i ygb = _mm256_srli_epi64(yga, 32);
+				__m128i xFe = _mm_loadu_si128(
+					(__m128i *)(Fe + v + hn));
+				__m128i xGe = _mm_loadu_si128(
+					(__m128i *)(Ge + v + hn));
+				__m256i yFp = _mm256_permute4x64_epi64(
+					_mm256_castsi128_si256(xFe), 0x50);
+				__m256i yGp = _mm256_permute4x64_epi64(
+					_mm256_castsi128_si256(xGe), 0x50);
+				yFp = _mm256_shuffle_epi32(yFp, 0x30);
+				yGp = _mm256_shuffle_epi32(yGp, 0x30);
+				yFp = mp_montymul_x4(yFp, yR2, yp, yp0i);
+				yGp = mp_montymul_x4(yGp, yR2, yp, yp0i);
+				__m256i yFe0 = mp_montymul_x4(
+					ygb, yFp, yp, yp0i);
+				__m256i yFe1 = mp_montymul_x4(
+					yga, yFp, yp, yp0i);
+				__m256i yGe0 = mp_montymul_x4(
+					yfb, yGp, yp, yp0i);
+				__m256i yGe1 = mp_montymul_x4(
+					yfa, yGp, yp, yp0i);
+				_mm256_storeu_si256((__m256i *)(Fe + (v << 1)),
+					_mm256_or_si256(yFe0,
+						_mm256_slli_epi64(yFe1, 32)));
+				_mm256_storeu_si256((__m256i *)(Ge + (v << 1)),
+					_mm256_or_si256(yGe0,
+						_mm256_slli_epi64(yGe1, 32)));
+			}
+		} else {
+			for (size_t v = 0; v < hn; v ++) {
+				uint32_t fa = fx[(v << 1) + 0];
+				uint32_t fb = fx[(v << 1) + 1];
+				uint32_t ga = gx[(v << 1) + 0];
+				uint32_t gb = gx[(v << 1) + 1];
+				uint32_t mFp = mp_montymul(
+					Fe[v + hn], R2, p, p0i);
+				uint32_t mGp = mp_montymul(
+					Ge[v + hn], R2, p, p0i);
+				Fe[(v << 1) + 0] = mp_montymul(gb, mFp, p, p0i);
+				Fe[(v << 1) + 1] = mp_montymul(ga, mFp, p, p0i);
+				Ge[(v << 1) + 0] = mp_montymul(fb, mGp, p, p0i);
+				Ge[(v << 1) + 1] = mp_montymul(fa, mGp, p, p0i);
+			}
+		}
+#else // NTRUGEN_AVX2
+		for (size_t v = 0; v < hn; v ++) {
+			uint32_t fa = fx[(v << 1) + 0];
+			uint32_t fb = fx[(v << 1) + 1];
+			uint32_t ga = gx[(v << 1) + 0];
+			uint32_t gb = gx[(v << 1) + 1];
+			uint32_t mFp = mp_montymul(Fe[v + hn], R2, p, p0i);
+			uint32_t mGp = mp_montymul(Ge[v + hn], R2, p, p0i);
+			Fe[(v << 1) + 0] = mp_montymul(gb, mFp, p, p0i);
+			Fe[(v << 1) + 1] = mp_montymul(ga, mFp, p, p0i);
+			Ge[(v << 1) + 0] = mp_montymul(fb, mGp, p, p0i);
+			Ge[(v << 1) + 1] = mp_montymul(fa, mGp, p, p0i);
+		}
+#endif // NTRUGEN_AVX2
+
+		/*
+		 * We want the new (F,G) in RNS only (no NTT).
+		 */
+		mp_iNTT(logn, Fe, igm, p, p0i);
+		mp_iNTT(logn, Ge, igm, p, p0i);
+	}
+
+	/*
+	 * Edge case: if slen == llen, then we have not rebuilt (f,g)
+	 * into plain representation yet, so we do it now.
+	 */
+	if (slen == llen) {
+		zint_rebuild_CRT(ft, slen, n, 2, 1, t1);
+	}
+
+	/*
+	 * We now have the unreduced (F,G) in RNS. We rebuild their
+	 * plain representation.
+	 */
+	zint_rebuild_CRT(Ft, llen, n, 2, 1, t1);
+
+	/*
+	 * We now reduce these (F,G) with Babai's nearest plane
+	 * algorithm. The reduction conceptually goes as follows:
+	 *   k <- round((F*adj(f) + G*adj(g))/(f*adj(f) + g*adj(g)))
+	 *   (F, G) <- (F - k*f, G - k*g)
+	 * We use fixed-point approximations of (f,g) and (F, G) to get
+	 * a value k as a small polynomial with scaling; we then apply
+	 * k on the full-width polynomial. Each iteration "shaves" a
+	 * a few bits off F and G.
+	 *
+	 * We apply the process sufficiently many times to reduce (F, G)
+	 * to the size of (f, g) with a reasonable probability of success.
+	 * Since we want full constant-time processing, the number of
+	 * iterations and the accessed slots work on some assumptions on
+	 * the sizes of values (sizes have been measured over many samples,
+	 * and a margin of 5 times the standard deviation).
+	 */
+
+	/*
+	 * If depth is at least 2, and we will use the NTT to subtract
+	 * k*(f,g) from (F,G), then we will need to convert (f,g) to NTT over
+	 * slen+1 words, which requires an extra word to ft and gt.
+	 */
+	int use_sub_ntt = (depth > 1 && logn >= MIN_LOGN_FGNTT);
+	if (use_sub_ntt) {
+		memmove(gt + n, gt, n * slen * sizeof *gt);
+		gt += n;
+		t1 += 2 * n;
+	}
+
+	/*
+	 * New layout:
+	 *   Ft    F from this level (unreduced) (llen*n)
+	 *   Gt    G from this level (unreduced) (llen*n)
+	 *   ft    f from this level (slen*n) (+n if use_sub_ntt)
+	 *   gt    g from this level (slen*n) (+n if use_sub_ntt)
+	 *   rt3   (n fxr = 2*n)
+	 *   rt4   (n fxr = 2*n)
+	 *   rt1   (hn fxr = n)
+	 */
+
+	fxr *rt3 = (fxr *)t1;
+	fxr *rt4 = rt3 + n;
+	fxr *rt1 = rt4 + n;
+	/* obsolete
+	kg_stats_set_max_size(logn_top, depth,
+		2 * llen * n + 2 * slen * n + 5 * n);
+	*/
+
+	/*
+	 * We consider only the top rlen words of (f,g).
+	 */
+	size_t rlen = prof->word_win[depth];
+	if (rlen > slen) {
+		rlen = slen;
+	}
+	size_t blen = slen - rlen;
+	uint32_t *ftb = ft + blen * n;
+	uint32_t *gtb = gt + blen * n;
+	uint32_t scale_fg = 31 * (uint32_t)blen;
+	uint32_t scale_FG = 31 * (uint32_t)llen;
+
+	/*
+	 * Convert f and g into fixed-point approximations, in rt3 and rt4,
+	 * respectively. They are scaled down by 2^(scale_fg + scale_x).
+	 * scale_fg is public (it depends only on the recursion depth), but
+	 * scale_x comes from a measurement on the actual values of (f,g) and
+	 * is thus secret.
+	 *
+	 * The value scale_x is adjusted so that the largest coefficient is
+	 * close to, but lower than, some limit t (in absolute value). The
+	 * limit t is chosen so that f*adj(f) + g*adj(g) does not overflow,
+	 * i.e. all coefficients must remain below 2^31.
+	 *
+	 * Let n be the degree; we know that n <= 2^10. The squared norm
+	 * of a polynomial is the sum of the squared norms of the
+	 * coefficients, with the squared norm of a complex number being
+	 * the product of that number with its complex conjugate. If all
+	 * coefficients of f are less than t (in absolute value), then
+	 * the squared norm of f is less than n*t^2. The squared norm of
+	 * FFT(f) (f in FFT representation) is exactly n times the
+	 * squared norm of f, so this leads to n^2*t^2 as a maximum
+	 * bound. adj(f) has the same norm as f. This implies that each
+	 * complex coefficient of FFT(f) has a maximum squared norm of
+	 * n^2*t^2 (with a maximally imbalanced polynomial with all
+	 * coefficient but one being zero). The computation of f*adj(f)
+	 * exactly is, in FFT representation, the product of each
+	 * coefficient with its conjugate; thus, the coefficients of
+	 * f*adj(f), in FFT representation, are at most n^2*t^2.
+	 *
+	 * Since we want the coefficients of f*adj(f)+g*adj(g) not to exceed
+	 * 2^31, we need n^2*t^2 <= 2^30, i.e. n*t <= 2^15. We can adjust t
+	 * accordingly (called scale_t in the code below). We also need to
+	 * take care that t must not exceed scale_x. Approximation of f and
+	 * g are extracted with scale scale_fg + scale_x - scale_t, and
+	 * later fixed by dividing them by 2^scale_t.
+	 */
+	uint32_t scale_xf = poly_max_bitlength(logn, ftb, rlen);
+	uint32_t scale_xg = poly_max_bitlength(logn, gtb, rlen);
+	uint32_t scale_x = scale_xf;
+	scale_x ^= (scale_xf ^ scale_xg) & tbmask(scale_xf - scale_xg);
+	uint32_t scale_t = 15 - logn;
+	scale_t ^= (scale_t ^ scale_x) & tbmask(scale_x - scale_t);
+	uint32_t scdiff = scale_x - scale_t;
+
+	poly_big_to_fixed(logn, rt3, ftb, rlen, scdiff);
+	poly_big_to_fixed(logn, rt4, gtb, rlen, scdiff);
+
+	/*
+	 * Compute adj(f)/(f*adj(f) + g*adj(g)) into rt3 (FFT).
+	 * Compute adj(g)/(f*adj(f) + g*adj(g)) into rt4 (FFT).
+	 */
+	vect_FFT(logn, rt3);
+	vect_FFT(logn, rt4);
+	vect_norm_fft(logn, rt1, rt3, rt4);
+	vect_mul2e(logn, rt3, scale_t);
+	vect_mul2e(logn, rt4, scale_t);
+	for (size_t u = 0; u < hn; u ++) {
+#if NTRUGEN_AVX2
+		fxr ni3 = fxr_neg(rt3[u + hn]);
+		fxr ni4 = fxr_neg(rt4[u + hn]);
+		fxr_div_x4_1(&rt3[u], &ni3, &rt4[u], &ni4, rt1[u]);
+		rt3[u + hn] = ni3;
+		rt4[u + hn] = ni4;
+#else // NTRUGEN_AVX2
+		rt3[u] = fxr_div(rt3[u], rt1[u]);
+		rt3[u + hn] = fxr_div(fxr_neg(rt3[u + hn]), rt1[u]);
+		rt4[u] = fxr_div(rt4[u], rt1[u]);
+		rt4[u + hn] = fxr_div(fxr_neg(rt4[u + hn]), rt1[u]);
+#endif // NTRUGEN_AVX2
+	}
+
+	/*
+	 * New layout:
+	 *   Ft    F from this level (unreduced) (llen*n)
+	 *   Gt    G from this level (unreduced) (llen*n)
+	 *   ft    f from this level (slen*n) (+n if use_sub_ntt)
+	 *   gt    g from this level (slen*n) (+n if use_sub_ntt)
+	 *   rt3   (n fxr = 2*n)
+	 *   rt4   (n fxr = 2*n)
+	 *   rt1   (n fxr = 2*n)     |   k    (n)
+	 *   rt2   (n fxr = 2*n)     |   t2   (3*n)
+	 * Exception: at depth == 1, we omit ft and gt:
+	 *   Ft    F from this level (unreduced) (llen*n)
+	 *   Gt    G from this level (unreduced) (llen*n)
+	 *   rt3   (n fxr = 2*n)
+	 *   rt4   (n fxr = 2*n)
+	 *   rt1   (n fxr = 2*n)     |   k    (n)
+	 *   rt2   (n fxr = 2*n)     |   t2   (3*n)
+	 */
+	if (depth == 1) {
+		t1 = ft;
+		fxr *nrt3 = (fxr *)t1;
+		memmove(nrt3, rt3, 2 * n * sizeof *rt3);
+		rt3 = nrt3;
+		rt4 = rt3 + n;
+		rt1 = rt4 + n;
+	}
+	int32_t *k = (int32_t *)rt1;
+	uint32_t *t2 = (uint32_t *)(k + n);
+	fxr *rt2 = (fxr *)t2;
+	if (rt2 < (rt1 + n)) {
+		rt2 = rt1 + n;
+	}
+	/* obsolete
+	if (depth == 1) {
+		kg_stats_set_max_size(logn_top, depth,
+			2 * llen * n + 8 * n);
+	} else {
+		kg_stats_set_max_size(logn_top, depth,
+			2 * llen * n + 2 * slen * n + 8 * n);
+		kg_stats_set_max_size(logn_top, depth,
+			2 * llen * n + 2 * slen * n + 4 * n + n
+			+ 2 * n + (slen + 1) * n + n);
+	}
+	*/
+
+	/*
+	 * If we are going to use poly_sub_scaled_ntt(), then we convert
+	 * f and g to the NTT representation. Since poly_sub_scaled_ntt()
+	 * itself will use more than n*(slen+2) words in t2[], we can do
+	 * the same here.
+	 */
+	if (use_sub_ntt) {
+		uint32_t *gm = t2;
+		uint32_t *tn = gm + n;
+		for (size_t u = 0; u <= slen; u ++) {
+			uint32_t p = PRIMES[u].p;
+			uint32_t p0i = PRIMES[u].p0i;
+			uint32_t R2 = PRIMES[u].R2;
+			uint32_t Rx = mp_Rx31((unsigned)slen, p, p0i, R2);
+			mp_mkgm(logn, gm, PRIMES[u].g, p, p0i);
+			for (size_t v = 0; v < n; v ++) {
+				tn[v] = zint_mod_small_signed(
+					ft + v, slen, n, p, p0i, R2, Rx);
+			}
+			mp_NTT(logn, tn, gm, p, p0i);
+			tn += n;
+		}
+		tn = gm + n;
+		memmove(ft, tn, (slen + 1) * n * sizeof *tn);
+		for (size_t u = 0; u <= slen; u ++) {
+			uint32_t p = PRIMES[u].p;
+			uint32_t p0i = PRIMES[u].p0i;
+			uint32_t R2 = PRIMES[u].R2;
+			uint32_t Rx = mp_Rx31((unsigned)slen, p, p0i, R2);
+			mp_mkgm(logn, gm, PRIMES[u].g, p, p0i);
+			for (size_t v = 0; v < n; v ++) {
+				tn[v] = zint_mod_small_signed(
+					gt + v, slen, n, p, p0i, R2, Rx);
+			}
+			mp_NTT(logn, tn, gm, p, p0i);
+			tn += n;
+		}
+		tn = gm + n;
+		memmove(gt, tn, (slen + 1) * n * sizeof *tn);
+	}
+
+	/*
+	 * Reduce F and G repeatedly.
+	 */
+	size_t FGlen = llen;
+	for (;;) {
+		/*
+		 * Convert the current F and G into fixed-point. We want
+		 * to apply scaling scale_FG + scale_x.
+		 */
+		uint32_t tlen, toff;
+		DIVREM31(tlen, toff, scale_FG);
+		poly_big_to_fixed(logn, rt1,
+			Ft + tlen * n, FGlen - tlen, scale_x + toff);
+		poly_big_to_fixed(logn, rt2,
+			Gt + tlen * n, FGlen - tlen, scale_x + toff);
+
+		/*
+		 * rt2 <- (F*adj(f) + G*adj(g)) / (f*adj(f) + g*adj(g))
+		 */
+		vect_FFT(logn, rt1);
+		vect_FFT(logn, rt2);
+		vect_mul_fft(logn, rt1, rt3);
+		vect_mul_fft(logn, rt2, rt4);
+		vect_add(logn, rt2, rt1);
+		vect_iFFT(logn, rt2);
+
+		/*
+		 * k <- round(rt2)
+		 */
+		for (size_t u = 0; u < n; u ++) {
+			k[u] = fxr_round(rt2[u]);
+		}
+
+		/*
+		 * (f,g) are scaled by scale_fg + scale_x
+		 * (F,G) are scaled by scale_FG + scale_x
+		 * Thus, k is scaled by scale_FG - scale_fg, which is public.
+		 */
+		uint32_t scale_k = scale_FG - scale_fg;
+		if (depth == 1) {
+			poly_sub_kfg_scaled_depth1(logn_top, Ft, Gt, FGlen,
+				(uint32_t *)k, scale_k, f, g, t2);
+		} else if (use_sub_ntt) {
+			poly_sub_scaled_ntt(logn, Ft, FGlen, ft, slen,
+				k, scale_k, t2);
+			poly_sub_scaled_ntt(logn, Gt, FGlen, gt, slen,
+				k, scale_k, t2);
+		} else {
+			poly_sub_scaled(logn, Ft, FGlen, ft, slen, k, scale_k);
+			poly_sub_scaled(logn, Gt, FGlen, gt, slen, k, scale_k);
+		}
+
+		/*
+		 * We now assume that F and G have shrunk by at least
+		 * reduce_bits (profile-dependent). We adjust FGlen accordinly.
+		 */
+		if (scale_FG <= scale_fg) {
+			break;
+		}
+		if (scale_FG <= (scale_fg + prof->reduce_bits)) {
+			scale_FG = scale_fg;
+		} else {
+			scale_FG -= prof->reduce_bits;
+		}
+		while (FGlen > slen
+			&& 31 * (FGlen - slen) > scale_FG - scale_fg + 30)
+		{
+			FGlen --;
+		}
+	}
+
+	/*
+	 * Output F is already in the right place; G is in Gt, and must be
+	 * moved back a bit.
+	 */
+	memmove(tmp + slen * n, Gt, slen * n * sizeof *tmp);
+	Gt = tmp + slen * n;
+
+	/*
+	 * Reduction is done. We test the current solution modulo a single
+	 * prime.
+	 * Exception: we cannot do that if depth == 1, since in that case
+	 * we did not keep (ft,gt). Reduction errors rarely occur at this
+	 * stage, so we can omit that test (depth-0 test will cover it).
+	 *
+	 * If use_sub_ntt != 0, then ft and gt are already in NTT
+	 * representation.
+	 */
+	if (depth == 1) {
+		return SOLVE_OK;
+	}
+
+	t2 = t1 + n;
+	uint32_t *t3 = t2 + n;
+	uint32_t *t4 = t3 + n;
+	uint32_t p = PRIMES[0].p;
+	uint32_t p0i = PRIMES[0].p0i;
+	uint32_t R2 = PRIMES[0].R2;
+	uint32_t Rx = mp_Rx31(slen, p, p0i, R2);
+	mp_mkgm(logn, t4, PRIMES[0].g, p, p0i);
+	if (use_sub_ntt) {
+		t1 = ft;
+		for (size_t u = 0; u < n; u ++) {
+			t2[u] = zint_mod_small_signed(
+				Gt + u, slen, n, p, p0i, R2, Rx);
+		}
+		mp_NTT(logn, t2, t4, p, p0i);
+	} else {
+		for (size_t u = 0; u < n; u ++) {
+			t1[u] = zint_mod_small_signed(
+				ft + u, slen, n, p, p0i, R2, Rx);
+			t2[u] = zint_mod_small_signed(
+				Gt + u, slen, n, p, p0i, R2, Rx);
+		}
+		mp_NTT(logn, t1, t4, p, p0i);
+		mp_NTT(logn, t2, t4, p, p0i);
+	}
+#if NTRUGEN_AVX2
+	if (n >= 8) {
+		__m256i yp = _mm256_set1_epi32(p);
+		__m256i yp0i = _mm256_set1_epi32(p0i);
+		for (size_t u = 0; u < n; u += 8) {
+			__m256i y1 = _mm256_loadu_si256((__m256i *)(t1 + u));
+			__m256i y2 = _mm256_loadu_si256((__m256i *)(t2 + u));
+			__m256i y3 = mp_montymul_x8(y1, y2, yp, yp0i);
+			_mm256_storeu_si256((__m256i *)(t3 + u), y3);
+		}
+	} else {
+		for (size_t u = 0; u < n; u ++) {
+			t3[u] = mp_montymul(t1[u], t2[u], p, p0i);
+		}
+	}
+#else // NTRUGEN_AVX2
+	for (size_t u = 0; u < n; u ++) {
+		t3[u] = mp_montymul(t1[u], t2[u], p, p0i);
+	}
+#endif // NTRUGEN_AVX2
+	if (use_sub_ntt) {
+		t1 = gt;
+		for (size_t u = 0; u < n; u ++) {
+			t2[u] = zint_mod_small_signed(
+				Ft + u, slen, n, p, p0i, R2, Rx);
+		}
+		mp_NTT(logn, t2, t4, p, p0i);
+	} else {
+		for (size_t u = 0; u < n; u ++) {
+			t1[u] = zint_mod_small_signed(
+				gt + u, slen, n, p, p0i, R2, Rx);
+			t2[u] = zint_mod_small_signed(
+				Ft + u, slen, n, p, p0i, R2, Rx);
+		}
+		mp_NTT(logn, t1, t4, p, p0i);
+		mp_NTT(logn, t2, t4, p, p0i);
+	}
+	uint32_t rv = mp_montymul(prof->q, 1, p, p0i);
+#if NTRUGEN_AVX2
+	if (n >= 8) {
+		__m256i yp = _mm256_set1_epi32(p);
+		__m256i yp0i = _mm256_set1_epi32(p0i);
+		__m256i yrv = _mm256_set1_epi32(rv);
+		for (size_t u = 0; u < n; u += 8) {
+			__m256i y1 = _mm256_loadu_si256((__m256i *)(t1 + u));
+			__m256i y2 = _mm256_loadu_si256((__m256i *)(t2 + u));
+			__m256i y3 = _mm256_loadu_si256((__m256i *)(t3 + u));
+			__m256i yx = mp_sub_x8(y3,
+				mp_montymul_x8(y1, y2, yp, yp0i), yp);
+			if ((uint32_t)_mm256_movemask_epi8(
+				_mm256_cmpeq_epi32(yx, yrv)) != 0xFFFFFFFF)
+			{
+				return SOLVE_ERR_REDUCE;
+			}
+		}
+	} else {
+		for (size_t u = 0; u < n; u ++) {
+			uint32_t x = mp_montymul(t1[u], t2[u], p, p0i);
+			if (mp_sub(t3[u], x, p) != rv) {
+				return SOLVE_ERR_REDUCE;
+			}
+		}
+	}
+#else // NTRUGEN_AVX2
+	for (size_t u = 0; u < n; u ++) {
+		uint32_t x = mp_montymul(t1[u], t2[u], p, p0i);
+		if (mp_sub(t3[u], x, p) != rv) {
+			return SOLVE_ERR_REDUCE;
+		}
+	}
+#endif // NTRUGEN_AVX2
+
+	return SOLVE_OK;
+}
+
+/*
+ * Solving the NTRU equation, top recursion level. This is a specialized
+ * variant for solve_NTRU_intermediate() with depth == 0, for lower RAM
+ * usage and faster operation.
+ *
+ * Returned value: 0 on success, a negative error code otherwise.
+ */
+TARGET_AVX2
+static int
+solve_NTRU_depth0(const ntru_profile *restrict prof,
+	unsigned logn,
+	const int8_t *restrict f, const int8_t *restrict g,
+	uint32_t *restrict tmp)
+{
+	size_t n = (size_t)1 << logn;
+	size_t hn = n >> 1;
+
+	/*
+	 * At depth 0, all values fit on 30 bits, so we work with a
+	 * single modulus p.
+	 */
+	uint32_t p = PRIMES[0].p;
+	uint32_t p0i = PRIMES[0].p0i;
+	uint32_t R2 = PRIMES[0].R2;
+
+	/*
+	 * Buffer layout:
+	 *   Fd   F from upper level (hn)
+	 *   Gd   G from upper level (hn)
+	 *   ft   f (n)
+	 *   gt   g (n)
+	 *   gm   helper for NTT
+	 */
+	uint32_t *Fd = tmp;
+	uint32_t *Gd = Fd + hn;
+	uint32_t *ft = Gd + hn;
+	uint32_t *gt = ft + n;
+	uint32_t *gm = gt + n;
+	/* obsolete
+	kg_stats_set_max_size(logn, 0, 2 * hn + 3 * n);
+	*/
+
+	/*
+	 * Load f and g, and convert to RNS+NTT.
+	 */
+	mp_mkgm(logn, gm, PRIMES[0].g, p, p0i);
+	poly_mp_set_small(logn, ft, f, p);
+	poly_mp_set_small(logn, gt, g, p);
+	mp_NTT(logn, ft, gm, p, p0i);
+	mp_NTT(logn, gt, gm, p, p0i);
+
+	/*
+	 * Convert Fd and Gd to RNS+NTT.
+	 */
+	poly_mp_set(logn - 1, Fd, p);
+	poly_mp_set(logn - 1, Gd, p);
+	mp_NTT(logn - 1, Fd, gm, p, p0i);
+	mp_NTT(logn - 1, Gd, gm, p, p0i);
+
+	/*
+	 * Build the unreduced (F,G) into ft and gt.
+	 */
+#if NTRUGEN_AVX2
+	if (hn >= 4) {
+		__m256i yp = _mm256_set1_epi32(p);
+		__m256i yp0i = _mm256_set1_epi32(p0i);
+		__m256i yR2 = _mm256_set1_epi32(R2);
+		for (size_t v = 0; v < hn; v += 4) {
+			__m256i yfa = _mm256_loadu_si256(
+				(__m256i *)(ft + (v << 1)));
+			__m256i yga = _mm256_loadu_si256(
+				(__m256i *)(gt + (v << 1)));
+			__m256i yfb = _mm256_srli_epi64(yfa, 32);
+			__m256i ygb = _mm256_srli_epi64(yga, 32);
+			__m128i xFd = _mm_loadu_si128((__m128i *)(Fd + v));
+			__m128i xGd = _mm_loadu_si128((__m128i *)(Gd + v));
+			__m256i yFd = _mm256_permute4x64_epi64(
+				_mm256_castsi128_si256(xFd), 0x50);
+			__m256i yGd = _mm256_permute4x64_epi64(
+				_mm256_castsi128_si256(xGd), 0x50);
+			yFd = _mm256_shuffle_epi32(yFd, 0x30);
+			yGd = _mm256_shuffle_epi32(yGd, 0x30);
+			yFd = mp_montymul_x4(yFd, yR2, yp, yp0i);
+			yGd = mp_montymul_x4(yGd, yR2, yp, yp0i);
+			__m256i yFe0 = mp_montymul_x4(ygb, yFd, yp, yp0i);
+			__m256i yFe1 = mp_montymul_x4(yga, yFd, yp, yp0i);
+			__m256i yGe0 = mp_montymul_x4(yfb, yGd, yp, yp0i);
+			__m256i yGe1 = mp_montymul_x4(yfa, yGd, yp, yp0i);
+			_mm256_storeu_si256((__m256i *)(ft + (v << 1)),
+				_mm256_or_si256(yFe0,
+					_mm256_slli_epi64(yFe1, 32)));
+			_mm256_storeu_si256((__m256i *)(gt + (v << 1)),
+				_mm256_or_si256(yGe0,
+					_mm256_slli_epi64(yGe1, 32)));
+		}
+	} else {
+		for (size_t v = 0; v < hn; v ++) {
+			uint32_t fa = ft[(v << 1) + 0];
+			uint32_t fb = ft[(v << 1) + 1];
+			uint32_t ga = gt[(v << 1) + 0];
+			uint32_t gb = gt[(v << 1) + 1];
+			uint32_t mFd = mp_montymul(Fd[v], R2, p, p0i);
+			uint32_t mGd = mp_montymul(Gd[v], R2, p, p0i);
+			ft[(v << 1) + 0] = mp_montymul(gb, mFd, p, p0i);
+			ft[(v << 1) + 1] = mp_montymul(ga, mFd, p, p0i);
+			gt[(v << 1) + 0] = mp_montymul(fb, mGd, p, p0i);
+			gt[(v << 1) + 1] = mp_montymul(fa, mGd, p, p0i);
+		}
+	}
+#else // NTRUGEN_AVX2
+	for (size_t u = 0; u < hn; u ++) {
+		uint32_t fa = ft[(u << 1) + 0];
+		uint32_t fb = ft[(u << 1) + 1];
+		uint32_t ga = gt[(u << 1) + 0];
+		uint32_t gb = gt[(u << 1) + 1];
+		uint32_t mFd = mp_montymul(Fd[u], R2, p, p0i);
+		uint32_t mGd = mp_montymul(Gd[u], R2, p, p0i);
+		ft[(u << 1) + 0] = mp_montymul(gb, mFd, p, p0i);
+		ft[(u << 1) + 1] = mp_montymul(ga, mFd, p, p0i);
+		gt[(u << 1) + 0] = mp_montymul(fb, mGd, p, p0i);
+		gt[(u << 1) + 1] = mp_montymul(fa, mGd, p, p0i);
+	}
+#endif // NTRUGEN_AVX2
+
+	/*
+	 * Reorganize buffers:
+	 *   Fp   unreduced F (RNS+NTT) (n)
+	 *   Gp   unreduced G (RNS+NTT) (n)
+	 *   t1   free (n)
+	 *   t2   NTT support (gm) (n)
+	 *   t3   free (n)
+	 *   t4   free (n)
+	 */
+	uint32_t *Fp = tmp;
+	uint32_t *Gp = Fp + n;
+	uint32_t *t1 = Gp + n;
+	uint32_t *t2 = t1 + n;  /* alias on gm */
+	uint32_t *t3 = t2 + n;
+	uint32_t *t4 = t3 + n;
+	memmove(Fp, ft, 2 * n * sizeof *ft);
+	/* obsolete
+	kg_stats_set_max_size(logn, 0, 6 * n);
+	*/
+
+	/*
+	 * Working modulo p (using the NTT), we compute:
+	 *    t1 <- F*adj(f) + G*adj(g)
+	 *    t2 <- f*adj(f) + g*adj(g)
+	 */
+
+	/*
+	 * t4 <- f (RNS+NTT)
+	 */
+	poly_mp_set_small(logn, t4, f, p);
+	mp_NTT(logn, t4, gm, p, p0i);
+
+	/*
+	 * t1 <- F*adj(f) (RNS+NTT)
+	 * t3 <- f*adj(f) (RNS+NTT)
+	 */
+	for (size_t u = 0; u < n; u ++) {
+		uint32_t w = mp_montymul(t4[(n - 1) - u], R2, p, p0i);
+		t1[u] = mp_montymul(w, Fp[u], p, p0i);
+		t3[u] = mp_montymul(w, t4[u], p, p0i);
+	}
+
+	/*
+	 * t4 <- g (RNS+NTT)
+	 */
+	poly_mp_set_small(logn, t4, g, p);
+	mp_NTT(logn, t4, gm, p, p0i);
+
+	/*
+	 * t1 <- t1 + G*adj(g)
+	 * t3 <- t3 + g*adj(g)
+	 */
+	for (size_t u = 0; u < n; u ++) {
+		uint32_t w = mp_montymul(t4[(n - 1) - u], R2, p, p0i);
+		t1[u] = mp_add(t1[u], mp_montymul(w, Gp[u], p, p0i), p);
+		t3[u] = mp_add(t3[u], mp_montymul(w, t4[u], p, p0i), p);
+	}
+
+	/*
+	 * Convert back F*adj(f) + G*adj(g) and f*adj(f) + g*adj(g) to
+	 * plain representation, and move f*adj(f) + g*adj(g) to t2.
+	 */
+	mp_mkigm(logn, t4, PRIMES[0].ig, p, p0i);
+	mp_iNTT(logn, t1, t4, p, p0i);
+	mp_iNTT(logn, t3, t4, p, p0i);
+	for (size_t u = 0; u < n; u ++) {
+		/*
+		 * NOTE: no truncature to 31 bits.
+		 */
+		t1[u] = (uint32_t)mp_norm(t1[u], p);
+		t2[u] = (uint32_t)mp_norm(t3[u], p);
+	}
+
+	/*
+	 * Buffer contents:
+	 *   Fp   unreduced F (RNS+NTT) (n)
+	 *   Gp   unreduced G (RNS+NTT) (n)
+	 *   t1   F*adj(f) + G*adj(g) (plain, 32-bit) (n)
+	 *   t2   f*adj(f) + g*adj(g) (plain, 32-bit) (n)
+	 */
+
+	/*
+	 * We need to divide t1 by t2, and round the result. We convert
+	 * them to FFT representation, downscaled by 2^10 (to avoid overflows).
+	 * We first convert f*adj(f) + g*adj(g), which is auto-adjoint;
+	 * thus, its FFT representation only has half-size.
+	 */
+	fxr *rt3 = (fxr *)t3;
+	for (size_t u = 0; u < n; u ++) {
+		uint64_t x = (uint64_t)*(int32_t *)&t2[u] << 22;
+		rt3[u] = fxr_of_scaled32(x);
+	}
+	vect_FFT(logn, rt3);
+	fxr *rt2 = (fxr *)t2;
+	memmove(rt2, rt3, hn * sizeof *rt3);
+	rt3 = rt2 + hn;
+	/* obsolete
+	kg_stats_set_max_size(logn, 0, 6 * n);
+	*/
+
+	/*
+	 * Buffer contents:
+	 *   Fp    unreduced F (RNS+NTT) (n)
+	 *   Gp    unreduced G (RNS+NTT) (n)
+	 *   t1    F*adj(f) + G*adj(g) (plain, 32-bit) (n)
+	 *   rt2   f*adj(f) + g*adj(g) (FFT, auto-ajdoint) (hn fxr values = n)
+	 *   rt3   free (n fxr values = 2*n)
+	 */
+
+	/*
+	 * Convert F*adj(f) + G*adj(g) to FFT (scaled by 2^10) (into rt3).
+	 */
+	for (size_t u = 0; u < n; u ++) {
+		uint64_t x = (uint64_t)*(int32_t *)&t1[u] << 22;
+		rt3[u] = fxr_of_scaled32(x);
+	}
+	vect_FFT(logn, rt3);
+
+	/*
+	 * Divide F*adj(f) + G*adj(g) by f*adj(f) + g*adj(g) and round
+	 * the result into t1, with conversion to RNS.
+	 */
+	vect_div_autoadj_fft(logn, rt3, rt2);
+	vect_iFFT(logn, rt3);
+	for (size_t u = 0; u < n; u ++) {
+		t1[u] = mp_set(fxr_round(rt3[u]), p);
+	}
+
+	/*
+	 * Buffer contents:
+	 *   Fp    unreduced F (RNS+NTT) (n)
+	 *   Gp    unreduced G (RNS+NTT) (n)
+	 *   t1    k (RNS) (n)
+	 *   t2    free (n)
+	 *   t3    free (n)
+	 *   t4    free (n)
+	 */
+
+	/*
+	 * Convert k to RNS+NTT+Montgomery.
+	 */
+	mp_mkgm(logn, t4, PRIMES[0].g, p, p0i);
+	mp_NTT(logn, t1, t4, p, p0i);
+	for (size_t u = 0; u < n; u ++) {
+		t1[u] = mp_montymul(t1[u], R2, p, p0i);
+	}
+
+	/*
+	 * Subtract k*f from F and k*g from G.
+	 * We also compute f*G - g*F (in RNS+NTT) to check that the solution
+	 * is correct.
+	 */
+	for (size_t u = 0; u < n; u ++) {
+		t2[u] = mp_set(f[u], p);
+		t3[u] = mp_set(g[u], p);
+	}
+	mp_NTT(logn, t2, t4, p, p0i);
+	mp_NTT(logn, t3, t4, p, p0i);
+	uint32_t rv = mp_montymul(prof->q, 1, p, p0i);
+	for (size_t u = 0; u < n; u ++) {
+		Fp[u] = mp_sub(Fp[u], mp_montymul(t1[u], t2[u], p, p0i), p);
+		Gp[u] = mp_sub(Gp[u], mp_montymul(t1[u], t3[u], p, p0i), p);
+		uint32_t x = mp_sub(
+			mp_montymul(t2[u], Gp[u], p, p0i),
+			mp_montymul(t3[u], Fp[u], p, p0i), p);
+		if (x != rv) {
+			return SOLVE_ERR_REDUCE;
+		}
+	}
+
+	/*
+	 * Convert back F and G into normal representation.
+	 */
+	mp_mkigm(logn, t4, PRIMES[0].ig, p, p0i);
+	mp_iNTT(logn, Fp, t4, p, p0i);
+	mp_iNTT(logn, Gp, t4, p, p0i);
+	poly_mp_norm(logn, Fp, p);
+	poly_mp_norm(logn, Gp, p);
+
+	return SOLVE_OK;
+}
+
+#if 0
+/* unused */
+/*
+ * Verify that the given f, g, F and G fulfill the NTRU equation.
+ * Returned value is 1 on success, 0 on error.
+ *
+ * RAM USAGE: 4*n words
+ */
+static int
+verify_NTRU(const ntru_profile *restrict prof, unsigned logn,
+	const int8_t *restrict f, const int8_t *restrict g,
+	const int8_t *restrict F, const int8_t *restrict G,
+	uint32_t *tmp)
+{
+	size_t n = (size_t)1 << logn;
+	uint32_t *t1 = tmp;
+	uint32_t *t2 = t1 + n;
+	uint32_t *t3 = t2 + n;
+	uint32_t *t4 = t3 + n;
+	uint32_t p = PRIMES[0].p;
+	uint32_t p0i = PRIMES[0].p0i;
+	mp_mkgm(logn, t4, PRIMES[0].g, p, p0i);
+	for (size_t u = 0; u < n; u ++) {
+		t1[u] = mp_set(f[u], p);
+		t2[u] = mp_set(G[u], p);
+	}
+	mp_NTT(logn, t1, t4, p, p0i);
+	mp_NTT(logn, t2, t4, p, p0i);
+	for (size_t u = 0; u < n; u ++) {
+		t3[u] = mp_montymul(t1[u], t2[u], p, p0i);
+	}
+	for (size_t u = 0; u < n; u ++) {
+		t1[u] = mp_set(g[u], p);
+		t2[u] = mp_set(F[u], p);
+	}
+	mp_NTT(logn, t1, t4, p, p0i);
+	mp_NTT(logn, t2, t4, p, p0i);
+	uint32_t rv = mp_montymul(prof->q, 1, p, p0i);
+	for (size_t u = 0; u < n; u ++) {
+		uint32_t x = mp_montymul(t1[u], t2[u], p, p0i);
+		if (mp_sub(t3[u], x, p) != rv) {
+			return 0;
+		}
+	}
+	return 1;
+}
+#endif
+
+/* see ng_inner.h */
+int
+solve_NTRU(const ntru_profile *restrict prof, unsigned logn,
+	const int8_t *restrict f, const int8_t *restrict g, uint32_t *tmp)
+{
+	size_t n = (size_t)1 << logn;
+
+	int err = solve_NTRU_deepest(prof, logn, f, g, tmp);
+	if (err != SOLVE_OK) {
+		return err;
+	}
+	unsigned depth = logn;
+	while (depth -- > 1) {
+		err = solve_NTRU_intermediate(prof, logn, f, g, depth, tmp);
+		if (err != SOLVE_OK) {
+			return err;
+		}
+	}
+	err = solve_NTRU_depth0(prof, logn, f, g, tmp);
+	if (err != SOLVE_OK) {
+		return err;
+	}
+
+	/*
+	 * F and G are at the start of tmp[] (plain, 31 bits per value).
+	 * We need to convert them to 8-bit representation, and check
+	 * that they are within the expected range.
+	 */
+	int8_t *F = (int8_t *)(tmp + 2 * n);
+	int8_t *G = F + n;
+	int lim = prof->coeff_FG_limit[logn];
+	if (!poly_big_to_small(logn, F, tmp, lim)) {
+		return SOLVE_ERR_LIMIT;
+	}
+	if (!poly_big_to_small(logn, G, tmp + n, lim)) {
+		return SOLVE_ERR_LIMIT;
+	}
+	memmove(tmp, F, 2 * n);
+
+	return SOLVE_OK;
+}
+
+/* see ng_inner.h */
+int
+recover_G(unsigned logn, int32_t q, uint32_t ulim,
+	const int8_t *restrict f, const int8_t *restrict g,
+	const int8_t *restrict F, uint32_t *restrict tmp)
+{
+	size_t n = (size_t)1 << logn;
+	uint32_t *gm = tmp;
+	uint32_t *t1 = gm + n;
+	uint32_t *t2 = t1 + n;
+
+	uint32_t p = PRIMES[0].p;
+	uint32_t p0i = PRIMES[0].p0i;
+	uint32_t R2 = PRIMES[0].R2;
+	mp_mkgm(logn, gm, PRIMES[0].g, p, p0i);
+
+	/*
+	 * t2 <- q + g*F (RNS+NTT)
+	 */
+	for (size_t u = 0; u < n; u ++) {
+		t1[u] = mp_set(g[u], p);
+		t2[u] = mp_set(F[u], p);
+	}
+	mp_NTT(logn, t1, gm, p, p0i);
+	mp_NTT(logn, t2, gm, p, p0i);
+	uint32_t mq = mp_set(q, p);
+	for (size_t u = 0; u < n; u ++) {
+		uint32_t x = mp_montymul(t1[u], t2[u], p, p0i);
+		t2[u] = mp_add(mq, mp_montymul(x, R2, p, p0i), p);
+	}
+
+	/*
+	 * t2 <- (q + g*F)/f = G (RNS)
+	 */
+	for (size_t u = 0; u < n; u ++) {
+		t1[u] = mp_set(f[u], p);
+	}
+	mp_NTT(logn, t1, gm, p, p0i);
+	uint32_t b = 0;
+	for (size_t u = 0; u < n; u ++) {
+		b |= t1[u] - 1;
+		t2[u] = mp_div(t2[u], t1[u], p);
+	}
+	uint32_t *igm = gm;
+	mp_mkigm(logn, igm, PRIMES[0].ig, p, p0i);
+	mp_iNTT(logn, t2, igm, p, p0i);
+
+	/*
+	 * Check that the G coefficients are in the proper range.
+	 */
+	int8_t *G = (int8_t *)tmp;
+	for (size_t u = 0; u < n; u ++) {
+		uint32_t x = t2[u];
+		uint32_t y = tbmask((ulim << 1) - mp_add(x, ulim, p));
+		b |= y;
+		int32_t z = mp_norm(x & ~y, p);
+		G[u] = (int8_t)z;
+	}
+
+	/*
+	 * This failed if f was not invertible, i.e. one of its NTT
+	 * coefficients was zero, of if any of the G coefficients was
+	 * out of range.
+	 */
+	return (int)(1 - (b >> 31));
+}
+
+#if NTRUGEN_STATS
+/*
+ * All counters for statistics are gathered here. They are not thread-safe
+ * and thus should be disabled in normal builds.
+ */
+uint32_t stats_hawk_ctt_attempt = 0;
+uint32_t stats_hawk_ctt_reject = 0;
+uint32_t stats_solve_attempt = 0;
+uint32_t stats_solve_err_gcd = 0;
+uint32_t stats_solve_err_reduce = 0;
+uint32_t stats_solve_err_limit = 0;
+uint32_t stats_solve_success = 0;
+uint32_t stats_compute_w_attempt = 0;
+uint32_t stats_compute_w_err_lim1 = 0;
+uint32_t stats_compute_w_err_lim2 = 0;
+uint32_t stats_compute_w_err_lim3 = 0;
+uint32_t stats_compute_w_err_norm = 0;
+uint32_t stats_compute_w_success = 0;
+
+/* see ng_inner.h */
+void
+stats_init(void)
+{
+	stats_hawk_ctt_attempt = 0;
+	stats_hawk_ctt_reject = 0;
+	stats_solve_attempt = 0;
+	stats_solve_err_gcd = 0;
+	stats_solve_err_reduce = 0;
+	stats_solve_err_limit = 0;
+	stats_solve_success = 0;
+	stats_compute_w_attempt = 0;
+	stats_compute_w_err_lim1 = 0;
+	stats_compute_w_err_lim2 = 0;
+	stats_compute_w_err_lim3 = 0;
+	stats_compute_w_err_norm = 0;
+	stats_compute_w_success = 0;
+}
+#endif
diff --git a/lib/dns/hawk/ng_poly.c b/lib/dns/hawk/ng_poly.c
new file mode 100644
index 0000000000..6eec69cf73
--- /dev/null
+++ b/lib/dns/hawk/ng_poly.c
@@ -0,0 +1,1185 @@
+#include "ng_inner.h"
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+poly_mp_set_small(unsigned logn, uint32_t *restrict d,
+	const int8_t *restrict f, uint32_t p)
+{
+	size_t n = (size_t)1 << logn;
+#if NTRUGEN_AVX2
+	if (n >= 8) {
+		__m256i yp = _mm256_set1_epi32(p);
+		for (size_t u = 0; u < n; u += 8) {
+			__m256i yf = _mm256_setr_epi32(
+				f[u + 0], f[u + 1], f[u + 2], f[u + 3],
+				f[u + 4], f[u + 5], f[u + 6], f[u + 7]);
+			yf = mp_set_x8(yf, yp);
+			_mm256_storeu_si256((__m256i *)(d + u), yf);
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (size_t u = 0; u < n; u ++) {
+		d[u] = mp_set(f[u], p);
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+poly_mp_set(unsigned logn, uint32_t *f, uint32_t p)
+{
+	size_t n = (size_t)1 << logn;
+#if NTRUGEN_AVX2
+	if (n >= 8) {
+		__m256i yps = _mm256_set1_epi32(p + 0x80000000);
+		__m256i yt = _mm256_set1_epi32(0x3FFFFFFF);
+		for (size_t u = 0; u < n; u += 8) {
+			__m256i yf = _mm256_loadu_si256((__m256i *)(f + u));
+			yf = _mm256_add_epi32(yf, _mm256_and_si256(yps,
+				_mm256_cmpgt_epi32(yf, yt)));
+			_mm256_storeu_si256((__m256i *)(f + u), yf);
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (size_t u = 0; u < n; u ++) {
+		uint32_t x = f[u];
+		x |= (x & 0x40000000) << 1;
+		f[u] = mp_set(*(int32_t *)&x, p);
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+poly_mp_norm(unsigned logn, uint32_t *f, uint32_t p)
+{
+	size_t n = (size_t)1 << logn;
+#if NTRUGEN_AVX2
+	if (n >= 8) {
+		__m256i yp = _mm256_set1_epi32(p);
+		__m256i yhp = _mm256_srli_epi32(yp, 1);
+		__m256i ym = _mm256_set1_epi32(0x7FFFFFFF);
+		for (size_t u = 0; u < n; u += 8) {
+			__m256i yf = _mm256_loadu_si256((__m256i *)(f + u));
+			yf = _mm256_and_si256(ym, mp_norm_x8(yf, yp, yhp));
+			_mm256_storeu_si256((__m256i *)(f + u), yf);
+		}
+		return;
+	}
+#endif // NTRUGEN_AVX2
+	for (size_t u = 0; u < n; u ++) {
+		f[u] = (uint32_t)mp_norm(f[u], p) & 0x7FFFFFFF;
+	}
+}
+
+/* see ng_inner.h */
+int
+poly_big_to_small(unsigned logn, int8_t *restrict d,
+	const uint32_t *restrict s, int lim)
+{
+	size_t n = (size_t)1 << logn;
+	for (size_t u = 0; u < n; u ++) {
+		uint32_t x = s[u];
+		x |= (x & 0x40000000) << 1;
+		int32_t z = *(int32_t *)&x;
+		if (z < -lim || z > lim) {
+			return 0;
+		}
+		d[u] = (int8_t)z;
+	}
+	return 1;
+}
+
+/* see ng_inner.h */
+uint32_t
+poly_max_bitlength(unsigned logn, const uint32_t *f, size_t flen)
+{
+	if (flen == 0) {
+		return 0;
+	}
+
+	size_t n = (size_t)1 << logn;
+	uint32_t t = 0;
+	uint32_t tk = 0;
+	for (size_t u = 0; u < n; u ++, f ++) {
+		/* Extend sign bit into a 31-bit mask. */
+		uint32_t m = -(f[(flen - 1) << logn] >> 30) & 0x7FFFFFFF;
+
+		/* Get top non-zero sign-adjusted word, with index. */
+		uint32_t c = 0;
+		uint32_t ck = 0;
+		for (size_t v = 0; v < flen; v ++) {
+			uint32_t w = f[v << logn] ^ m; /* sign-adjusted word */
+			uint32_t nz = ((w - 1) >> 31) - 1;
+			c ^= nz & (c ^ w);
+			ck ^= nz & (ck ^ (uint32_t)v);
+		}
+
+		/* If ck > tk, or tk == ck but c > t, then (c,ck) must
+		   replace (t,tk) as current candidate for top word/index. */
+		uint32_t rr = tbmask((tk - ck) | (((tk ^ ck) - 1) & (t - c)));
+		t ^= rr & (t ^ c);
+		tk ^= rr & (tk ^ ck);
+	}
+
+	/*
+	 * Get bit length of the top word (which has been sign-adjusted)
+	 * and return the result.
+	 */
+	return 31 * tk + 32 - lzcnt(t);
+}
+
+/* see ng_inner.h */
+void
+poly_big_to_fixed(unsigned logn, fxr *restrict d, const uint32_t *restrict f,
+	size_t len, uint32_t sc)
+{
+	size_t n = (size_t)1 << logn;
+	if (len == 0) {
+		memset(d, 0, n * sizeof *d);
+		return;
+	}
+
+	/*
+	 * We split the bit length into sch and scl such that:
+	 *   sc = 31*sch + scl
+	 * We also want scl in the 1..31 range, not 0..30. It may happen
+	 * that sch becomes -1, which will "wrap around" (harmlessly).
+	 *
+	 * For each coefficient, we need three words, each with a given
+	 * left shift (negative for a right shift):
+	 *    sch-1   1 - scl
+	 *    sch     32 - scl
+	 *    sch+1   63 - scl
+	 */
+	uint32_t sch, scl;
+	DIVREM31(sch, scl, sc);
+	uint32_t z = (scl - 1) >> 31;
+	sch -= z;
+	scl |= 31 & -z;
+
+	uint32_t t0 = (uint32_t)(sch - 1) & 0xFFFFFF;
+	uint32_t t1 = sch & 0xFFFFFF;
+	uint32_t t2 = (uint32_t)(sch + 1) & 0xFFFFFF;
+
+	for (size_t u = 0; u < n; u ++, f ++) {
+		uint32_t w0, w1, w2, ws, xl, xh;
+
+		w0 = 0;
+		w1 = 0;
+		w2 = 0;
+		for (size_t v = 0; v < len; v ++) {
+			uint32_t t, w;
+
+			w = f[v << logn];
+			t = (uint32_t)v & 0xFFFFFF;
+			w0 |= w & -((uint32_t)((t ^ t0) - 1) >> 31);
+			w1 |= w & -((uint32_t)((t ^ t1) - 1) >> 31);
+			w2 |= w & -((uint32_t)((t ^ t2) - 1) >> 31);
+		}
+
+		/*
+		 * If there were not enough words for the requested
+		 * scaling, then we must supply copies with the proper
+		 * sign.
+		 */
+		ws = -(f[(len - 1) << logn] >> 30) >> 1;
+		w0 |= ws & -((uint32_t)((uint32_t)len - sch) >> 31);
+		w1 |= ws & -((uint32_t)((uint32_t)len - sch - 1) >> 31);
+		w2 |= ws & -((uint32_t)((uint32_t)len - sch - 2) >> 31);
+
+		/*
+		 * Assemble the 64-bit value with the shifts. We assume
+		 * that shifts on 32-bit values are constant-time with
+		 * regard to the shift count (this should be true on all
+		 * modern architectures; the last notable arch on which
+		 * shift timing depended on the count was the Pentium IV).
+		 *
+		 * Since the shift count (scl) is guaranteed to be in 1..31,
+		 * we do not have special cases to handle.
+		 *
+		 * We must sign-extend w2 to ensure the sign bit is properly
+		 * set in the fnr value.
+		 */
+		w2 |= (uint32_t)(w2 & 0x40000000) << 1;
+		xl = (w0 >> (scl - 1)) | (w1 << (32 - scl));
+		xh = (w1 >> scl) | (w2 << (31 - scl));
+		d[u] = fxr_of_scaled32((uint64_t)xl | ((uint64_t)xh << 32));
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+poly_sub_scaled(unsigned logn,
+	uint32_t *restrict F, size_t Flen,
+	const uint32_t *restrict f, size_t flen,
+	const int32_t *restrict k, uint32_t sc)
+{
+	if (flen == 0) {
+		return;
+	}
+	uint32_t sch, scl;
+	DIVREM31(sch, scl, sc);
+	if (sch >= Flen) {
+		return;
+	}
+	F += (size_t)sch << logn;
+	Flen -= sch;
+	switch (logn) {
+	case 1: {
+		uint32_t t0 = 0;
+		uint32_t t1 = 0;
+		uint32_t signf0 = -(f[(flen << 1) - 2] >> 30) >> 1;
+		uint32_t signf1 = -(f[(flen << 1) - 1] >> 30) >> 1;
+		int32_t k0 = k[0];
+		int32_t k1 = k[1];
+		int64_t cc0 = 0;
+		int64_t cc1 = 0;
+		for (size_t u = 0; u < Flen; u ++) {
+			/*
+			 * Next word, shifted.
+			 */
+			uint32_t f0, f1;
+			if (u < flen) {
+				f0 = f[(u << 1) + 0];
+				f1 = f[(u << 1) + 1];
+			} else {
+				f0 = signf0;
+				f1 = signf1;
+			}
+			uint32_t fs0 = ((f0 << scl) & 0x7FFFFFFF) | t0;
+			uint32_t fs1 = ((f1 << scl) & 0x7FFFFFFF) | t1;
+			t0 = f0 >> (31 - scl);
+			t1 = f1 >> (31 - scl);
+
+			uint32_t F0 = F[(u << 1) + 0];
+			uint32_t F1 = F[(u << 1) + 1];
+			int64_t z0 = (int64_t)F0 + cc0
+				- (int64_t)fs0 * (int64_t)k0
+				+ (int64_t)fs1 * (int64_t)k1;
+			int64_t z1 = (int64_t)F1 + cc1
+				- (int64_t)fs0 * (int64_t)k1
+				- (int64_t)fs1 * (int64_t)k0;
+			F[(u << 1) + 0] = (uint32_t)z0 & 0x7FFFFFFF;
+			F[(u << 1) + 1] = (uint32_t)z1 & 0x7FFFFFFF;
+			cc0 = z0 >> 31;
+			cc1 = z1 >> 31;
+		}
+		return;
+	}
+
+	case 2: {
+#if NTRUGEN_AVX2
+		__m128i xt = _mm_setzero_si128();
+		__m128i xsignf = _mm_loadu_si128(
+			(const __m128i *)(f + (flen << 2) - 4));
+		xsignf = _mm_srli_epi32(
+			_mm_srai_epi32(_mm_slli_epi32(xsignf, 1), 31), 1);
+		int32_t k0 = k[0];
+		int32_t k1 = k[1];
+		int32_t k2 = k[2];
+		int32_t k3 = k[3];
+		__m256i yk0 = _mm256_setr_epi32(-k0, 0, -k1, 0, -k2, 0, -k3, 0);
+		__m256i yk1 = _mm256_setr_epi32(+k3, 0, -k0, 0, -k1, 0, -k2, 0);
+		__m256i yk2 = _mm256_setr_epi32(+k2, 0, +k3, 0, -k0, 0, -k1, 0);
+		__m256i yk3 = _mm256_setr_epi32(+k1, 0, +k2, 0, +k3, 0, -k0, 0);
+		__m128i xscl = _mm_cvtsi32_si128(scl);
+		__m128i xnscl = _mm_cvtsi32_si128(31 - scl);
+		__m256i ycc = _mm256_setzero_si256();
+		__m128i x31 = _mm_set1_epi32(0x7FFFFFFF);
+		__m256i y31 = _mm256_set1_epi32(0x7FFFFFFF);
+		__m256i y31lo = _mm256_set1_epi64x(0x7FFFFFFF);
+		for (size_t u = 0; u < Flen; u ++) {
+			/*
+			 * Next word, shifted.
+			 */
+			__m128i xf;
+			if (u < flen) {
+				xf = _mm_loadu_si128((__m128i *)(f + (u << 2)));
+			} else {
+				xf = xsignf;
+			}
+			__m128i xfs = _mm_or_si128(xt,
+				_mm_and_si128(_mm_sll_epi32(xf, xscl), x31));
+			xt = _mm_srl_epi32(xf, xnscl);
+
+			__m256i yfs0 = _mm256_broadcastd_epi32(xfs);
+			__m256i yfs1 = _mm256_broadcastd_epi32(
+				_mm_bsrli_si128(xfs, 4));
+			__m256i yfs2 = _mm256_broadcastd_epi32(
+				_mm_bsrli_si128(xfs, 8));
+			__m256i yfs3 = _mm256_broadcastd_epi32(
+				_mm_bsrli_si128(xfs, 12));
+
+			__m256i yF = _mm256_castsi128_si256(
+				_mm_loadu_si128((__m128i *)(F + (u << 2))));
+			yF = _mm256_shuffle_epi32(
+				_mm256_permute4x64_epi64(yF, 0x10), 0x10);
+			yF = _mm256_and_si256(yF, y31lo);
+
+			__m256i yv0 = _mm256_mul_epi32(yfs0, yk0);
+			__m256i yv1 = _mm256_mul_epi32(yfs1, yk1);
+			__m256i yv2 = _mm256_mul_epi32(yfs2, yk2);
+			__m256i yv3 = _mm256_mul_epi32(yfs3, yk3);
+			__m256i yz = _mm256_add_epi64(
+				_mm256_add_epi64(
+					_mm256_add_epi64(yv0, yv1),
+					_mm256_add_epi64(yv2, yv3)),
+				_mm256_add_epi64(ycc, yF));
+			ycc = _mm256_blend_epi32(
+				_mm256_srli_epi64(yz, 31),
+				_mm256_srai_epi32(yz, 31), 0xAA);
+			yz = _mm256_shuffle_epi32(
+				_mm256_and_si256(yz, y31), 0x88);
+			yz = _mm256_permute4x64_epi64(yz, 0x88);
+			__m128i xF = _mm256_castsi256_si128(yz);
+			_mm_storeu_si128((__m128i *)(F + (u << 2)), xF);
+		}
+#else // NTRUGEN_AVX2
+		uint32_t t0 = 0;
+		uint32_t t1 = 0;
+		uint32_t t2 = 0;
+		uint32_t t3 = 0;
+		uint32_t signf0 = -(f[(flen << 2) - 4] >> 30) >> 1;
+		uint32_t signf1 = -(f[(flen << 2) - 3] >> 30) >> 1;
+		uint32_t signf2 = -(f[(flen << 2) - 2] >> 30) >> 1;
+		uint32_t signf3 = -(f[(flen << 2) - 1] >> 30) >> 1;
+		int32_t k0 = k[0];
+		int32_t k1 = k[1];
+		int32_t k2 = k[2];
+		int32_t k3 = k[3];
+		int64_t cc0 = 0;
+		int64_t cc1 = 0;
+		int64_t cc2 = 0;
+		int64_t cc3 = 0;
+		for (size_t u = 0; u < Flen; u ++) {
+			/*
+			 * Next word, shifted.
+			 */
+			uint32_t f0, f1, f2, f3;
+			if (u < flen) {
+				f0 = f[(u << 2) + 0];
+				f1 = f[(u << 2) + 1];
+				f2 = f[(u << 2) + 2];
+				f3 = f[(u << 2) + 3];
+			} else {
+				f0 = signf0;
+				f1 = signf1;
+				f2 = signf2;
+				f3 = signf3;
+			}
+			uint32_t fs0 = ((f0 << scl) & 0x7FFFFFFF) | t0;
+			uint32_t fs1 = ((f1 << scl) & 0x7FFFFFFF) | t1;
+			uint32_t fs2 = ((f2 << scl) & 0x7FFFFFFF) | t2;
+			uint32_t fs3 = ((f3 << scl) & 0x7FFFFFFF) | t3;
+			t0 = f0 >> (31 - scl);
+			t1 = f1 >> (31 - scl);
+			t2 = f2 >> (31 - scl);
+			t3 = f3 >> (31 - scl);
+
+			uint32_t F0 = F[(u << 2) + 0];
+			uint32_t F1 = F[(u << 2) + 1];
+			uint32_t F2 = F[(u << 2) + 2];
+			uint32_t F3 = F[(u << 2) + 3];
+			int64_t z0 = (int64_t)F0 + cc0
+				- (int64_t)fs0 * (int64_t)k0
+				+ (int64_t)fs1 * (int64_t)k3
+				+ (int64_t)fs2 * (int64_t)k2
+				+ (int64_t)fs3 * (int64_t)k1;
+			int64_t z1 = (int64_t)F1 + cc1
+				- (int64_t)fs0 * (int64_t)k1
+				- (int64_t)fs1 * (int64_t)k0
+				+ (int64_t)fs2 * (int64_t)k3
+				+ (int64_t)fs3 * (int64_t)k2;
+			int64_t z2 = (int64_t)F2 + cc2
+				- (int64_t)fs0 * (int64_t)k2
+				- (int64_t)fs1 * (int64_t)k1
+				- (int64_t)fs2 * (int64_t)k0
+				+ (int64_t)fs3 * (int64_t)k3;
+			int64_t z3 = (int64_t)F3 + cc3
+				- (int64_t)fs0 * (int64_t)k3
+				- (int64_t)fs1 * (int64_t)k2
+				- (int64_t)fs2 * (int64_t)k1
+				- (int64_t)fs3 * (int64_t)k0;
+			F[(u << 2) + 0] = (uint32_t)z0 & 0x7FFFFFFF;
+			F[(u << 2) + 1] = (uint32_t)z1 & 0x7FFFFFFF;
+			F[(u << 2) + 2] = (uint32_t)z2 & 0x7FFFFFFF;
+			F[(u << 2) + 3] = (uint32_t)z3 & 0x7FFFFFFF;
+			cc0 = z0 >> 31;
+			cc1 = z1 >> 31;
+			cc2 = z2 >> 31;
+			cc3 = z3 >> 31;
+		}
+#endif // NTRUGEN_AVX2
+		return;
+	}
+
+	case 3: {
+#if NTRUGEN_AVX2
+		__m256i yt = _mm256_setzero_si256();
+		__m256i ysignf = _mm256_loadu_si256(
+			(const __m256i *)(f + (flen << 3) - 8));
+		ysignf = _mm256_srli_epi32(
+			_mm256_srai_epi32(_mm256_slli_epi32(ysignf, 1), 31), 1);
+
+		int32_t k0 = k[0];
+		int32_t k1 = k[1];
+		int32_t k2 = k[2];
+		int32_t k3 = k[3];
+		int32_t k4 = k[4];
+		int32_t k5 = k[5];
+		int32_t k6 = k[6];
+		int32_t k7 = k[7];
+		__m256i yk0l = _mm256_setr_epi32(
+			-k0, -k1, -k2, -k3, -k4, -k5, -k6, -k7);
+		__m256i yk1l = _mm256_setr_epi32(
+			+k7, -k0, -k1, -k2, -k3, -k4, -k5, -k6);
+		__m256i yk2l = _mm256_setr_epi32(
+			+k6, +k7, -k0, -k1, -k2, -k3, -k4, -k5);
+		__m256i yk3l = _mm256_setr_epi32(
+			+k5, +k6, +k7, -k0, -k1, -k2, -k3, -k4);
+		__m256i yk4l = _mm256_setr_epi32(
+			+k4, +k5, +k6, +k7, -k0, -k1, -k2, -k3);
+		__m256i yk5l = _mm256_setr_epi32(
+			+k3, +k4, +k5, +k6, +k7, -k0, -k1, -k2);
+		__m256i yk6l = _mm256_setr_epi32(
+			+k2, +k3, +k4, +k5, +k6, +k7, -k0, -k1);
+		__m256i yk7l = _mm256_setr_epi32(
+			+k1, +k2, +k3, +k4, +k5, +k6, +k7, -k0);
+		__m256i yk0h = _mm256_srli_epi64(yk0l, 32);
+		__m256i yk1h = _mm256_srli_epi64(yk1l, 32);
+		__m256i yk2h = _mm256_srli_epi64(yk2l, 32);
+		__m256i yk3h = _mm256_srli_epi64(yk3l, 32);
+		__m256i yk4h = _mm256_srli_epi64(yk4l, 32);
+		__m256i yk5h = _mm256_srli_epi64(yk5l, 32);
+		__m256i yk6h = _mm256_srli_epi64(yk6l, 32);
+		__m256i yk7h = _mm256_srli_epi64(yk7l, 32);
+
+		__m128i xscl = _mm_cvtsi32_si128(scl);
+		__m128i xnscl = _mm_cvtsi32_si128(31 - scl);
+		__m256i ycc0 = _mm256_setzero_si256();
+		__m256i ycc1 = _mm256_setzero_si256();
+		__m256i y31 = _mm256_set1_epi32(0x7FFFFFFF);
+		__m256i y31lo = _mm256_set1_epi64x(0x7FFFFFFF);
+		for (size_t u = 0; u < Flen; u ++) {
+			/*
+			 * Next word, shifted.
+			 */
+			__m256i yf;
+			if (u < flen) {
+				yf = _mm256_loadu_si256(
+					(const __m256i *)(f + (u << 3)));
+			} else {
+				yf = ysignf;
+			}
+			__m256i yfs = _mm256_or_si256(yt,
+				_mm256_and_si256(
+					_mm256_sll_epi32(yf, xscl), y31));
+			yt = _mm256_srl_epi32(yf, xnscl);
+
+			__m128i xfs0 = _mm256_castsi256_si128(yfs);
+			__m128i xfs1 = _mm256_extracti128_si256(yfs, 1);
+			__m256i yfs0 = _mm256_broadcastd_epi32(xfs0);
+			__m256i yfs1 = _mm256_broadcastd_epi32(
+				_mm_bsrli_si128(xfs0, 4));
+			__m256i yfs2 = _mm256_broadcastd_epi32(
+				_mm_bsrli_si128(xfs0, 8));
+			__m256i yfs3 = _mm256_broadcastd_epi32(
+				_mm_bsrli_si128(xfs0, 12));
+			__m256i yfs4 = _mm256_broadcastd_epi32(xfs1);
+			__m256i yfs5 = _mm256_broadcastd_epi32(
+				_mm_bsrli_si128(xfs1, 4));
+			__m256i yfs6 = _mm256_broadcastd_epi32(
+				_mm_bsrli_si128(xfs1, 8));
+			__m256i yfs7 = _mm256_broadcastd_epi32(
+				_mm_bsrli_si128(xfs1, 12));
+
+			__m256i yF = _mm256_loadu_si256(
+				(const __m256i *)(F + (u << 3)));
+			__m256i yF0 = _mm256_and_si256(yF, y31lo);
+			__m256i yF1 = _mm256_srli_epi64(yF, 32);
+
+			__m256i yv0l = _mm256_mul_epi32(yfs0, yk0l);
+			__m256i yv0h = _mm256_mul_epi32(yfs0, yk0h);
+			__m256i yv1l = _mm256_mul_epi32(yfs1, yk1l);
+			__m256i yv1h = _mm256_mul_epi32(yfs1, yk1h);
+			__m256i yv2l = _mm256_mul_epi32(yfs2, yk2l);
+			__m256i yv2h = _mm256_mul_epi32(yfs2, yk2h);
+			__m256i yv3l = _mm256_mul_epi32(yfs3, yk3l);
+			__m256i yv3h = _mm256_mul_epi32(yfs3, yk3h);
+			__m256i yv4l = _mm256_mul_epi32(yfs4, yk4l);
+			__m256i yv4h = _mm256_mul_epi32(yfs4, yk4h);
+			__m256i yv5l = _mm256_mul_epi32(yfs5, yk5l);
+			__m256i yv5h = _mm256_mul_epi32(yfs5, yk5h);
+			__m256i yv6l = _mm256_mul_epi32(yfs6, yk6l);
+			__m256i yv6h = _mm256_mul_epi32(yfs6, yk6h);
+			__m256i yv7l = _mm256_mul_epi32(yfs7, yk7l);
+			__m256i yv7h = _mm256_mul_epi32(yfs7, yk7h);
+
+			__m256i yz0 = _mm256_add_epi64(
+				_mm256_add_epi64(ycc0, yF0),
+				_mm256_add_epi64(
+					_mm256_add_epi64(
+						_mm256_add_epi64(yv0l, yv1l),
+						_mm256_add_epi64(yv2l, yv3l)),
+					_mm256_add_epi64(
+						_mm256_add_epi64(yv4l, yv5l),
+						_mm256_add_epi64(yv6l, yv7l))));
+			__m256i yz1 = _mm256_add_epi64(
+				_mm256_add_epi64(ycc1, yF1),
+				_mm256_add_epi64(
+					_mm256_add_epi64(
+						_mm256_add_epi64(yv0h, yv1h),
+						_mm256_add_epi64(yv2h, yv3h)),
+					_mm256_add_epi64(
+						_mm256_add_epi64(yv4h, yv5h),
+						_mm256_add_epi64(yv6h, yv7h))));
+			ycc0 = _mm256_blend_epi32(
+				_mm256_srli_epi64(yz0, 31),
+				_mm256_srai_epi32(yz0, 31), 0xAA);
+			ycc1 = _mm256_blend_epi32(
+				_mm256_srli_epi64(yz1, 31),
+				_mm256_srai_epi32(yz1, 31), 0xAA);
+			yF = _mm256_or_si256(
+				_mm256_and_si256(yz0, y31lo),
+				_mm256_slli_epi64(
+					_mm256_and_si256(yz1, y31lo), 32));
+			_mm256_storeu_si256((__m256i *)(F + (u << 3)), yF);
+		}
+#else // NTRUGEN_AVX2
+		uint32_t t0 = 0;
+		uint32_t t1 = 0;
+		uint32_t t2 = 0;
+		uint32_t t3 = 0;
+		uint32_t t4 = 0;
+		uint32_t t5 = 0;
+		uint32_t t6 = 0;
+		uint32_t t7 = 0;
+		uint32_t signf0 = -(f[(flen << 3) - 8] >> 30) >> 1;
+		uint32_t signf1 = -(f[(flen << 3) - 7] >> 30) >> 1;
+		uint32_t signf2 = -(f[(flen << 3) - 6] >> 30) >> 1;
+		uint32_t signf3 = -(f[(flen << 3) - 5] >> 30) >> 1;
+		uint32_t signf4 = -(f[(flen << 3) - 4] >> 30) >> 1;
+		uint32_t signf5 = -(f[(flen << 3) - 3] >> 30) >> 1;
+		uint32_t signf6 = -(f[(flen << 3) - 2] >> 30) >> 1;
+		uint32_t signf7 = -(f[(flen << 3) - 1] >> 30) >> 1;
+		int32_t k0 = k[0];
+		int32_t k1 = k[1];
+		int32_t k2 = k[2];
+		int32_t k3 = k[3];
+		int32_t k4 = k[4];
+		int32_t k5 = k[5];
+		int32_t k6 = k[6];
+		int32_t k7 = k[7];
+		int64_t cc0 = 0;
+		int64_t cc1 = 0;
+		int64_t cc2 = 0;
+		int64_t cc3 = 0;
+		int64_t cc4 = 0;
+		int64_t cc5 = 0;
+		int64_t cc6 = 0;
+		int64_t cc7 = 0;
+		for (size_t u = 0; u < Flen; u ++) {
+			/*
+			 * Next word, shifted.
+			 */
+			uint32_t f0, f1, f2, f3, f4, f5, f6, f7;
+			if (u < flen) {
+				f0 = f[(u << 3) + 0];
+				f1 = f[(u << 3) + 1];
+				f2 = f[(u << 3) + 2];
+				f3 = f[(u << 3) + 3];
+				f4 = f[(u << 3) + 4];
+				f5 = f[(u << 3) + 5];
+				f6 = f[(u << 3) + 6];
+				f7 = f[(u << 3) + 7];
+			} else {
+				f0 = signf0;
+				f1 = signf1;
+				f2 = signf2;
+				f3 = signf3;
+				f4 = signf4;
+				f5 = signf5;
+				f6 = signf6;
+				f7 = signf7;
+			}
+			uint32_t fs0 = ((f0 << scl) & 0x7FFFFFFF) | t0;
+			uint32_t fs1 = ((f1 << scl) & 0x7FFFFFFF) | t1;
+			uint32_t fs2 = ((f2 << scl) & 0x7FFFFFFF) | t2;
+			uint32_t fs3 = ((f3 << scl) & 0x7FFFFFFF) | t3;
+			uint32_t fs4 = ((f4 << scl) & 0x7FFFFFFF) | t4;
+			uint32_t fs5 = ((f5 << scl) & 0x7FFFFFFF) | t5;
+			uint32_t fs6 = ((f6 << scl) & 0x7FFFFFFF) | t6;
+			uint32_t fs7 = ((f7 << scl) & 0x7FFFFFFF) | t7;
+			t0 = f0 >> (31 - scl);
+			t1 = f1 >> (31 - scl);
+			t2 = f2 >> (31 - scl);
+			t3 = f3 >> (31 - scl);
+			t4 = f4 >> (31 - scl);
+			t5 = f5 >> (31 - scl);
+			t6 = f6 >> (31 - scl);
+			t7 = f7 >> (31 - scl);
+
+			uint32_t F0 = F[(u << 3) + 0];
+			uint32_t F1 = F[(u << 3) + 1];
+			uint32_t F2 = F[(u << 3) + 2];
+			uint32_t F3 = F[(u << 3) + 3];
+			uint32_t F4 = F[(u << 3) + 4];
+			uint32_t F5 = F[(u << 3) + 5];
+			uint32_t F6 = F[(u << 3) + 6];
+			uint32_t F7 = F[(u << 3) + 7];
+			int64_t z0 = (int64_t)F0 + cc0
+				- (int64_t)fs0 * (int64_t)k0
+				+ (int64_t)fs1 * (int64_t)k7
+				+ (int64_t)fs2 * (int64_t)k6
+				+ (int64_t)fs3 * (int64_t)k5
+				+ (int64_t)fs4 * (int64_t)k4
+				+ (int64_t)fs5 * (int64_t)k3
+				+ (int64_t)fs6 * (int64_t)k2
+				+ (int64_t)fs7 * (int64_t)k1;
+			int64_t z1 = (int64_t)F1 + cc1
+				- (int64_t)fs0 * (int64_t)k1
+				- (int64_t)fs1 * (int64_t)k0
+				+ (int64_t)fs2 * (int64_t)k7
+				+ (int64_t)fs3 * (int64_t)k6
+				+ (int64_t)fs4 * (int64_t)k5
+				+ (int64_t)fs5 * (int64_t)k4
+				+ (int64_t)fs6 * (int64_t)k3
+				+ (int64_t)fs7 * (int64_t)k2;
+			int64_t z2 = (int64_t)F2 + cc2
+				- (int64_t)fs0 * (int64_t)k2
+				- (int64_t)fs1 * (int64_t)k1
+				- (int64_t)fs2 * (int64_t)k0
+				+ (int64_t)fs3 * (int64_t)k7
+				+ (int64_t)fs4 * (int64_t)k6
+				+ (int64_t)fs5 * (int64_t)k5
+				+ (int64_t)fs6 * (int64_t)k4
+				+ (int64_t)fs7 * (int64_t)k3;
+			int64_t z3 = (int64_t)F3 + cc3
+				- (int64_t)fs0 * (int64_t)k3
+				- (int64_t)fs1 * (int64_t)k2
+				- (int64_t)fs2 * (int64_t)k1
+				- (int64_t)fs3 * (int64_t)k0
+				+ (int64_t)fs4 * (int64_t)k7
+				+ (int64_t)fs5 * (int64_t)k6
+				+ (int64_t)fs6 * (int64_t)k5
+				+ (int64_t)fs7 * (int64_t)k4;
+			int64_t z4 = (int64_t)F4 + cc4
+				- (int64_t)fs0 * (int64_t)k4
+				- (int64_t)fs1 * (int64_t)k3
+				- (int64_t)fs2 * (int64_t)k2
+				- (int64_t)fs3 * (int64_t)k1
+				- (int64_t)fs4 * (int64_t)k0
+				+ (int64_t)fs5 * (int64_t)k7
+				+ (int64_t)fs6 * (int64_t)k6
+				+ (int64_t)fs7 * (int64_t)k5;
+			int64_t z5 = (int64_t)F5 + cc5
+				- (int64_t)fs0 * (int64_t)k5
+				- (int64_t)fs1 * (int64_t)k4
+				- (int64_t)fs2 * (int64_t)k3
+				- (int64_t)fs3 * (int64_t)k2
+				- (int64_t)fs4 * (int64_t)k1
+				- (int64_t)fs5 * (int64_t)k0
+				+ (int64_t)fs6 * (int64_t)k7
+				+ (int64_t)fs7 * (int64_t)k6;
+			int64_t z6 = (int64_t)F6 + cc6
+				- (int64_t)fs0 * (int64_t)k6
+				- (int64_t)fs1 * (int64_t)k5
+				- (int64_t)fs2 * (int64_t)k4
+				- (int64_t)fs3 * (int64_t)k3
+				- (int64_t)fs4 * (int64_t)k2
+				- (int64_t)fs5 * (int64_t)k1
+				- (int64_t)fs6 * (int64_t)k0
+				+ (int64_t)fs7 * (int64_t)k7;
+			int64_t z7 = (int64_t)F7 + cc7
+				- (int64_t)fs0 * (int64_t)k7
+				- (int64_t)fs1 * (int64_t)k6
+				- (int64_t)fs2 * (int64_t)k5
+				- (int64_t)fs3 * (int64_t)k4
+				- (int64_t)fs4 * (int64_t)k3
+				- (int64_t)fs5 * (int64_t)k2
+				- (int64_t)fs6 * (int64_t)k1
+				- (int64_t)fs7 * (int64_t)k0;
+			F[(u << 3) + 0] = (uint32_t)z0 & 0x7FFFFFFF;
+			F[(u << 3) + 1] = (uint32_t)z1 & 0x7FFFFFFF;
+			F[(u << 3) + 2] = (uint32_t)z2 & 0x7FFFFFFF;
+			F[(u << 3) + 3] = (uint32_t)z3 & 0x7FFFFFFF;
+			F[(u << 3) + 4] = (uint32_t)z4 & 0x7FFFFFFF;
+			F[(u << 3) + 5] = (uint32_t)z5 & 0x7FFFFFFF;
+			F[(u << 3) + 6] = (uint32_t)z6 & 0x7FFFFFFF;
+			F[(u << 3) + 7] = (uint32_t)z7 & 0x7FFFFFFF;
+			cc0 = z0 >> 31;
+			cc1 = z1 >> 31;
+			cc2 = z2 >> 31;
+			cc3 = z3 >> 31;
+			cc4 = z4 >> 31;
+			cc5 = z5 >> 31;
+			cc6 = z6 >> 31;
+			cc7 = z7 >> 31;
+		}
+#endif // NTRUGEN_AVX2
+		return;
+	}
+	}
+	size_t n = (size_t)1 << logn;
+	for (size_t u = 0; u < n; u ++) {
+		int32_t kf = -k[u];
+		uint32_t *x = F + u;
+		for (size_t v = 0; v < n; v ++) {
+			zint_add_scaled_mul_small(
+				x, Flen, f + v, flen, n, kf, 0, scl);
+			if (u + v == n - 1) {
+				x = F;
+				kf = -kf;
+			} else {
+				x ++;
+			}
+		}
+	}
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+poly_sub_scaled_ntt(unsigned logn, uint32_t *restrict F, size_t Flen,
+	const uint32_t *restrict f, size_t flen,
+	const int32_t *restrict k, uint32_t sc, uint32_t *restrict tmp)
+{
+#if NTRUGEN_AVX2
+	/* Fail-safe. */
+	if (logn < 3) {
+		poly_sub_scaled(logn, F, Flen, f, flen, k, sc);
+		return;
+	}
+#endif // NTRUGEN_AVX2
+
+	size_t n = (size_t)1 << logn;
+	size_t tlen = flen + 1;
+	uint32_t *gm = tmp;
+	uint32_t *igm = gm + n;
+	uint32_t *fk = igm + n;
+	uint32_t *t1 = fk + (tlen << logn);
+	uint32_t sch, scl;
+	DIVREM31(sch, scl, sc);
+
+	/*
+	 * Compute k*f in fk[], in RNS notation.
+	 * f is assumed to be already in RNS+NTT over flen+1 words.
+	 */
+	for (size_t u = 0; u < tlen; u ++) {
+		uint32_t p = PRIMES[u].p;
+		uint32_t p0i = PRIMES[u].p0i;
+		uint32_t R2 = PRIMES[u].R2;
+		mp_mkgmigm(logn, gm, igm, PRIMES[u].g, PRIMES[u].ig, p, p0i);
+#if NTRUGEN_AVX2
+		__m256i yp = _mm256_set1_epi32(p);
+		for (size_t v = 0; v < n; v += 8) {
+			__m256i yk = _mm256_loadu_si256((__m256i *)(k + v));
+			_mm256_storeu_si256((__m256i *)(t1 + v),
+				mp_set_x8(yk, yp));
+		}
+#else // NTRUGEN_AVX2
+		for (size_t v = 0; v < n; v ++) {
+			t1[v] = mp_set(k[v], p);
+		}
+#endif // NTRUGEN_AVX2
+		mp_NTT(logn, t1, gm, p, p0i);
+
+		const uint32_t *fs = f + (u << logn);
+		uint32_t *ff = fk + (u << logn);
+#if NTRUGEN_AVX2
+		__m256i yp0i = _mm256_set1_epi32(p0i);
+		__m256i yR2 = _mm256_set1_epi32(R2);
+		for (size_t v = 0; v < n; v += 8) {
+			__m256i y1 = _mm256_loadu_si256((__m256i *)(t1 + v));
+			__m256i y2 = _mm256_loadu_si256((__m256i *)(fs + v));
+			_mm256_storeu_si256((__m256i *)(ff + v),
+				mp_montymul_x8(
+					mp_montymul_x8(y1, y2, yp, yp0i),
+					yR2, yp, yp0i));
+		}
+#else // NTRUGEN_AVX2
+		for (size_t v = 0; v < n; v ++) {
+			ff[v] = mp_montymul(
+				mp_montymul(t1[v], fs[v], p, p0i), R2, p, p0i);
+		}
+#endif // NTRUGEN_AVX2
+		mp_iNTT(logn, ff, igm, p, p0i);
+	}
+
+	/*
+	 * Rebuild k*f.
+	 */
+	zint_rebuild_CRT(fk, tlen, n, 1, 1, t1);
+
+	/*
+	 * Subtract k*f, scaled, from F.
+	 */
+	for (size_t u = 0; u < n; u ++) {
+		zint_sub_scaled(F + u, Flen, fk + u, tlen, n, sch, scl);
+	}
+}
+
+/* see ng_inner.h */
+void
+poly_sub_kfg_scaled_depth1(unsigned logn_top,
+	uint32_t *restrict F, uint32_t *restrict G, size_t FGlen,
+	uint32_t *restrict k, uint32_t sc,
+	const int8_t *restrict f, const int8_t *restrict g,
+	uint32_t *restrict tmp)
+{
+	unsigned logn = logn_top - 1;
+	size_t n = (size_t)1 << logn;
+	size_t hn = n >> 1;
+	uint32_t *gm = tmp;
+	uint32_t *t1 = gm + n;
+	uint32_t *t2 = t1 + n;
+
+	/*
+	 * Step 1: convert F and G to RNS. Since FGlen is equal to 1 or 2,
+	 * we do it with some specialized code. We assume that the RNS
+	 * representation does not lose information (i.e. each signed
+	 * coefficient is lower than (p0*p1)/2, with FGlen = 2 and the two
+	 * prime moduli are p0 and p1).
+	 */
+	if (FGlen == 1) {
+		uint32_t p = PRIMES[0].p;
+		for (size_t u = 0; u < n; u ++) {
+			uint32_t xf = F[u];
+			uint32_t xg = G[u];
+			xf |= (xf & 0x40000000) << 1;
+			xg |= (xg & 0x40000000) << 1;
+			F[u] = mp_set(*(int32_t *)&xf, p);
+			G[u] = mp_set(*(int32_t *)&xg, p);
+		}
+	} else {
+		uint32_t p0 = PRIMES[0].p;
+		uint32_t p0_0i = PRIMES[0].p0i;
+		uint32_t z0 = mp_half(PRIMES[0].R2, p0);
+		uint32_t p1 = PRIMES[1].p;
+		uint32_t p1_0i = PRIMES[1].p0i;
+		uint32_t z1 = mp_half(PRIMES[1].R2, p1);
+		for (size_t u = 0; u < n; u ++) {
+			uint32_t xl, xh, yl0, yh0, r0, yl1, yh1, r1;
+
+			xl = F[u];
+			xh = F[u + n] | ((F[u + n] & 0x40000000) << 1);
+			yl0 = xl - (p0 & ~tbmask(xl - p0));
+			yh0 = mp_set(*(int32_t *)&xh, p0);
+			r0 = mp_add(yl0, mp_montymul(yh0, z0, p0, p0_0i), p0);
+			yl1 = xl - (p1 & ~tbmask(xl - p1));
+			yh1 = mp_set(*(int32_t *)&xh, p1);
+			r1 = mp_add(yl1, mp_montymul(yh1, z1, p1, p1_0i), p1);
+			F[u] = r0;
+			F[u + n] = r1;
+
+			xl = G[u];
+			xh = G[u + n] | ((G[u + n] & 0x40000000) << 1);
+			yl0 = xl - (p0 & ~tbmask(xl - p0));
+			yh0 = mp_set(*(int32_t *)&xh, p0);
+			r0 = mp_add(yl0, mp_montymul(yh0, z0, p0, p0_0i), p0);
+			yl1 = xl - (p1 & ~tbmask(xl - p1));
+			yh1 = mp_set(*(int32_t *)&xh, p1);
+			r1 = mp_add(yl1, mp_montymul(yh1, z1, p1, p1_0i), p1);
+			G[u] = r0;
+			G[u + n] = r1;
+		}
+	}
+
+	/*
+	 * Step 2: for FGlen small primes, convert F and G to RNS+NTT,
+	 * and subtract (2^sc)*(ft,gt). The (ft,gt) polynomials are computed
+	 * in RNS+NTT dynamically.
+	 */
+	for (size_t u = 0; u < FGlen; u ++) {
+		uint32_t p = PRIMES[u].p;
+		uint32_t p0i = PRIMES[u].p0i;
+		uint32_t R2 = PRIMES[u].R2;
+		uint32_t R3 = mp_montymul(R2, R2, p, p0i);
+		mp_mkgm(logn, gm, PRIMES[u].g, p, p0i);
+
+		/*
+		 * k <- (2^sc)*k (and into NTT).
+		 */
+		uint32_t scv = mp_montymul(
+			(uint32_t)1 << (sc & 31), R2, p, p0i);
+		for (uint32_t m = sc >> 5; m > 0; m --) {
+			scv = mp_montymul(scv, R2, p, p0i);
+		}
+		for (size_t v = 0; v < n; v ++) {
+			uint32_t x = mp_set(*(int32_t *)&k[v], p);
+			k[v] = mp_montymul(scv, x, p, p0i);
+		}
+		mp_NTT(logn, k, gm, p, p0i);
+
+		/*
+		 * Convert F and G to NTT.
+		 */
+		uint32_t *Fu = F + (u << logn);
+		uint32_t *Gu = G + (u << logn);
+		mp_NTT(logn, Fu, gm, p, p0i);
+		mp_NTT(logn, Gu, gm, p, p0i);
+
+		/*
+		 * Given the top-level f, we obtain ft = N(f) (the f at
+		 * depth 1) with:
+		 *    f = f_e(X^2) + X*f_o(X^2)
+		 * with f_e and f_o being modulo X^n+1. Then:
+		 *    N(f) = f_e^2 - X*f_o^2
+		 * The NTT representation of X is obtained from the gm[] tab:
+		 *    NTT(X)[2*j + 0] = gm[j + n/2]
+		 *    NTT(X)[2*j + 1] = -NTT(X)[2*j + 0]
+		 * Note that the values in gm[] are in Montgomery
+		 * representation.
+		 */
+		for (size_t v = 0; v < n; v ++) {
+			t1[v] = mp_set(f[(v << 1) + 0], p);
+			t2[v] = mp_set(f[(v << 1) + 1], p);
+		}
+		mp_NTT(logn, t1, gm, p, p0i);
+		mp_NTT(logn, t2, gm, p, p0i);
+		for (size_t v = 0; v < hn; v ++) {
+			uint32_t xe0 = t1[(v << 1) + 0];
+			uint32_t xe1 = t1[(v << 1) + 1];
+			uint32_t xo0 = t2[(v << 1) + 0];
+			uint32_t xo1 = t2[(v << 1) + 1];
+			uint32_t xv0 = gm[hn + v];
+			uint32_t xv1 = p - xv0;
+			xe0 = mp_montymul(xe0, xe0, p, p0i);
+			xe1 = mp_montymul(xe1, xe1, p, p0i);
+			xo0 = mp_montymul(xo0, xo0, p, p0i);
+			xo1 = mp_montymul(xo1, xo1, p, p0i);
+			uint32_t xf0 = mp_sub(xe0,
+				mp_montymul(xo0, xv0, p, p0i), p);
+			uint32_t xf1 = mp_sub(xe1,
+				mp_montymul(xo1, xv1, p, p0i), p);
+
+			uint32_t xkf0 = mp_montymul(
+				mp_montymul(xf0, k[(v << 1) + 0], p, p0i),
+				R3, p, p0i);
+			uint32_t xkf1 = mp_montymul(
+				mp_montymul(xf1, k[(v << 1) + 1], p, p0i),
+				R3, p, p0i);
+			Fu[(v << 1) + 0] = mp_sub(Fu[(v << 1) + 0], xkf0, p);
+			Fu[(v << 1) + 1] = mp_sub(Fu[(v << 1) + 1], xkf1, p);
+		}
+
+		/*
+		 * Same treatment for G and gt.
+		 */
+		for (size_t v = 0; v < n; v ++) {
+			t1[v] = mp_set(g[(v << 1) + 0], p);
+			t2[v] = mp_set(g[(v << 1) + 1], p);
+		}
+		mp_NTT(logn, t1, gm, p, p0i);
+		mp_NTT(logn, t2, gm, p, p0i);
+		for (size_t v = 0; v < hn; v ++) {
+			uint32_t xe0 = t1[(v << 1) + 0];
+			uint32_t xe1 = t1[(v << 1) + 1];
+			uint32_t xo0 = t2[(v << 1) + 0];
+			uint32_t xo1 = t2[(v << 1) + 1];
+			uint32_t xv0 = gm[hn + v];
+			uint32_t xv1 = p - xv0;
+			xe0 = mp_montymul(xe0, xe0, p, p0i);
+			xe1 = mp_montymul(xe1, xe1, p, p0i);
+			xo0 = mp_montymul(xo0, xo0, p, p0i);
+			xo1 = mp_montymul(xo1, xo1, p, p0i);
+			uint32_t xg0 = mp_sub(xe0,
+				mp_montymul(xo0, xv0, p, p0i), p);
+			uint32_t xg1 = mp_sub(xe1,
+				mp_montymul(xo1, xv1, p, p0i), p);
+
+			uint32_t xkg0 = mp_montymul(
+				mp_montymul(xg0, k[(v << 1) + 0], p, p0i),
+				R3, p, p0i);
+			uint32_t xkg1 = mp_montymul(
+				mp_montymul(xg1, k[(v << 1) + 1], p, p0i),
+				R3, p, p0i);
+			Gu[(v << 1) + 0] = mp_sub(Gu[(v << 1) + 0], xkg0, p);
+			Gu[(v << 1) + 1] = mp_sub(Gu[(v << 1) + 1], xkg1, p);
+		}
+
+		/*
+		 * Convert back F and G to RNS.
+		 */
+		mp_mkigm(logn, t1, PRIMES[u].ig, p, p0i);
+		mp_iNTT(logn, Fu, t1, p, p0i);
+		mp_iNTT(logn, Gu, t1, p, p0i);
+
+		/*
+		 * We replaced k (plain 32-bit) with (2^sc)*k (NTT). We must
+		 * put it back to its initial value if there should be another
+		 * iteration.
+		 */
+		if ((u + 1) < FGlen) {
+			mp_iNTT(logn, k, t1, p, p0i);
+			scv = (uint32_t)1 << (-sc & 31);
+			for (uint32_t m = sc >> 5; m > 0; m --) {
+				scv = mp_montymul(scv, 1, p, p0i);
+			}
+			for (size_t v = 0; v < n; v ++) {
+				k[v] = (uint32_t)mp_norm(
+					mp_montymul(scv, k[v], p, p0i), p);
+			}
+		}
+	}
+
+	/*
+	 * Output F and G are in RNS (non-NTT), but we want plain integers.
+	 */
+	if (FGlen == 1) {
+		uint32_t p = PRIMES[0].p;
+		for (size_t u = 0; u < n; u ++) {
+			F[u] = (uint32_t)mp_norm(F[u], p) & 0x7FFFFFFF;
+			G[u] = (uint32_t)mp_norm(G[u], p) & 0x7FFFFFFF;
+		}
+	} else {
+		uint32_t p0 = PRIMES[0].p;
+		uint32_t p1 = PRIMES[1].p;
+		uint32_t p1_0i = PRIMES[1].p0i;
+		uint32_t s = PRIMES[1].s;
+		uint64_t pp = (uint64_t)p0 * (uint64_t)p1;
+		uint64_t hpp = pp >> 1;
+		for (size_t u = 0; u < n; u ++) {
+			/*
+			 * Apply CRT with two primes on the coefficient of F.
+			 */
+			uint32_t x0 = F[u];      /* mod p0 */
+			uint32_t x1 = F[u + n];  /* mod p1 */
+			uint32_t x0m1 = x0 - (p1 & ~tbmask(x0 - p1));
+			uint32_t y = mp_montymul(
+				mp_sub(x1, x0m1, p1), s, p1, p1_0i);
+			uint64_t z = (uint64_t)x0 + (uint64_t)p0 * (uint64_t)y;
+			z -= pp & -((hpp - z) >> 63);
+			F[u] = (uint32_t)z & 0x7FFFFFFF;
+			F[u + n] = (uint32_t)(z >> 31) & 0x7FFFFFFF;
+		}
+		for (size_t u = 0; u < n; u ++) {
+			/*
+			 * Apply CRT with two primes on the coefficient of G.
+			 */
+			uint32_t x0 = G[u];      /* mod p0 */
+			uint32_t x1 = G[u + n];  /* mod p1 */
+			uint32_t x0m1 = x0 - (p1 & ~tbmask(x0 - p1));
+			uint32_t y = mp_montymul(
+				mp_sub(x1, x0m1, p1), s, p1, p1_0i);
+			uint64_t z = (uint64_t)x0 + (uint64_t)p0 * (uint64_t)y;
+			z -= pp & -((hpp - z) >> 63);
+			G[u] = (uint32_t)z & 0x7FFFFFFF;
+			G[u + n] = (uint32_t)(z >> 31) & 0x7FFFFFFF;
+		}
+	}
+}
+
+/* see ng_inner.h */
+int
+poly_is_invertible(unsigned logn, const int8_t *restrict f,
+	uint32_t p, uint32_t p0i, uint32_t s,
+	uint32_t r, uint32_t rm, unsigned rs, uint32_t *restrict tmp)
+{
+	size_t n = (size_t)1 << logn;
+	uint32_t *t1 = tmp;
+	uint32_t *t2 = t1 + n;
+	mp_mkgm(logn, t1, s, p, p0i);
+	for (size_t u = 0; u < n; u ++) {
+		t2[u] = mp_set(f[u], p);
+	}
+	mp_NTT(logn, t2, t1, p, p0i);
+	uint32_t b = 0;
+	for (size_t u = 0; u < n; u ++) {
+		/*
+		 * Reduce coefficient u modulo r, and set the top bit of b
+		 * if the result is zero.
+		 */
+		uint32_t x = t2[u];
+		uint32_t y = (uint32_t)(((uint64_t)x * (uint64_t)rm) >> rs);
+		x -= r * y;
+		b |= x - 1;
+	}
+
+	/*
+	 * If any of the NTT coefficients was zero, then the top bit of b
+	 * is 1, and the polynomial is not invertible. Otherwise, the top bit
+	 * of b is 0 and the polynomial is invertible.
+	 */
+	return 1 - (b >> 31);
+}
+
+/* see ng_inner.h */
+int
+poly_is_invertible_ext(unsigned logn, const int8_t *restrict f,
+	uint32_t r1, uint32_t r2, uint32_t p, uint32_t p0i, uint32_t s,
+	uint32_t r1m, unsigned r1s, uint32_t r2m, unsigned r2s,
+	uint32_t *restrict tmp)
+{
+	size_t n = (size_t)1 << logn;
+	uint32_t *t1 = tmp;
+	uint32_t *t2 = t1 + n;
+	mp_mkgm(logn, t1, s, p, p0i);
+	for (size_t u = 0; u < n; u ++) {
+		t2[u] = mp_set(f[u], p);
+	}
+	mp_NTT(logn, t2, t1, p, p0i);
+	uint32_t b = 0;
+	for (size_t u = 0; u < n; u ++) {
+		uint32_t x = t2[u];
+
+		/*
+		 * Reduce coefficient u modulo r1, and set the top bit of b
+		 * if the result is zero.
+		 */
+		uint32_t y1 = (uint32_t)(((uint64_t)x * (uint64_t)r1m) >> r1s);
+		b |= (x - r1 * y1) - 1;
+
+		/*
+		 * Idem for r2.
+		 */
+		uint32_t y2 = (uint32_t)(((uint64_t)x * (uint64_t)r2m) >> r2s);
+		b |= (x - r2 * y2) - 1;
+	}
+
+	/*
+	 * If any of the NTT coefficients was zero, then the top bit of b
+	 * is 1, and the polynomial is not invertible. Otherwise, the top bit
+	 * of b is 0 and the polynomial is invertible.
+	 */
+	return 1 - (b >> 31);
+}
+
+/* see ng_inner.h */
+TARGET_AVX2
+uint32_t
+poly_sqnorm(unsigned logn, const int8_t *f)
+{
+	size_t n = (size_t)1 << logn;
+#if NTRUGEN_AVX2
+	if (logn >= 4) {
+		__m256i ys = _mm256_setzero_si256();
+		__m256i ym = _mm256_set1_epi32(0xFFFF);
+		for (size_t u = 0; u < n; u += 16) {
+			__m128i x = _mm_loadu_si128((const __m128i *)(f + u));
+			__m256i y = _mm256_cvtepi8_epi16(x);
+			y = _mm256_mullo_epi16(y, y);
+			__m256i y0 = _mm256_and_si256(y, ym);
+			__m256i y1 = _mm256_srli_epi32(y, 16);
+			ys = _mm256_add_epi32(ys, _mm256_add_epi32(y0, y1));
+		}
+		ys = _mm256_add_epi32(ys, _mm256_srli_epi64(ys, 32));
+		ys = _mm256_add_epi32(ys, _mm256_bsrli_epi128(ys, 8));
+		return (uint32_t)_mm_cvtsi128_si32(
+			_mm_add_epi32(
+				_mm256_castsi256_si128(ys),
+				_mm256_extracti128_si256(ys, 1)));
+	}
+#endif // NTRUGEN_AVX2
+	uint32_t s = 0;
+	for (size_t u = 0; u < n; u ++) {
+		int32_t x = f[u];
+		s += (uint32_t)(x * x);
+	}
+	return s;
+}
diff --git a/lib/dns/hawk/ng_zint31.c b/lib/dns/hawk/ng_zint31.c
new file mode 100644
index 0000000000..24e0f02eda
--- /dev/null
+++ b/lib/dns/hawk/ng_zint31.c
@@ -0,0 +1,808 @@
+#include "ng_inner.h"
+
+/* ==================================================================== */
+/*
+ * Custom bignum implementation.
+ *
+ * Big integers are represented as sequences of 32-bit integers; the
+ * integer values are not necessarily consecutive in RAM (a dynamically
+ * provided "stride" value is added to the current word pointer, to get
+ * to the next word). The "len" parameter qualifies the number of words.
+ *
+ * Normal representation uses 31-bit limbs; each limb is stored in a
+ * 32-bit word, with the top bit (31) always cleared. Limbs are in
+ * low-to-high order. Signed integers use two's complement (hence, bit 30
+ * of the last limb is the sign bit).
+ *
+ * RNS representation of a big integer x is the sequence of values
+ * x modulo p, for the primes p defined in the PRIMES[] array.
+ */
+
+/* see ng_inner.h */
+uint32_t
+zint_mul_small(uint32_t *m, size_t len, uint32_t x)
+{
+	uint32_t cc = 0;
+	for (size_t u = 0; u < len; u ++) {
+		uint64_t z = (uint64_t)m[u] * (uint64_t)x + cc;
+		m[u] = (uint32_t)z & 0x7FFFFFFF;
+		cc = (uint32_t)(z >> 31);
+	}
+	return cc;
+}
+
+/* see ng_inner.h */
+uint32_t
+zint_mod_small_unsigned(const uint32_t *d, size_t len, size_t stride,
+	uint32_t p, uint32_t p0i, uint32_t R2)
+{
+	/*
+	 * Algorithm: we inject words one by one, starting with the high
+	 * word. Each step is:
+	 *  - multiply x by 2^31
+	 *  - add new word
+	 */
+	uint32_t x = 0;
+	uint32_t z = mp_half(R2, p);
+	d += len * stride;
+	for (uint32_t u = len; u > 0; u --) {
+		d -= stride;
+		uint32_t w = *d - p;
+		w += p & tbmask(w);
+		x = mp_montymul(x, z, p, p0i);
+		x = mp_add(x, w, p);
+	}
+	return x;
+}
+
+#if NTRUGEN_AVX2
+/* see ng_inner.h */
+TARGET_AVX2
+__m256i
+zint_mod_small_unsigned_x8(const uint32_t *d, size_t len, size_t stride,
+	__m256i yp, __m256i yp0i, __m256i yR2)
+{
+	__m256i yx = _mm256_setzero_si256();
+	__m256i yz = mp_half_x8(yR2, yp);
+	d += len * stride;
+	for (size_t u = len; u > 0; u --) {
+		d -= stride;
+		__m256i yw = _mm256_sub_epi32(
+			_mm256_loadu_si256((__m256i *)d), yp);
+		yw = _mm256_add_epi32(yw,
+			_mm256_and_si256(yp, _mm256_srai_epi32(yw, 31)));
+		yx = mp_montymul_x8(yx, yz, yp, yp0i);
+		yx = mp_add_x8(yx, yw, yp);
+	}
+	return yx;
+}
+#endif // NTRUGEN_AVX2
+
+/* see ng_inner.h */
+void
+zint_add_mul_small(uint32_t *restrict x, size_t len, size_t xstride,
+	const uint32_t *restrict y, uint32_t s)
+{
+	uint32_t cc = 0;
+	for (size_t u = 0; u < len; u ++) {
+		uint32_t xw, yw;
+		uint64_t z;
+
+		xw = *x;
+		yw = y[u];
+		z = (uint64_t)yw * (uint64_t)s + (uint64_t)xw + (uint64_t)cc;
+		*x = (uint32_t)z & 0x7FFFFFFF;
+		cc = (uint32_t)(z >> 31);
+		x += xstride;
+	}
+	*x = cc;
+}
+
+#if NTRUGEN_AVX2
+/*
+ * Like zint_add_mul_small, except that it handles eight destination integers
+ * in parallel, whose start addresses are consecutive.
+ *    d0 <- d0 + s0*a
+ *    d1 <- d1 + s1*a
+ *     ...
+ *    d7 <- d7 + s7*a
+ */
+TARGET_AVX2
+void
+zint_add_mul_small_x8(uint32_t *restrict d, size_t len, size_t dstride,
+	const uint32_t *restrict a, __m256i ys)
+{
+	__m256i cc0 = _mm256_setzero_si256();
+	__m256i cc1 = _mm256_setzero_si256();
+	__m256i ys0 = ys;
+	__m256i ys1 = _mm256_srli_epi64(ys, 32);
+	__m256i yw32 = _mm256_set1_epi64x(0xFFFFFFFF);
+	__m256i ym31 = _mm256_set1_epi32(0x7FFFFFFF);
+	for (size_t u = 0; u < len; u ++) {
+		__m256i ya = _mm256_set1_epi64x(a[u]);
+		__m256i z0 = _mm256_mul_epu32(ya, ys0);
+		__m256i z1 = _mm256_mul_epu32(ya, ys1);
+		__m256i yd = _mm256_loadu_si256((__m256i *)d);
+		__m256i yd0 = _mm256_and_si256(yd, yw32);
+		__m256i yd1 = _mm256_srli_epi64(yd, 32);
+		z0 = _mm256_add_epi64(z0, _mm256_add_epi64(yd0, cc0));
+		z1 = _mm256_add_epi64(z1, _mm256_add_epi64(yd1, cc1));
+		cc0 = _mm256_srli_epi64(z0, 31);
+		cc1 = _mm256_srli_epi64(z1, 31);
+		yd = _mm256_blend_epi32(z0, _mm256_slli_epi64(z1, 32), 0xAA);
+		_mm256_storeu_si256((__m256i *)d, _mm256_and_si256(yd, ym31));
+		d += dstride;
+	}
+
+	_mm256_storeu_si256((__m256i *)d,
+		_mm256_and_si256(_mm256_blend_epi32(
+			cc0, _mm256_slli_epi64(cc1, 32), 0xAA), ym31));
+}
+#endif // NTRUGEN_AVX2
+
+/* see ng_inner.h */
+void
+zint_norm_zero(uint32_t *restrict x, size_t len, size_t xstride,
+	const uint32_t *restrict p)
+{
+	/*
+	 * Compare x with p/2. We use the shifted version of p, and p
+	 * is odd, so we really compare with (p-1)/2; we want to perform
+	 * the subtraction if and only if x > (p-1)/2.
+	 */
+	uint32_t r = 0;
+	uint32_t bb = 0;
+	x += len * xstride;
+	size_t u = len;
+	while (u -- > 0) {
+		x -= xstride;
+		/*
+		 * Get the two words to compare in wx and wp (both over
+		 * 31 bits exactly).
+		 */
+		uint32_t wx = *x;
+		uint32_t wp = (p[u] >> 1) | (bb << 30);
+		bb = p[u] & 1;
+
+		/*
+		 * We set cc to -1, 0 or 1, depending on whether wp is
+		 * lower than, equal to, or greater than wx.
+		 */
+		uint32_t cc = wp - wx;
+		cc = ((-cc) >> 31) | -(cc >> 31);
+
+		/*
+		 * If r != 0 then it is either 1 or -1, and we keep its
+		 * value. Otherwise, if r = 0, then we replace it with cc.
+		 */
+		r |= cc & ((r & 1) - 1);
+	}
+
+	/*
+	 * At this point, r = -1, 0 or 1, depending on whether (p-1)/2
+	 * is lower than, equal to, or greater than x. We thus want to
+	 * do the subtraction only if r = -1.
+	 */
+	uint32_t cc = 0;
+	uint32_t m = tbmask(r);
+	for (size_t j = 0; j < len; j ++) {
+		uint32_t xw = *x;
+		uint32_t w = xw - p[j] - cc;
+		cc = w >> 31;
+		xw ^= ((w & 0x7FFFFFFF) ^ xw) & m;
+		*x = xw;
+		x += xstride;
+	}
+}
+
+#if NTRUGEN_AVX2
+TARGET_AVX2
+static void
+zint_norm_zero_x8(uint32_t *restrict x, size_t len, size_t xstride,
+	const uint32_t *restrict p)
+{
+	/*
+	 * Compare x with p/2. We use the shifted version of p, and p
+	 * is odd, so we really compare with (p-1)/2; we want to perform
+	 * the subtraction if and only if x > (p-1)/2.
+	 */
+	__m256i yr = _mm256_setzero_si256();
+	__m256i yone = _mm256_set1_epi32(1);
+	uint32_t bb = 0;
+	x += len * xstride;
+	size_t u = len;
+	while (u -- > 0) {
+		x -= xstride;
+		/*
+		 * Get the two words to compare in wx and wp (both over
+		 * 31 bits exactly).
+		 */
+		__m256i yx = _mm256_loadu_si256((__m256i *)x);
+		uint32_t wp = (p[u] >> 1) | (bb << 30);
+		bb = p[u] & 1;
+
+		/*
+		 * We set cc to -1, 0 or 1, depending on whether wp is
+		 * lower than, equal to, or greater than wx.
+		 */
+		__m256i ycc = _mm256_sub_epi32(_mm256_set1_epi32(wp), yx);
+		ycc = _mm256_or_si256(
+			_mm256_srli_epi32(_mm256_sub_epi32(
+				_mm256_setzero_si256(), ycc), 31),
+			_mm256_srai_epi32(ycc, 31));
+
+		/*
+		 * If r != 0 then it is either 1 or -1, and we keep its
+		 * value. Otherwise, if r = 0, then we replace it with cc.
+		 */
+		yr = _mm256_or_si256(yr, _mm256_and_si256(ycc,
+			_mm256_sub_epi32(_mm256_and_si256(yr, yone), yone)));
+	}
+
+	/*
+	 * At this point, r = -1, 0 or 1, depending on whether (p-1)/2
+	 * is lower than, equal to, or greater than x. We thus want to
+	 * do the subtraction only if r = -1.
+	 */
+	__m256i ycc = _mm256_setzero_si256();
+	__m256i ym = _mm256_srai_epi32(yr, 31);
+	__m256i y31 = _mm256_set1_epi32(0x7FFFFFFF);
+	for (size_t j = 0; j < len; j ++) {
+		__m256i yx = _mm256_loadu_si256((__m256i *)x);
+		__m256i y = _mm256_sub_epi32(
+			_mm256_sub_epi32(yx, ycc),
+			_mm256_set1_epi32(p[j]));
+		ycc = _mm256_srli_epi32(y, 31);
+		yx = _mm256_or_si256(
+			_mm256_andnot_si256(ym, yx),
+			_mm256_and_si256(ym, _mm256_and_si256(y, y31)));
+		_mm256_storeu_si256((__m256i *)x, yx);
+		x += xstride;
+	}
+}
+#endif // NTRUGEN_AVX2
+
+/* see ng_inner.h */
+TARGET_AVX2
+void
+zint_rebuild_CRT(uint32_t *restrict xx, size_t xlen, size_t n,
+	size_t num_sets, int normalize_signed, uint32_t *restrict tmp)
+{
+	size_t uu = 0;
+	tmp[0] = PRIMES[0].p;
+	for (size_t u = 1; u < xlen; u ++) {
+		/*
+		 * At the entry of each loop iteration:
+		 *  - the first u words of each array have been
+		 *    reassembled;
+		 *  - the first u words of tmp[] contains the
+		 * product of the prime moduli processed so far.
+		 *
+		 * We call 'q' the product of all previous primes.
+		 */
+		uint32_t p = PRIMES[u].p;
+		uint32_t p0i = PRIMES[u].p0i;
+		uint32_t R2 = PRIMES[u].R2;
+		uint32_t s = PRIMES[u].s;
+#if NTRUGEN_AVX2
+		__m256i yp = _mm256_set1_epi32(p);
+		__m256i yp0i = _mm256_set1_epi32(p0i);
+		__m256i yR2 = _mm256_set1_epi32(R2);
+		__m256i ys = _mm256_set1_epi32(s);
+#endif // NTRUGEN_AVX2
+		uu += n;
+		size_t kk = 0;
+		for (size_t k = 0; k < num_sets; k ++) {
+			size_t v = 0;
+#if NTRUGEN_AVX2
+			for (; (v + 7) < n; v += 8) {
+				__m256i y1 = _mm256_loadu_si256(
+					(__m256i *)(xx + kk + uu + v));
+				__m256i y2 = zint_mod_small_unsigned_x8(
+					xx + kk + v, u, n, yp, yp0i, yR2);
+				__m256i yr = mp_montymul_x8(ys,
+					mp_sub_x8(y1, y2, yp), yp, yp0i);
+				zint_add_mul_small_x8(
+					xx + kk + v, u, n, tmp, yr);
+			}
+#endif // NTRUGEN_AVX2
+			for (; v < n; v ++) {
+				/*
+				 * xp = the integer x modulo the prime p for
+				 *      this iteration
+				 * xq = (x mod q) mod p
+				 */
+				uint32_t xp = xx[kk + v + uu];
+				uint32_t xq = zint_mod_small_unsigned(
+					xx + kk + v, u, n, p, p0i, R2);
+
+				/*
+				 * New value is:
+				 *   (x mod q) + q*(s*(xp - xq) mod p)
+				 */
+				uint32_t xr = mp_montymul(
+					s, mp_sub(xp, xq, p), p, p0i);
+				zint_add_mul_small(xx + kk + v, u, n, tmp, xr);
+			}
+			kk += n * xlen;
+		}
+
+		/*
+		 * Update product of primes in tmp[].
+		 */
+		tmp[u] = zint_mul_small(tmp, u, p);
+	}
+
+	/*
+	 * Normalize the reconstructed values around 0.
+	 */
+	if (normalize_signed) {
+		size_t kk = 0;
+		for (size_t k = 0; k < num_sets; k ++) {
+			size_t v = 0;
+#if NTRUGEN_AVX2
+			for (; (v + 7) < n; v += 8) {
+				zint_norm_zero_x8(xx + kk + v, xlen, n, tmp);
+			}
+#endif // NTRUGEN_AVX2
+			for (; v < n; v ++) {
+				zint_norm_zero(xx + kk + v, xlen, n, tmp);
+			}
+			kk += n * xlen;
+		}
+	}
+}
+
+/* see ng_inner.h */
+void
+zint_negate(uint32_t *a, size_t len, uint32_t ctl)
+{
+	/*
+	 * If ctl = 1 then we flip the bits of a by XORing with
+	 * 0x7FFFFFFF, and we add 1 to the value. If ctl = 0 then we XOR
+	 * with 0 and add 0, which leaves the value unchanged.
+	 */
+	uint32_t cc = ctl;
+	uint32_t m = -ctl >> 1;
+	for (size_t u = 0; u < len; u ++) {
+		uint32_t aw;
+
+		aw = a[u];
+		aw = (aw ^ m) + cc;
+		a[u] = aw & 0x7FFFFFFF;
+		cc = aw >> 31;
+	}
+}
+
+/*
+ * Replace a with (a*xa+b*xb)/(2^31) and b with (a*ya+b*yb)/(2^31).
+ * The low bits are dropped (the caller should compute the coefficients
+ * such that these dropped bits are all zeros). If either or both
+ * yields a negative value, then the value is negated.
+ *
+ * Returned value is:
+ *  0  both values were positive
+ *  1  new a had to be negated
+ *  2  new b had to be negated
+ *  3  both new a and new b had to be negated
+ *
+ * Coefficients xa, xb, ya and yb may use the full signed 32-bit range.
+ * Integers a and b use stride 1.
+ */
+static uint32_t
+zint_co_reduce(uint32_t *restrict a, uint32_t *restrict b, size_t len,
+	int64_t xa, int64_t xb, int64_t ya, int64_t yb)
+{
+	int64_t cca = 0;
+	int64_t ccb = 0;
+	for (size_t u = 0; u < len; u ++) {
+		uint32_t wa, wb;
+		uint64_t za, zb;
+
+		wa = a[u];
+		wb = b[u];
+		za = wa * (uint64_t)xa + wb * (uint64_t)xb + (uint64_t)cca;
+		zb = wa * (uint64_t)ya + wb * (uint64_t)yb + (uint64_t)ccb;
+		if (u > 0) {
+			a[u - 1] = (uint32_t)za & 0x7FFFFFFF;
+			b[u - 1] = (uint32_t)zb & 0x7FFFFFFF;
+		}
+		cca = *(int64_t *)&za >> 31;
+		ccb = *(int64_t *)&zb >> 31;
+	}
+	a[len - 1] = (uint32_t)cca & 0x7FFFFFFF;
+	b[len - 1] = (uint32_t)ccb & 0x7FFFFFFF;
+
+	uint32_t nega = (uint32_t)((uint64_t)cca >> 63);
+	uint32_t negb = (uint32_t)((uint64_t)ccb >> 63);
+	zint_negate(a, len, nega);
+	zint_negate(b, len, negb);
+	return nega | (negb << 1);
+}
+
+/*
+ * Finish modular reduction. Rules on input parameters:
+ *
+ *   if neg = 1, then -m <= a < 0
+ *   if neg = 0, then 0 <= a < 2*m
+ *
+ * If neg = 0, then the top word of a[] is allowed to use 32 bits.
+ *
+ * Modulus m must be odd. Integers a and m have the same length len,
+ * and both use stride 1.
+ */
+static void
+zint_finish_mod(uint32_t *restrict a, const uint32_t *restrict m,
+	size_t len, uint32_t neg)
+{
+	/*
+	 * First pass: compare a (assumed nonnegative) with m. Note that
+	 * if the top word uses 32 bits, subtracting m must yield a
+	 * value less than 2^31 since a < 2*m.
+	 */
+	uint32_t cc = 0;
+	for (size_t u = 0; u < len; u ++) {
+		cc = (a[u] - m[u] - cc) >> 31;
+	}
+
+	/*
+	 * If neg = 1 then we must add m (regardless of cc)
+	 * If neg = 0 and cc = 0 then we must subtract m
+	 * If neg = 0 and cc = 1 then we must do nothing
+	 *
+	 * In the loop below, we conditionally subtract either m or -m
+	 * from a. Word xm is a word of m (if neg = 0) or -m (if neg = 1);
+	 * but if neg = 0 and cc = 1, then ym = 0 and it forces mw to 0.
+	 */
+	uint32_t xm = -neg >> 1;
+	uint32_t ym = -(neg | (1 - cc));
+	cc = neg;
+	for (size_t u = 0; u < len; u ++) {
+		uint32_t mw = (m[u] ^ xm) & ym;
+		uint32_t aw = a[u] - mw - cc;
+		a[u] = aw & 0x7FFFFFFF;
+		cc = aw >> 31;
+	}
+}
+
+/*
+ * Replace a with (a*xa+b*xb)/(2^31) mod m, and b with
+ * (a*ya+b*yb)/(2^31) mod m. Modulus m must be odd; m0i = -1/m[0] mod 2^31.
+ * Integers a, b and m all have length len and stride 1.
+ */
+static void
+zint_co_reduce_mod(uint32_t *restrict a, uint32_t *restrict b,
+	const uint32_t *restrict m, size_t len,
+	uint32_t m0i, int64_t xa, int64_t xb, int64_t ya, int64_t yb)
+{
+	/*
+	 * These are actually four combined Montgomery multiplications.
+	 */
+	int64_t cca = 0;
+	int64_t ccb = 0;
+	uint32_t fa = ((a[0] * (uint32_t)xa + b[0] * (uint32_t)xb) * m0i)
+		& 0x7FFFFFFF;
+	uint32_t fb = ((a[0] * (uint32_t)ya + b[0] * (uint32_t)yb) * m0i)
+		& 0x7FFFFFFF;
+	for (size_t u = 0; u < len; u ++) {
+		uint32_t wa = a[u];
+		uint32_t wb = b[u];
+		uint64_t za = wa * (uint64_t)xa + wb * (uint64_t)xb
+			+ m[u] * (uint64_t)fa + (uint64_t)cca;
+		uint64_t zb = wa * (uint64_t)ya + wb * (uint64_t)yb
+			+ m[u] * (uint64_t)fb + (uint64_t)ccb;
+		if (u > 0) {
+			a[u - 1] = (uint32_t)za & 0x7FFFFFFF;
+			b[u - 1] = (uint32_t)zb & 0x7FFFFFFF;
+		}
+		cca = *(int64_t *)&za >> 31;
+		ccb = *(int64_t *)&zb >> 31;
+	}
+	a[len - 1] = (uint32_t)cca;
+	b[len - 1] = (uint32_t)ccb;
+
+	/*
+	 * At this point:
+	 *   -m <= a < 2*m
+	 *   -m <= b < 2*m
+	 * (this is a case of Montgomery reduction)
+	 * The top words of 'a' and 'b' may have a 32-th bit set.
+	 * We want to add or subtract the modulus, as required.
+	 */
+	zint_finish_mod(a, m, len, (uint32_t)((uint64_t)cca >> 63));
+	zint_finish_mod(b, m, len, (uint32_t)((uint64_t)ccb >> 63));
+}
+
+/*
+ * Given an odd x, compute -1/x mod 2^31.
+ */
+static inline uint32_t
+mp_ninv31(uint32_t x)
+{
+	uint32_t y = 2 - x;
+	y *= 2 - x * y;
+	y *= 2 - x * y;
+	y *= 2 - x * y;
+	y *= 2 - x * y;
+	return (-y) & 0x7FFFFFFF;
+}
+
+/* see ng_inner.h */
+int
+zint_bezout(uint32_t *restrict u, uint32_t *restrict v,
+	const uint32_t *restrict x, const uint32_t *restrict y,
+	size_t len, uint32_t *restrict tmp)
+{
+	if (len == 0) {
+		return 0;
+	}
+
+	/*
+	 * Algorithm is basically the optimized binary GCD as described in:
+	 *    https://eprint.iacr.org/2020/972
+	 * The paper shows that with registers of size 2*k bits, one can
+	 * do k-1 inner iterations and get a reduction by k-1 bits. In
+	 * fact, it also works with registers of 2*k-1 bits (though not
+	 * 2*k-2; the "upper half" of the approximation must have at
+	 * least one extra bit). Here, we want to perform 31 inner
+	 * iterations (since that maps well to Montgomery reduction with
+	 * our 31-bit words) so we must use 63-bit approximations.
+	 *
+	 * We also slightly expand the original algorithm by maintaining
+	 * four coefficients (u0, u1, v0 and v1) instead of the two
+	 * coefficients (u, v), because we want a full Bezout relation,
+	 * not just a modular inverse.
+	 *
+	 * We set up integers u0, v0, u1, v1, a and b. Throughout the
+	 * algorithm, they maintain the following invariants:
+	 *   a = x*u0 - y*v0
+	 *   b = x*u1 - y*v1
+	 *   0 <= a <= x
+	 *   0 <= b <= y
+	 *   0 <= u0 < y
+	 *   0 <= v0 < x
+	 *   0 <= u1 <= y
+	 *   0 <= v1 < x
+	 */
+	uint32_t *u0 = tmp;
+	uint32_t *v0 = u0 + len;
+	uint32_t *u1 = u;
+	uint32_t *v1 = v;
+	uint32_t *a = v0 + len;
+	uint32_t *b = a + len;
+
+	/*
+	 * We'll need the Montgomery reduction coefficients.
+	 */
+	uint32_t x0i = mp_ninv31(x[0]);
+	uint32_t y0i = mp_ninv31(y[0]);
+
+	/*
+	 * Initial values:
+	 *   a = x   u0 = 1   v0 = 0
+	 *   b = y   u1 = y   v1 = x - 1
+	 * Note that x is odd, so computing x-1 is easy.
+	 */
+	memcpy(a, x, len * sizeof *x);
+	memcpy(b, y, len * sizeof *y);
+	u0[0] = 1;
+	memset(u0 + 1, 0, (len - 1) * sizeof *u0);
+	memset(v0, 0, len * sizeof *v0);
+	memcpy(u1, y, len * sizeof *u1);
+	memcpy(v1, x, len * sizeof *v1);
+	v1[0] --;
+
+	/*
+	 * Each input operand may be as large as 31*len bits, and we
+	 * reduce the total length by at least 31 bits at each iteration.
+	 */
+	for (uint32_t num = 62 * (uint32_t)len + 31; num >= 30; num -= 31) {
+		/*
+		 * Extract the top 32 bits of a and b: if j is such that:
+		 *   2^(j-1) <= max(a,b) < 2^j
+		 * then we want:
+		 *   xa = (2^31)*floor(a / 2^(j-32)) + (a mod 2^31)
+		 *   xb = (2^31)*floor(a / 2^(j-32)) + (b mod 2^31)
+		 * (if j < 63 then xa = a and xb = b).
+		 */
+		uint32_t c0 = 0xFFFFFFFF;
+		uint32_t c1 = 0xFFFFFFFF;
+		uint32_t cp = 0xFFFFFFFF;
+		uint32_t a0 = 0;
+		uint32_t a1 = 0;
+		uint32_t b0 = 0;
+		uint32_t b1 = 0;
+		size_t j = len;
+		while (j -- > 0) {
+			uint32_t aw = a[j];
+			uint32_t bw = b[j];
+			a1 ^= c1 & (a1 ^ aw);
+			a0 ^= c0 & (a0 ^ aw);
+			b1 ^= c1 & (b1 ^ bw);
+			b0 ^= c0 & (b0 ^ bw);
+			cp = c0;
+			c0 = c1;
+			c1 &= (((aw | bw) + 0x7FFFFFFF) >> 31) - 1;
+		}
+
+		/*
+		 * Possible situations:
+		 *   cp = 0, c0 = 0, c1 = 0
+		 *     j >= 63, top words of a and b are in a0:a1 and b0:b1
+		 *     (a1 and b1 are highest, a1|b1 != 0)
+		 *
+		 *   cp = -1, c0 = 0, c1 = 0
+		 *     32 <= j <= 62, a0:a1 and b0:b1 contain a and b, exactly
+		 *
+		 *   cp = -1, c0 = -1, c1 = 0
+		 *     j <= 31, a0 and a1 both contain a, b0 and b1 contain b
+		 *
+		 * When j >= 63, we align the top words to ensure that we get
+		 * the full 32 bits. We also take care to always call
+		 * lzcnt() with a non-zero operand.
+		 */
+		unsigned s = lzcnt_nonzero(a1 | b1 | ((cp & c0) >> 1));
+		uint32_t ha = (a1 << s) | (a0 >> (31 - s));
+		uint32_t hb = (b1 << s) | (b0 >> (31 - s));
+
+		/*
+		 * If j <= 62, then we instead use the non-aligned bits.
+		 */
+		ha ^= (cp & (ha ^ a1));
+		hb ^= (cp & (hb ^ b1));
+
+		/*
+		 * If j <= 31, then all of the above was bad, and we simply
+		 * clear the upper bits.
+		 */
+		ha &= ~c0;
+		hb &= ~c0;
+
+		/*
+		 * Assemble the approximate values xa and xb (63 bits each).
+		 */
+		uint64_t xa = ((uint64_t)ha << 31) | a[0];
+		uint64_t xb = ((uint64_t)hb << 31) | b[0];
+
+		/*
+		 * Compute reduction factors:
+		 *   a' = a*pa + b*pb
+		 *   b' = a*qa + b*qb
+		 * such that a' and b' are both multiples of 2^31, but are
+		 * only marginally larger than a and b.
+		 * Each coefficient is in the -(2^31-1)..+2^31 range. To keep
+		 * them on 32-bit values, we compute pa+(2^31-1)... and so on.
+		 */
+		uint64_t fg0 = 1;
+		uint64_t fg1 = (uint64_t)1 << 32;
+		for (int i = 0; i < 31; i ++) {
+			uint64_t a_odd = -(xa & 1);
+			uint64_t dx = xa - xb;
+			dx = (uint64_t)(*(int64_t *)&dx >> 63);
+			uint64_t swap = a_odd & dx;
+			uint64_t t1 = swap & (xa ^ xb);
+			xa ^= t1;
+			xb ^= t1;
+			uint64_t t2 = swap & (fg0 ^ fg1);
+			fg0 ^= t2;
+			fg1 ^= t2;
+			xa -= a_odd & xb;
+			fg0 -= a_odd & fg1;
+			xa >>= 1;
+			fg1 <<= 1;
+		}
+
+		/*
+		 * Split update factors.
+		 */
+		fg0 += (uint64_t)0x7FFFFFFF7FFFFFFF;
+		fg1 += (uint64_t)0x7FFFFFFF7FFFFFFF;
+		int64_t f0 = (int64_t)(fg0 & 0xFFFFFFFF) - 0x7FFFFFFF;
+		int64_t g0 = (int64_t)(fg0 >> 32) - 0x7FFFFFFF;
+		int64_t f1 = (int64_t)(fg1 & 0xFFFFFFFF) - 0x7FFFFFFF;
+		int64_t g1 = (int64_t)(fg1 >> 32) - 0x7FFFFFFF;
+
+		/*
+		 * Apply the update factors.
+		 */
+		uint32_t negab = zint_co_reduce(a, b, len, f0, g0, f1, g1);
+		f0 -= (f0 + f0) & -(int64_t)(negab & 1);
+		g0 -= (g0 + g0) & -(int64_t)(negab & 1);
+		f1 -= (f1 + f1) & -(int64_t)(negab >> 1);
+		g1 -= (g1 + g1) & -(int64_t)(negab >> 1);
+		zint_co_reduce_mod(u0, u1, y, len, y0i, f0, g0, f1, g1);
+		zint_co_reduce_mod(v0, v1, x, len, x0i, f0, g0, f1, g1);
+	}
+
+	/*
+	 * b contains GCD(x,y), provided that x and y were indeed odd.
+	 * Result is correct if the GCD is 1.
+	 */
+	uint32_t r = b[0] ^ 1;
+	for (size_t j = 1; j < len; j ++) {
+		r |= b[j];
+	}
+	r |= (x[0] & y[0] & 1) ^ 1;
+	return 1 - ((r | -r) >> 31);
+}
+
+/* see ng_inner.h */
+void
+zint_add_scaled_mul_small(uint32_t *restrict x, size_t xlen,
+	const uint32_t *restrict y, size_t ylen, size_t stride,
+	int32_t k, uint32_t sch, uint32_t scl)
+{
+	if (ylen == 0) {
+		return;
+	}
+
+	uint32_t ysign = -(y[stride * (ylen - 1)] >> 30) >> 1;
+	uint32_t tw = 0;
+	int32_t cc = 0;
+	x += sch * stride;
+	for (size_t u = sch; u < xlen; u ++) {
+		/*
+		 * Get the next word of (2^sc)*y.
+		 */
+		uint32_t wy;
+		if (ylen > 0) {
+			wy = *y;
+			y += stride;
+			ylen --;
+		} else {
+			wy = ysign;
+		}
+		uint32_t wys = ((wy << scl) & 0x7FFFFFFF) | tw;
+		tw = wy >> (31 - scl);
+
+		/*
+		 * The expression below does not overflow.
+		 */
+		uint64_t z = (uint64_t)((int64_t)wys * (int64_t)k
+			+ (int64_t)*x + cc);
+		*x = (uint32_t)z & 0x7FFFFFFF;
+		x += stride;
+
+		/*
+		 * New carry word is a _signed_ right-shift of z.
+		 */
+		uint32_t ccu = (uint32_t)(z >> 31);
+		cc = *(int32_t *)&ccu;
+	}
+}
+
+/* see ng_inner.h */
+void
+zint_sub_scaled(uint32_t *restrict x, size_t xlen,
+	const uint32_t *restrict y, size_t ylen, size_t stride,
+	uint32_t sch, uint32_t scl)
+{
+	if (ylen == 0) {
+		return;
+	}
+
+	uint32_t ysign = -(y[stride * (ylen - 1)] >> 30) >> 1;
+	uint32_t tw = 0;
+	int32_t cc = 0;
+	x += sch * stride;
+	for (size_t u = sch; u < xlen; u ++) {
+		/*
+		 * Get the next word of (2^sc)*y.
+		 */
+		uint32_t wy;
+		if (ylen > 0) {
+			wy = *y;
+			y += stride;
+			ylen --;
+		} else {
+			wy = ysign;
+		}
+		uint32_t wys = ((wy << scl) & 0x7FFFFFFF) | tw;
+		tw = wy >> (31 - scl);
+
+		uint32_t w = *x - wys - cc;
+		*x = w & 0x7FFFFFFF;
+		cc = w >> 31;
+		x += stride;
+	}
+}
diff --git a/lib/dns/hawk/sha3.c b/lib/dns/hawk/sha3.c
new file mode 100644
index 0000000000..22df92c81a
--- /dev/null
+++ b/lib/dns/hawk/sha3.c
@@ -0,0 +1,1537 @@
+/*
+ * SHA3 and SHAKE implementation.
+ */
+
+#include <stddef.h>
+#include <stdint.h>
+#include <string.h>
+
+#include "ng_inner.h"
+
+#if NTRUGEN_ASM_CORTEXM4
+/*
+ * Process the provided state.
+ */
+__attribute__((naked))
+static void
+process_block(uint64_t *A __attribute__((unused)))
+{
+	__asm__ (
+	"push	{ r1, r2, r3, r4, r5, r6, r7, r8, r10, r11, r12, lr }\n\t"
+	"sub	sp, sp, #232\n\t"
+	"\n\t"
+	"@ Invert some words (alternate internal representation, which\n\t"
+	"@ saves some operations).\n\t"
+	"\n\t"
+
+#define INVERT_WORDS \
+	"@ Invert A[1] and A[2].\n\t" \
+	"adds	r1, r0, #8\n\t" \
+	"ldm	r1, { r2, r3, r4, r5 }\n\t" \
+	"mvns	r2, r2\n\t" \
+	"mvns	r3, r3\n\t" \
+	"mvns	r4, r4\n\t" \
+	"mvns	r5, r5\n\t" \
+	"stm	r1!, { r2, r3, r4, r5 }\n\t" \
+	"@ Invert A[8]\n\t" \
+	"adds	r1, r0, #64\n\t" \
+	"ldm	r1, { r2, r3 }\n\t" \
+	"mvns	r2, r2\n\t" \
+	"mvns	r3, r3\n\t" \
+	"stm	r1!, { r2, r3 }\n\t" \
+	"@ Invert A[12]\n\t" \
+	"adds	r1, r0, #96\n\t" \
+	"ldm	r1, { r2, r3 }\n\t" \
+	"mvns	r2, r2\n\t" \
+	"mvns	r3, r3\n\t" \
+	"stm	r1!, { r2, r3 }\n\t" \
+	"@ Invert A[17]\n\t" \
+	"adds	r1, r0, #136\n\t" \
+	"ldm	r1, { r2, r3 }\n\t" \
+	"mvns	r2, r2\n\t" \
+	"mvns	r3, r3\n\t" \
+	"stm	r1!, { r2, r3 }\n\t" \
+	"@ Invert A[20]\n\t" \
+	"adds	r1, r0, #160\n\t" \
+	"ldm	r1, { r2, r3 }\n\t" \
+	"mvns	r2, r2\n\t" \
+	"mvns	r3, r3\n\t" \
+	"stm	r1!, { r2, r3 }\n\t" \
+	"\n\t"
+
+	INVERT_WORDS
+
+	"@ Do 24 rounds. Each loop iteration performs one rounds. We\n\t"
+	"@ keep eight times the current round counter in [sp] (i.e.\n\t"
+	"@ a multiple of 8, from 0 to 184).\n\t"
+	"\n\t"
+	"eors	r1, r1\n\t"
+	"str	r1, [sp, #0]\n\t"
+".process_block_loop:\n\t"
+	"\n\t"
+	"@ xor(A[5*i+0]) -> r1:r2\n\t"
+	"@ xor(A[5*i+1]) -> r3:r4\n\t"
+	"@ xor(A[5*i+2]) -> r5:r6\n\t"
+	"@ xor(A[5*i+3]) -> r7:r8\n\t"
+	"@ xor(A[5*i+4]) -> r10:r11\n\t"
+	"ldm	r0!, { r1, r2, r3, r4, r5, r6, r7, r8 }\n\t"
+	"adds	r0, #8\n\t"
+	"ldm	r0!, { r10, r11, r12 }\n\t"
+	"eors	r1, r10\n\t"
+	"eors	r2, r11\n\t"
+	"eors	r3, r12\n\t"
+	"ldm	r0!, { r10, r11, r12 }\n\t"
+	"eors	r4, r10\n\t"
+	"eors	r5, r11\n\t"
+	"eors	r6, r12\n\t"
+	"ldm	r0!, { r10, r11 }\n\t"
+	"eors	r7, r10\n\t"
+	"eors	r8, r11\n\t"
+	"adds	r0, #8\n\t"
+	"ldm	r0!, { r10, r11, r12 }\n\t"
+	"eors	r1, r10\n\t"
+	"eors	r2, r11\n\t"
+	"eors	r3, r12\n\t"
+	"ldm	r0!, { r10, r11, r12 }\n\t"
+	"eors	r4, r10\n\t"
+	"eors	r5, r11\n\t"
+	"eors	r6, r12\n\t"
+	"ldm	r0!, { r10, r11 }\n\t"
+	"eors	r7, r10\n\t"
+	"eors	r8, r11\n\t"
+	"adds	r0, #8\n\t"
+	"ldm	r0!, { r10, r11, r12 }\n\t"
+	"eors	r1, r10\n\t"
+	"eors	r2, r11\n\t"
+	"eors	r3, r12\n\t"
+	"ldm	r0!, { r10, r11, r12 }\n\t"
+	"eors	r4, r10\n\t"
+	"eors	r5, r11\n\t"
+	"eors	r6, r12\n\t"
+	"ldm	r0!, { r10, r11 }\n\t"
+	"eors	r7, r10\n\t"
+	"eors	r8, r11\n\t"
+	"adds	r0, #8\n\t"
+	"ldm	r0!, { r10, r11, r12 }\n\t"
+	"eors	r1, r10\n\t"
+	"eors	r2, r11\n\t"
+	"eors	r3, r12\n\t"
+	"ldm	r0!, { r10, r11, r12 }\n\t"
+	"eors	r4, r10\n\t"
+	"eors	r5, r11\n\t"
+	"eors	r6, r12\n\t"
+	"ldm	r0!, { r10, r11 }\n\t"
+	"eors	r7, r10\n\t"
+	"eors	r8, r11\n\t"
+	"ldm	r0!, { r10, r11 }\n\t"
+	"subs	r0, #200\n\t"
+	"ldr	r12, [r0, #32]\n\t"
+	"eors	r10, r12\n\t"
+	"ldr	r12, [r0, #36]\n\t"
+	"eors	r11, r12\n\t"
+	"ldr	r12, [r0, #72]\n\t"
+	"eors	r10, r12\n\t"
+	"ldr	r12, [r0, #76]\n\t"
+	"eors	r11, r12\n\t"
+	"ldr	r12, [r0, #112]\n\t"
+	"eors	r10, r12\n\t"
+	"ldr	r12, [r0, #116]\n\t"
+	"eors	r11, r12\n\t"
+	"ldr	r12, [r0, #152]\n\t"
+	"eors	r10, r12\n\t"
+	"ldr	r12, [r0, #156]\n\t"
+	"eors	r11, r12\n\t"
+	"\n\t"
+	"@ t0 = xor(A[5*i+4]) ^ rotl1(xor(A[5*i+1])) -> r10:r11\n\t"
+	"@ t1 = xor(A[5*i+0]) ^ rotl1(xor(A[5*i+2])) -> r1:r2\n\t"
+	"@ t2 = xor(A[5*i+1]) ^ rotl1(xor(A[5*i+3])) -> r3:r4\n\t"
+	"@ t3 = xor(A[5*i+2]) ^ rotl1(xor(A[5*i+4])) -> r5:r6\n\t"
+	"@ t4 = xor(A[5*i+3]) ^ rotl1(xor(A[5*i+0])) -> r7:r8\n\t"
+	"str	r11, [sp, #4]\n\t"
+	"mov	r12, r10\n\t"
+	"eors	r10, r10, r3, lsl #1\n\t"
+	"eors	r10, r10, r4, lsr #31\n\t"
+	"eors	r11, r11, r4, lsl #1\n\t"
+	"eors	r11, r11, r3, lsr #31\n\t"
+	"eors	r3, r3, r7, lsl #1\n\t"
+	"eors	r3, r3, r8, lsr #31\n\t"
+	"eors	r4, r4, r8, lsl #1\n\t"
+	"eors	r4, r4, r7, lsr #31\n\t"
+	"eors	r7, r7, r1, lsl #1\n\t"
+	"eors	r7, r7, r2, lsr #31\n\t"
+	"eors	r8, r8, r2, lsl #1\n\t"
+	"eors	r8, r8, r1, lsr #31\n\t"
+	"eors	r1, r1, r5, lsl #1\n\t"
+	"eors	r1, r1, r6, lsr #31\n\t"
+	"eors	r2, r2, r6, lsl #1\n\t"
+	"eors	r2, r2, r5, lsr #31\n\t"
+	"eors	r5, r5, r12, lsl #1\n\t"
+	"eors	r6, r6, r12, lsr #31\n\t"
+	"ldr	r12, [sp, #4]\n\t"
+	"eors	r5, r5, r12, lsr #31\n\t"
+	"eors	r6, r6, r12, lsl #1\n\t"
+	"\n\t"
+	"@ Save t2, t3 and t4 on the stack.\n\t"
+	"addw	r12, sp, #4\n\t"
+	"stm	r12, { r3, r4, r5, r6, r7, r8 }\n\t"
+	"\n\t"
+	"@ We XOR one of the t0..t4 values into each A[] word, and\n\t"
+	"@ rotate the result by some amount (each word has its own\n\t"
+	"@ amount). The results are written back into a stack buffer\n\t"
+	"@ that starts at sp+32\n\t"
+	"addw	r12, sp, #32\n\t"
+	"\n\t"
+	"@ XOR t0 into A[5*i+0] and t1 into A[5*i+1]; each A[i] is also\n\t"
+	"@ rotated left by some amount.\n\t"
+	"\n\t"
+	"@ A[0] and A[1]\n\t"
+	"ldm	r0!, { r5, r6, r7, r8 }\n\t"
+	"eors	r5, r10\n\t"
+	"eors	r6, r11\n\t"
+	"eors	r3, r7, r1\n\t"
+	"eors	r4, r8, r2\n\t"
+	"lsl	r7, r3, #1\n\t"
+	"orr	r7, r7, r4, lsr #31\n\t"
+	"lsl	r8, r4, #1\n\t"
+	"orr	r8, r8, r3, lsr #31\n\t"
+	"stm	r12!, { r5, r6, r7, r8 }\n\t"
+	"\n\t"
+	"@ A[5] and A[6]\n\t"
+	"adds	r0, #24\n\t"
+	"ldm	r0!, { r5, r6, r7, r8 }\n\t"
+	"eors	r3, r5, r10\n\t"
+	"eors	r4, r6, r11\n\t"
+	"lsl	r5, r4, #4\n\t"
+	"orr	r5, r5, r3, lsr #28\n\t"
+	"lsl	r6, r3, #4\n\t"
+	"orr	r6, r6, r4, lsr #28\n\t"
+	"eors	r3, r7, r1\n\t"
+	"eors	r4, r8, r2\n\t"
+	"lsl	r7, r4, #12\n\t"
+	"orr	r7, r7, r3, lsr #20\n\t"
+	"lsl	r8, r3, #12\n\t"
+	"orr	r8, r8, r4, lsr #20\n\t"
+	"stm	r12!, { r5, r6, r7, r8 }\n\t"
+	"\n\t"
+	"@ A[10] and A[11]\n\t"
+	"adds	r0, #24\n\t"
+	"ldm	r0!, { r5, r6, r7, r8 }\n\t"
+	"eors	r3, r5, r10\n\t"
+	"eors	r4, r6, r11\n\t"
+	"lsl	r5, r3, #3\n\t"
+	"orr	r5, r5, r4, lsr #29\n\t"
+	"lsl	r6, r4, #3\n\t"
+	"orr	r6, r6, r3, lsr #29\n\t"
+	"eors	r3, r7, r1\n\t"
+	"eors	r4, r8, r2\n\t"
+	"lsl	r7, r3, #10\n\t"
+	"orr	r7, r7, r4, lsr #22\n\t"
+	"lsl	r8, r4, #10\n\t"
+	"orr	r8, r8, r3, lsr #22\n\t"
+	"stm	r12!, { r5, r6, r7, r8 }\n\t"
+	"\n\t"
+	"@ A[15] and A[16]\n\t"
+	"adds	r0, #24\n\t"
+	"ldm	r0!, { r5, r6, r7, r8 }\n\t"
+	"eors	r3, r5, r10\n\t"
+	"eors	r4, r6, r11\n\t"
+	"lsl	r5, r4, #9\n\t"
+	"orr	r5, r5, r3, lsr #23\n\t"
+	"lsl	r6, r3, #9\n\t"
+	"orr	r6, r6, r4, lsr #23\n\t"
+	"eors	r3, r7, r1\n\t"
+	"eors	r4, r8, r2\n\t"
+	"lsl	r7, r4, #13\n\t"
+	"orr	r7, r7, r3, lsr #19\n\t"
+	"lsl	r8, r3, #13\n\t"
+	"orr	r8, r8, r4, lsr #19\n\t"
+	"stm	r12!, { r5, r6, r7, r8 }\n\t"
+	"\n\t"
+	"@ A[20] and A[21]\n\t"
+	"adds	r0, #24\n\t"
+	"ldm	r0!, { r5, r6, r7, r8 }\n\t"
+	"eors	r3, r5, r10\n\t"
+	"eors	r4, r6, r11\n\t"
+	"lsl	r5, r3, #18\n\t"
+	"orr	r5, r5, r4, lsr #14\n\t"
+	"lsl	r6, r4, #18\n\t"
+	"orr	r6, r6, r3, lsr #14\n\t"
+	"eors	r3, r7, r1\n\t"
+	"eors	r4, r8, r2\n\t"
+	"lsl	r7, r3, #2\n\t"
+	"orr	r7, r7, r4, lsr #30\n\t"
+	"lsl	r8, r4, #2\n\t"
+	"orr	r8, r8, r3, lsr #30\n\t"
+	"stm	r12!, { r5, r6, r7, r8 }\n\t"
+	"\n\t"
+	"@ XOR t2 into A[5*i+2] and t3 into A[5*i+3]; each A[i] is also\n\t"
+	"@ rotated left by some amount. We reload t2 into r1:r2 and t3\n\t"
+	"@ into r3:r4.\n\t"
+	"addw	r5, sp, #4\n\t"
+	"ldm	r5!, { r1, r2, r3, r4 }\n\t"
+	"\n\t"
+	"@ A[2] and A[3]\n\t"
+	"subs	r0, #160\n\t"
+	"ldm	r0!, { r5, r6, r7, r8 }\n\t"
+	"eors	r10, r5, r1\n\t"
+	"eors	r11, r6, r2\n\t"
+	"lsl	r5, r11, #30\n\t"
+	"orr	r5, r5, r10, lsr #2\n\t"
+	"lsl	r6, r10, #30\n\t"
+	"orr	r6, r6, r11, lsr #2\n\t"
+	"eors	r10, r7, r3\n\t"
+	"eors	r11, r8, r4\n\t"
+	"lsl	r7, r10, #28\n\t"
+	"orr	r7, r7, r11, lsr #4\n\t"
+	"lsl	r8, r11, #28\n\t"
+	"orr	r8, r8, r10, lsr #4\n\t"
+	"stm	r12!, { r5, r6, r7, r8 }\n\t"
+	"\n\t"
+	"@ A[7] and A[8]\n\t"
+	"adds	r0, #24\n\t"
+	"ldm	r0!, { r5, r6, r7, r8 }\n\t"
+	"eors	r10, r5, r1\n\t"
+	"eors	r11, r6, r2\n\t"
+	"lsl	r5, r10, #6\n\t"
+	"orr	r5, r5, r11, lsr #26\n\t"
+	"lsl	r6, r11, #6\n\t"
+	"orr	r6, r6, r10, lsr #26\n\t"
+	"eors	r10, r7, r3\n\t"
+	"eors	r11, r8, r4\n\t"
+	"lsl	r7, r11, #23\n\t"
+	"orr	r7, r7, r10, lsr #9\n\t"
+	"lsl	r8, r10, #23\n\t"
+	"orr	r8, r8, r11, lsr #9\n\t"
+	"stm	r12!, { r5, r6, r7, r8 }\n\t"
+	"\n\t"
+	"@ A[12] and A[13]\n\t"
+	"adds	r0, #24\n\t"
+	"ldm	r0!, { r5, r6, r7, r8 }\n\t"
+	"eors	r10, r5, r1\n\t"
+	"eors	r11, r6, r2\n\t"
+	"lsl	r5, r11, #11\n\t"
+	"orr	r5, r5, r10, lsr #21\n\t"
+	"lsl	r6, r10, #11\n\t"
+	"orr	r6, r6, r11, lsr #21\n\t"
+	"eors	r10, r7, r3\n\t"
+	"eors	r11, r8, r4\n\t"
+	"lsl	r7, r10, #25\n\t"
+	"orr	r7, r7, r11, lsr #7\n\t"
+	"lsl	r8, r11, #25\n\t"
+	"orr	r8, r8, r10, lsr #7\n\t"
+	"stm	r12!, { r5, r6, r7, r8 }\n\t"
+	"\n\t"
+	"@ A[17] and A[18]\n\t"
+	"adds	r0, #24\n\t"
+	"ldm	r0!, { r5, r6, r7, r8 }\n\t"
+	"eors	r10, r5, r1\n\t"
+	"eors	r11, r6, r2\n\t"
+	"lsl	r5, r10, #15\n\t"
+	"orr	r5, r5, r11, lsr #17\n\t"
+	"lsl	r6, r11, #15\n\t"
+	"orr	r6, r6, r10, lsr #17\n\t"
+	"eors	r10, r7, r3\n\t"
+	"eors	r11, r8, r4\n\t"
+	"lsl	r7, r10, #21\n\t"
+	"orr	r7, r7, r11, lsr #11\n\t"
+	"lsl	r8, r11, #21\n\t"
+	"orr	r8, r8, r10, lsr #11\n\t"
+	"stm	r12!, { r5, r6, r7, r8 }\n\t"
+	"\n\t"
+	"@ A[22] and A[23]\n\t"
+	"adds	r0, #24\n\t"
+	"ldm	r0!, { r5, r6, r7, r8 }\n\t"
+	"eors	r10, r5, r1\n\t"
+	"eors	r11, r6, r2\n\t"
+	"lsl	r5, r11, #29\n\t"
+	"orr	r5, r5, r10, lsr #3\n\t"
+	"lsl	r6, r10, #29\n\t"
+	"orr	r6, r6, r11, lsr #3\n\t"
+	"eors	r10, r7, r3\n\t"
+	"eors	r11, r8, r4\n\t"
+	"lsl	r7, r11, #24\n\t"
+	"orr	r7, r7, r10, lsr #8\n\t"
+	"lsl	r8, r10, #24\n\t"
+	"orr	r8, r8, r11, lsr #8\n\t"
+	"stm	r12!, { r5, r6, r7, r8 }\n\t"
+	"\n\t"
+	"@ XOR t4 into A[5*i+4]; each A[i] is also rotated left by some\n\t"
+	"@ amount. We reload t4 into r1:r2.\n\t"
+	"ldr	r1, [sp, #20]\n\t"
+	"ldr	r2, [sp, #24]\n\t"
+	"\n\t"
+	"@ A[4]\n\t"
+	"subs	r0, #160\n\t"
+	"ldm	r0!, { r5, r6 }\n\t"
+	"eors	r3, r5, r1\n\t"
+	"eors	r4, r6, r2\n\t"
+	"lsl	r5, r3, #27\n\t"
+	"orr	r5, r5, r4, lsr #5\n\t"
+	"lsl	r6, r4, #27\n\t"
+	"orr	r6, r6, r3, lsr #5\n\t"
+	"stm	r12!, { r5, r6 }\n\t"
+	"\n\t"
+	"@ A[9]\n\t"
+	"adds	r0, #32\n\t"
+	"ldm	r0!, { r5, r6 }\n\t"
+	"eors	r3, r5, r1\n\t"
+	"eors	r4, r6, r2\n\t"
+	"lsl	r5, r3, #20\n\t"
+	"orr	r5, r5, r4, lsr #12\n\t"
+	"lsl	r6, r4, #20\n\t"
+	"orr	r6, r6, r3, lsr #12\n\t"
+	"stm	r12!, { r5, r6 }\n\t"
+	"\n\t"
+	"@ A[14]\n\t"
+	"adds	r0, #32\n\t"
+	"ldm	r0!, { r5, r6 }\n\t"
+	"eors	r3, r5, r1\n\t"
+	"eors	r4, r6, r2\n\t"
+	"lsl	r5, r4, #7\n\t"
+	"orr	r5, r5, r3, lsr #25\n\t"
+	"lsl	r6, r3, #7\n\t"
+	"orr	r6, r6, r4, lsr #25\n\t"
+	"stm	r12!, { r5, r6 }\n\t"
+	"\n\t"
+	"@ A[19]\n\t"
+	"adds	r0, #32\n\t"
+	"ldm	r0!, { r5, r6 }\n\t"
+	"eors	r3, r5, r1\n\t"
+	"eors	r4, r6, r2\n\t"
+	"lsl	r5, r3, #8\n\t"
+	"orr	r5, r5, r4, lsr #24\n\t"
+	"lsl	r6, r4, #8\n\t"
+	"orr	r6, r6, r3, lsr #24\n\t"
+	"stm	r12!, { r5, r6 }\n\t"
+	"\n\t"
+	"@ A[24]\n\t"
+	"adds	r0, #32\n\t"
+	"ldm	r0!, { r5, r6 }\n\t"
+	"eors	r3, r5, r1\n\t"
+	"eors	r4, r6, r2\n\t"
+	"lsl	r5, r3, #14\n\t"
+	"orr	r5, r5, r4, lsr #18\n\t"
+	"lsl	r6, r4, #14\n\t"
+	"orr	r6, r6, r3, lsr #18\n\t"
+	"stm	r12!, { r5, r6 }\n\t"
+	"\n\t"
+	"subs	r0, #200\n\t"
+	"\n\t"
+	"@ At that point, the stack buffer at sp+32 contains the words\n\t"
+	"@ at the following indexes (0 to 24) and offsets (from sp)\n\t"
+	"@   A[ 0]    0      32\n\t"
+	"@   A[ 1]    1      40\n\t"
+	"@   A[ 2]   10     112\n\t"
+	"@   A[ 3]   11     120\n\t"
+	"@   A[ 4]   20     192\n\t"
+	"@   A[ 5]    2      48\n\t"
+	"@   A[ 6]    3      56\n\t"
+	"@   A[ 7]   12     128\n\t"
+	"@   A[ 8]   13     136\n\t"
+	"@   A[ 9]   21     200\n\t"
+	"@   A[10]    4      64\n\t"
+	"@   A[11]    5      72\n\t"
+	"@   A[12]   14     144\n\t"
+	"@   A[13]   15     152\n\t"
+	"@   A[14]   22     208\n\t"
+	"@   A[15]    6      80\n\t"
+	"@   A[16]    7      88\n\t"
+	"@   A[17]   16     160\n\t"
+	"@   A[18]   17     168\n\t"
+	"@   A[19]   23     216\n\t"
+	"@   A[20]    8      96\n\t"
+	"@   A[21]    9     104\n\t"
+	"@   A[22]   18     176\n\t"
+	"@   A[23]   19     184\n\t"
+	"@   A[24]   24     224\n\t"
+
+#define KHI_LOAD(s0, s1, s2, s3, s4) \
+	"ldr	r1, [sp, #(32 + 8 * " #s0 ")]\n\t" \
+	"ldr	r2, [sp, #(36 + 8 * " #s0 ")]\n\t" \
+	"ldr	r3, [sp, #(32 + 8 * " #s1 ")]\n\t" \
+	"ldr	r4, [sp, #(36 + 8 * " #s1 ")]\n\t" \
+	"ldr	r5, [sp, #(32 + 8 * " #s2 ")]\n\t" \
+	"ldr	r6, [sp, #(36 + 8 * " #s2 ")]\n\t" \
+	"ldr	r7, [sp, #(32 + 8 * " #s3 ")]\n\t" \
+	"ldr	r8, [sp, #(36 + 8 * " #s3 ")]\n\t" \
+	"ldr	r10, [sp, #(32 + 8 * " #s4 ")]\n\t" \
+	"ldr	r11, [sp, #(36 + 8 * " #s4 ")]\n\t"
+
+#define KHI_STEP(op, x0, x1, x2, x3, x4, x5, d) \
+	#op "	r12, " #x0 ", " #x2 "\n\t" \
+	"eors	r12, " #x4 "\n\t" \
+	"str	r12, [r0, #(8 * " #d ")]\n\t" \
+	#op "	r12, " #x1 ", " #x3 "\n\t" \
+	"eors	r12, " #x5 "\n\t" \
+	"str	r12, [r0, #(4 + 8 * " #d ")]\n\t"
+
+	"@ A[0], A[6], A[12], A[18] and A[24]\n\t"
+	KHI_LOAD(0, 3, 14, 17, 24)
+	KHI_STEP(orrs, r3, r4, r5, r6, r1, r2, 0)
+	KHI_STEP(orns, r7, r8, r5, r6, r3, r4, 1)
+	KHI_STEP(ands, r7, r8, r10, r11, r5, r6, 2)
+	KHI_STEP(orrs, r1, r2, r10, r11, r7, r8, 3)
+	KHI_STEP(ands, r1, r2, r3, r4, r10, r11, 4)
+	"\n\t"
+
+	"@ A[3], A[9], A[10], A[16] and A[22]\n\t"
+	KHI_LOAD(11, 21, 4, 7, 18)
+	KHI_STEP(orrs, r3, r4, r5, r6, r1, r2, 5)
+	KHI_STEP(ands, r7, r8, r5, r6, r3, r4, 6)
+	KHI_STEP(orns, r7, r8, r10, r11, r5, r6, 7)
+	KHI_STEP(orrs, r1, r2, r10, r11, r7, r8, 8)
+	KHI_STEP(ands, r1, r2, r3, r4, r10, r11, 9)
+	"\n\t"
+
+	"@ A[1], A[7], A[13], A[19] and A[20]\n\t"
+	KHI_LOAD(1, 12, 15, 23, 8)
+	KHI_STEP(orrs, r3, r4, r5, r6, r1, r2, 10)
+	KHI_STEP(ands, r7, r8, r5, r6, r3, r4, 11)
+	KHI_STEP(bics, r10, r11, r7, r8, r5, r6, 12)
+	"mvns	r7, r7\n\t"
+	"mvns	r8, r8\n\t"
+	KHI_STEP(orrs, r1, r2, r10, r11, r7, r8, 13)
+	KHI_STEP(ands, r1, r2, r3, r4, r10, r11, 14)
+	"\n\t"
+
+	"@ A[4], A[5], A[11], A[17] and A[23]\n\t"
+	KHI_LOAD(20, 2, 5, 16, 19)
+	KHI_STEP(ands, r3, r4, r5, r6, r1, r2, 15)
+	KHI_STEP(orrs, r7, r8, r5, r6, r3, r4, 16)
+	KHI_STEP(orns, r10, r11, r7, r8, r5, r6, 17)
+	"mvns	r7, r7\n\t"
+	"mvns	r8, r8\n\t"
+	KHI_STEP(ands, r1, r2, r10, r11, r7, r8, 18)
+	KHI_STEP(orrs, r1, r2, r3, r4, r10, r11, 19)
+	"\n\t"
+
+	"@ A[2], A[8], A[14], A[15] and A[21]\n\t"
+	KHI_LOAD(10, 13, 22, 6, 9)
+	KHI_STEP(bics, r5, r6, r3, r4, r1, r2, 20)
+	KHI_STEP(ands, r1, r2, r3, r4, r10, r11, 24)
+	"mvns	r3, r3\n\t"
+	"mvns	r4, r4\n\t"
+	KHI_STEP(orrs, r7, r8, r5, r6, r3, r4, 21)
+	KHI_STEP(ands, r7, r8, r10, r11, r5, r6, 22)
+	KHI_STEP(orrs, r1, r2, r10, r11, r7, r8, 23)
+	"\n\t"
+
+	"@ Get round counter XOR round constant into A[0]\n\t"
+	"ldr	r1, [sp, #0]\n\t"
+	"adr	r2, .process_block_RC\n\t"
+	"adds	r2, r1\n\t"
+	"ldm	r2, { r3, r4 }\n\t"
+	"ldm	r0, { r5, r6 }\n\t"
+	"eors	r5, r3\n\t"
+	"eors	r6, r4\n\t"
+	"stm	r0, { r5, r6 }\n\t"
+	"\n\t"
+	"@ Increment round counter, loop until all 24 rounds are done.\n\t"
+	"\n\t"
+	"adds	r1, #8\n\t"
+	"str	r1, [sp, #0]\n\t"
+	"cmp	r1, #192\n\t"
+	"blo	.process_block_loop\n\t"
+
+	INVERT_WORDS
+
+	"add	sp, sp, #232\n\t"
+	"pop	{ r1, r2, r3, r4, r5, r6, r7, r8, r10, r11, r12, pc }\n\t"
+	"\n\t"
+".process_block_RC:\n\t"
+	".word	0x00000001\n\t"
+	".word	0x00000000\n\t"
+	".word	0x00008082\n\t"
+	".word	0x00000000\n\t"
+	".word	0x0000808A\n\t"
+	".word	0x80000000\n\t"
+	".word	0x80008000\n\t"
+	".word	0x80000000\n\t"
+	".word	0x0000808B\n\t"
+	".word	0x00000000\n\t"
+	".word	0x80000001\n\t"
+	".word	0x00000000\n\t"
+	".word	0x80008081\n\t"
+	".word	0x80000000\n\t"
+	".word	0x00008009\n\t"
+	".word	0x80000000\n\t"
+	".word	0x0000008A\n\t"
+	".word	0x00000000\n\t"
+	".word	0x00000088\n\t"
+	".word	0x00000000\n\t"
+	".word	0x80008009\n\t"
+	".word	0x00000000\n\t"
+	".word	0x8000000A\n\t"
+	".word	0x00000000\n\t"
+	".word	0x8000808B\n\t"
+	".word	0x00000000\n\t"
+	".word	0x0000008B\n\t"
+	".word	0x80000000\n\t"
+	".word	0x00008089\n\t"
+	".word	0x80000000\n\t"
+	".word	0x00008003\n\t"
+	".word	0x80000000\n\t"
+	".word	0x00008002\n\t"
+	".word	0x80000000\n\t"
+	".word	0x00000080\n\t"
+	".word	0x80000000\n\t"
+	".word	0x0000800A\n\t"
+	".word	0x00000000\n\t"
+	".word	0x8000000A\n\t"
+	".word	0x80000000\n\t"
+	".word	0x80008081\n\t"
+	".word	0x80000000\n\t"
+	".word	0x00008080\n\t"
+	".word	0x80000000\n\t"
+	".word	0x80000001\n\t"
+	".word	0x00000000\n\t"
+	".word	0x80008008\n\t"
+	".word	0x80000000\n\t"
+
+#undef INVERT_WORDS
+#undef KHI_LOAD
+#undef KHI_STEP
+
+	);
+}
+#else // NTRUGEN_ASM_CORTEXM4
+
+/*
+ * Round constants.
+ */
+static const uint64_t RC[] = {
+	0x0000000000000001, 0x0000000000008082,
+	0x800000000000808A, 0x8000000080008000,
+	0x000000000000808B, 0x0000000080000001,
+	0x8000000080008081, 0x8000000000008009,
+	0x000000000000008A, 0x0000000000000088,
+	0x0000000080008009, 0x000000008000000A,
+	0x000000008000808B, 0x800000000000008B,
+	0x8000000000008089, 0x8000000000008003,
+	0x8000000000008002, 0x8000000000000080,
+	0x000000000000800A, 0x800000008000000A,
+	0x8000000080008081, 0x8000000000008080,
+	0x0000000080000001, 0x8000000080008008
+};
+
+/*
+ * Process the provided state.
+ */
+static void
+process_block(uint64_t *A)
+{
+	uint64_t t0, t1, t2, t3, t4;
+	uint64_t tt0, tt1, tt2, tt3;
+	uint64_t t, kt;
+	uint64_t c0, c1, c2, c3, c4, bnn;
+	int j;
+
+	/*
+	 * Invert some words (alternate internal representation, which
+	 * saves some operations).
+	 */
+	A[ 1] = ~A[ 1];
+	A[ 2] = ~A[ 2];
+	A[ 8] = ~A[ 8];
+	A[12] = ~A[12];
+	A[17] = ~A[17];
+	A[20] = ~A[20];
+
+	/*
+	 * Compute the 24 rounds. This loop is partially unrolled (each
+	 * iteration computes two rounds).
+	 */
+	for (j = 0; j < 24; j += 2) {
+		tt0 = A[ 1] ^ A[ 6];
+		tt1 = A[11] ^ A[16];
+		tt0 ^= A[21] ^ tt1;
+		tt0 = (tt0 << 1) | (tt0 >> 63);
+		tt2 = A[ 4] ^ A[ 9];
+		tt3 = A[14] ^ A[19];
+		tt0 ^= A[24];
+		tt2 ^= tt3;
+		t0 = tt0 ^ tt2;
+
+		tt0 = A[ 2] ^ A[ 7];
+		tt1 = A[12] ^ A[17];
+		tt0 ^= A[22] ^ tt1;
+		tt0 = (tt0 << 1) | (tt0 >> 63);
+		tt2 = A[ 0] ^ A[ 5];
+		tt3 = A[10] ^ A[15];
+		tt0 ^= A[20];
+		tt2 ^= tt3;
+		t1 = tt0 ^ tt2;
+
+		tt0 = A[ 3] ^ A[ 8];
+		tt1 = A[13] ^ A[18];
+		tt0 ^= A[23] ^ tt1;
+		tt0 = (tt0 << 1) | (tt0 >> 63);
+		tt2 = A[ 1] ^ A[ 6];
+		tt3 = A[11] ^ A[16];
+		tt0 ^= A[21];
+		tt2 ^= tt3;
+		t2 = tt0 ^ tt2;
+
+		tt0 = A[ 4] ^ A[ 9];
+		tt1 = A[14] ^ A[19];
+		tt0 ^= A[24] ^ tt1;
+		tt0 = (tt0 << 1) | (tt0 >> 63);
+		tt2 = A[ 2] ^ A[ 7];
+		tt3 = A[12] ^ A[17];
+		tt0 ^= A[22];
+		tt2 ^= tt3;
+		t3 = tt0 ^ tt2;
+
+		tt0 = A[ 0] ^ A[ 5];
+		tt1 = A[10] ^ A[15];
+		tt0 ^= A[20] ^ tt1;
+		tt0 = (tt0 << 1) | (tt0 >> 63);
+		tt2 = A[ 3] ^ A[ 8];
+		tt3 = A[13] ^ A[18];
+		tt0 ^= A[23];
+		tt2 ^= tt3;
+		t4 = tt0 ^ tt2;
+
+		A[ 0] = A[ 0] ^ t0;
+		A[ 5] = A[ 5] ^ t0;
+		A[10] = A[10] ^ t0;
+		A[15] = A[15] ^ t0;
+		A[20] = A[20] ^ t0;
+		A[ 1] = A[ 1] ^ t1;
+		A[ 6] = A[ 6] ^ t1;
+		A[11] = A[11] ^ t1;
+		A[16] = A[16] ^ t1;
+		A[21] = A[21] ^ t1;
+		A[ 2] = A[ 2] ^ t2;
+		A[ 7] = A[ 7] ^ t2;
+		A[12] = A[12] ^ t2;
+		A[17] = A[17] ^ t2;
+		A[22] = A[22] ^ t2;
+		A[ 3] = A[ 3] ^ t3;
+		A[ 8] = A[ 8] ^ t3;
+		A[13] = A[13] ^ t3;
+		A[18] = A[18] ^ t3;
+		A[23] = A[23] ^ t3;
+		A[ 4] = A[ 4] ^ t4;
+		A[ 9] = A[ 9] ^ t4;
+		A[14] = A[14] ^ t4;
+		A[19] = A[19] ^ t4;
+		A[24] = A[24] ^ t4;
+		A[ 5] = (A[ 5] << 36) | (A[ 5] >> (64 - 36));
+		A[10] = (A[10] <<  3) | (A[10] >> (64 -  3));
+		A[15] = (A[15] << 41) | (A[15] >> (64 - 41));
+		A[20] = (A[20] << 18) | (A[20] >> (64 - 18));
+		A[ 1] = (A[ 1] <<  1) | (A[ 1] >> (64 -  1));
+		A[ 6] = (A[ 6] << 44) | (A[ 6] >> (64 - 44));
+		A[11] = (A[11] << 10) | (A[11] >> (64 - 10));
+		A[16] = (A[16] << 45) | (A[16] >> (64 - 45));
+		A[21] = (A[21] <<  2) | (A[21] >> (64 - 2));
+		A[ 2] = (A[ 2] << 62) | (A[ 2] >> (64 - 62));
+		A[ 7] = (A[ 7] <<  6) | (A[ 7] >> (64 -  6));
+		A[12] = (A[12] << 43) | (A[12] >> (64 - 43));
+		A[17] = (A[17] << 15) | (A[17] >> (64 - 15));
+		A[22] = (A[22] << 61) | (A[22] >> (64 - 61));
+		A[ 3] = (A[ 3] << 28) | (A[ 3] >> (64 - 28));
+		A[ 8] = (A[ 8] << 55) | (A[ 8] >> (64 - 55));
+		A[13] = (A[13] << 25) | (A[13] >> (64 - 25));
+		A[18] = (A[18] << 21) | (A[18] >> (64 - 21));
+		A[23] = (A[23] << 56) | (A[23] >> (64 - 56));
+		A[ 4] = (A[ 4] << 27) | (A[ 4] >> (64 - 27));
+		A[ 9] = (A[ 9] << 20) | (A[ 9] >> (64 - 20));
+		A[14] = (A[14] << 39) | (A[14] >> (64 - 39));
+		A[19] = (A[19] <<  8) | (A[19] >> (64 -  8));
+		A[24] = (A[24] << 14) | (A[24] >> (64 - 14));
+
+		bnn = ~A[12];
+		kt = A[ 6] | A[12];
+		c0 = A[ 0] ^ kt;
+		kt = bnn | A[18];
+		c1 = A[ 6] ^ kt;
+		kt = A[18] & A[24];
+		c2 = A[12] ^ kt;
+		kt = A[24] | A[ 0];
+		c3 = A[18] ^ kt;
+		kt = A[ 0] & A[ 6];
+		c4 = A[24] ^ kt;
+		A[ 0] = c0;
+		A[ 6] = c1;
+		A[12] = c2;
+		A[18] = c3;
+		A[24] = c4;
+		bnn = ~A[22];
+		kt = A[ 9] | A[10];
+		c0 = A[ 3] ^ kt;
+		kt = A[10] & A[16];
+		c1 = A[ 9] ^ kt;
+		kt = A[16] | bnn;
+		c2 = A[10] ^ kt;
+		kt = A[22] | A[ 3];
+		c3 = A[16] ^ kt;
+		kt = A[ 3] & A[ 9];
+		c4 = A[22] ^ kt;
+		A[ 3] = c0;
+		A[ 9] = c1;
+		A[10] = c2;
+		A[16] = c3;
+		A[22] = c4;
+		bnn = ~A[19];
+		kt = A[ 7] | A[13];
+		c0 = A[ 1] ^ kt;
+		kt = A[13] & A[19];
+		c1 = A[ 7] ^ kt;
+		kt = bnn & A[20];
+		c2 = A[13] ^ kt;
+		kt = A[20] | A[ 1];
+		c3 = bnn ^ kt;
+		kt = A[ 1] & A[ 7];
+		c4 = A[20] ^ kt;
+		A[ 1] = c0;
+		A[ 7] = c1;
+		A[13] = c2;
+		A[19] = c3;
+		A[20] = c4;
+		bnn = ~A[17];
+		kt = A[ 5] & A[11];
+		c0 = A[ 4] ^ kt;
+		kt = A[11] | A[17];
+		c1 = A[ 5] ^ kt;
+		kt = bnn | A[23];
+		c2 = A[11] ^ kt;
+		kt = A[23] & A[ 4];
+		c3 = bnn ^ kt;
+		kt = A[ 4] | A[ 5];
+		c4 = A[23] ^ kt;
+		A[ 4] = c0;
+		A[ 5] = c1;
+		A[11] = c2;
+		A[17] = c3;
+		A[23] = c4;
+		bnn = ~A[ 8];
+		kt = bnn & A[14];
+		c0 = A[ 2] ^ kt;
+		kt = A[14] | A[15];
+		c1 = bnn ^ kt;
+		kt = A[15] & A[21];
+		c2 = A[14] ^ kt;
+		kt = A[21] | A[ 2];
+		c3 = A[15] ^ kt;
+		kt = A[ 2] & A[ 8];
+		c4 = A[21] ^ kt;
+		A[ 2] = c0;
+		A[ 8] = c1;
+		A[14] = c2;
+		A[15] = c3;
+		A[21] = c4;
+		A[ 0] = A[ 0] ^ RC[j + 0];
+
+		tt0 = A[ 6] ^ A[ 9];
+		tt1 = A[ 7] ^ A[ 5];
+		tt0 ^= A[ 8] ^ tt1;
+		tt0 = (tt0 << 1) | (tt0 >> 63);
+		tt2 = A[24] ^ A[22];
+		tt3 = A[20] ^ A[23];
+		tt0 ^= A[21];
+		tt2 ^= tt3;
+		t0 = tt0 ^ tt2;
+
+		tt0 = A[12] ^ A[10];
+		tt1 = A[13] ^ A[11];
+		tt0 ^= A[14] ^ tt1;
+		tt0 = (tt0 << 1) | (tt0 >> 63);
+		tt2 = A[ 0] ^ A[ 3];
+		tt3 = A[ 1] ^ A[ 4];
+		tt0 ^= A[ 2];
+		tt2 ^= tt3;
+		t1 = tt0 ^ tt2;
+
+		tt0 = A[18] ^ A[16];
+		tt1 = A[19] ^ A[17];
+		tt0 ^= A[15] ^ tt1;
+		tt0 = (tt0 << 1) | (tt0 >> 63);
+		tt2 = A[ 6] ^ A[ 9];
+		tt3 = A[ 7] ^ A[ 5];
+		tt0 ^= A[ 8];
+		tt2 ^= tt3;
+		t2 = tt0 ^ tt2;
+
+		tt0 = A[24] ^ A[22];
+		tt1 = A[20] ^ A[23];
+		tt0 ^= A[21] ^ tt1;
+		tt0 = (tt0 << 1) | (tt0 >> 63);
+		tt2 = A[12] ^ A[10];
+		tt3 = A[13] ^ A[11];
+		tt0 ^= A[14];
+		tt2 ^= tt3;
+		t3 = tt0 ^ tt2;
+
+		tt0 = A[ 0] ^ A[ 3];
+		tt1 = A[ 1] ^ A[ 4];
+		tt0 ^= A[ 2] ^ tt1;
+		tt0 = (tt0 << 1) | (tt0 >> 63);
+		tt2 = A[18] ^ A[16];
+		tt3 = A[19] ^ A[17];
+		tt0 ^= A[15];
+		tt2 ^= tt3;
+		t4 = tt0 ^ tt2;
+
+		A[ 0] = A[ 0] ^ t0;
+		A[ 3] = A[ 3] ^ t0;
+		A[ 1] = A[ 1] ^ t0;
+		A[ 4] = A[ 4] ^ t0;
+		A[ 2] = A[ 2] ^ t0;
+		A[ 6] = A[ 6] ^ t1;
+		A[ 9] = A[ 9] ^ t1;
+		A[ 7] = A[ 7] ^ t1;
+		A[ 5] = A[ 5] ^ t1;
+		A[ 8] = A[ 8] ^ t1;
+		A[12] = A[12] ^ t2;
+		A[10] = A[10] ^ t2;
+		A[13] = A[13] ^ t2;
+		A[11] = A[11] ^ t2;
+		A[14] = A[14] ^ t2;
+		A[18] = A[18] ^ t3;
+		A[16] = A[16] ^ t3;
+		A[19] = A[19] ^ t3;
+		A[17] = A[17] ^ t3;
+		A[15] = A[15] ^ t3;
+		A[24] = A[24] ^ t4;
+		A[22] = A[22] ^ t4;
+		A[20] = A[20] ^ t4;
+		A[23] = A[23] ^ t4;
+		A[21] = A[21] ^ t4;
+		A[ 3] = (A[ 3] << 36) | (A[ 3] >> (64 - 36));
+		A[ 1] = (A[ 1] <<  3) | (A[ 1] >> (64 -  3));
+		A[ 4] = (A[ 4] << 41) | (A[ 4] >> (64 - 41));
+		A[ 2] = (A[ 2] << 18) | (A[ 2] >> (64 - 18));
+		A[ 6] = (A[ 6] <<  1) | (A[ 6] >> (64 -  1));
+		A[ 9] = (A[ 9] << 44) | (A[ 9] >> (64 - 44));
+		A[ 7] = (A[ 7] << 10) | (A[ 7] >> (64 - 10));
+		A[ 5] = (A[ 5] << 45) | (A[ 5] >> (64 - 45));
+		A[ 8] = (A[ 8] <<  2) | (A[ 8] >> (64 - 2));
+		A[12] = (A[12] << 62) | (A[12] >> (64 - 62));
+		A[10] = (A[10] <<  6) | (A[10] >> (64 -  6));
+		A[13] = (A[13] << 43) | (A[13] >> (64 - 43));
+		A[11] = (A[11] << 15) | (A[11] >> (64 - 15));
+		A[14] = (A[14] << 61) | (A[14] >> (64 - 61));
+		A[18] = (A[18] << 28) | (A[18] >> (64 - 28));
+		A[16] = (A[16] << 55) | (A[16] >> (64 - 55));
+		A[19] = (A[19] << 25) | (A[19] >> (64 - 25));
+		A[17] = (A[17] << 21) | (A[17] >> (64 - 21));
+		A[15] = (A[15] << 56) | (A[15] >> (64 - 56));
+		A[24] = (A[24] << 27) | (A[24] >> (64 - 27));
+		A[22] = (A[22] << 20) | (A[22] >> (64 - 20));
+		A[20] = (A[20] << 39) | (A[20] >> (64 - 39));
+		A[23] = (A[23] <<  8) | (A[23] >> (64 -  8));
+		A[21] = (A[21] << 14) | (A[21] >> (64 - 14));
+
+		bnn = ~A[13];
+		kt = A[ 9] | A[13];
+		c0 = A[ 0] ^ kt;
+		kt = bnn | A[17];
+		c1 = A[ 9] ^ kt;
+		kt = A[17] & A[21];
+		c2 = A[13] ^ kt;
+		kt = A[21] | A[ 0];
+		c3 = A[17] ^ kt;
+		kt = A[ 0] & A[ 9];
+		c4 = A[21] ^ kt;
+		A[ 0] = c0;
+		A[ 9] = c1;
+		A[13] = c2;
+		A[17] = c3;
+		A[21] = c4;
+		bnn = ~A[14];
+		kt = A[22] | A[ 1];
+		c0 = A[18] ^ kt;
+		kt = A[ 1] & A[ 5];
+		c1 = A[22] ^ kt;
+		kt = A[ 5] | bnn;
+		c2 = A[ 1] ^ kt;
+		kt = A[14] | A[18];
+		c3 = A[ 5] ^ kt;
+		kt = A[18] & A[22];
+		c4 = A[14] ^ kt;
+		A[18] = c0;
+		A[22] = c1;
+		A[ 1] = c2;
+		A[ 5] = c3;
+		A[14] = c4;
+		bnn = ~A[23];
+		kt = A[10] | A[19];
+		c0 = A[ 6] ^ kt;
+		kt = A[19] & A[23];
+		c1 = A[10] ^ kt;
+		kt = bnn & A[ 2];
+		c2 = A[19] ^ kt;
+		kt = A[ 2] | A[ 6];
+		c3 = bnn ^ kt;
+		kt = A[ 6] & A[10];
+		c4 = A[ 2] ^ kt;
+		A[ 6] = c0;
+		A[10] = c1;
+		A[19] = c2;
+		A[23] = c3;
+		A[ 2] = c4;
+		bnn = ~A[11];
+		kt = A[ 3] & A[ 7];
+		c0 = A[24] ^ kt;
+		kt = A[ 7] | A[11];
+		c1 = A[ 3] ^ kt;
+		kt = bnn | A[15];
+		c2 = A[ 7] ^ kt;
+		kt = A[15] & A[24];
+		c3 = bnn ^ kt;
+		kt = A[24] | A[ 3];
+		c4 = A[15] ^ kt;
+		A[24] = c0;
+		A[ 3] = c1;
+		A[ 7] = c2;
+		A[11] = c3;
+		A[15] = c4;
+		bnn = ~A[16];
+		kt = bnn & A[20];
+		c0 = A[12] ^ kt;
+		kt = A[20] | A[ 4];
+		c1 = bnn ^ kt;
+		kt = A[ 4] & A[ 8];
+		c2 = A[20] ^ kt;
+		kt = A[ 8] | A[12];
+		c3 = A[ 4] ^ kt;
+		kt = A[12] & A[16];
+		c4 = A[ 8] ^ kt;
+		A[12] = c0;
+		A[16] = c1;
+		A[20] = c2;
+		A[ 4] = c3;
+		A[ 8] = c4;
+		A[ 0] = A[ 0] ^ RC[j + 1];
+
+		t = A[ 5];
+		A[ 5] = A[18];
+		A[18] = A[11];
+		A[11] = A[10];
+		A[10] = A[ 6];
+		A[ 6] = A[22];
+		A[22] = A[20];
+		A[20] = A[12];
+		A[12] = A[19];
+		A[19] = A[15];
+		A[15] = A[24];
+		A[24] = A[ 8];
+		A[ 8] = t;
+		t = A[ 1];
+		A[ 1] = A[ 9];
+		A[ 9] = A[14];
+		A[14] = A[ 2];
+		A[ 2] = A[13];
+		A[13] = A[23];
+		A[23] = A[ 4];
+		A[ 4] = A[21];
+		A[21] = A[16];
+		A[16] = A[ 3];
+		A[ 3] = A[17];
+		A[17] = A[ 7];
+		A[ 7] = t;
+	}
+
+	/*
+	 * Invert some words back to normal representation.
+	 */
+	A[ 1] = ~A[ 1];
+	A[ 2] = ~A[ 2];
+	A[ 8] = ~A[ 8];
+	A[12] = ~A[12];
+	A[17] = ~A[17];
+	A[20] = ~A[20];
+}
+#endif // NTRUGEN_ASM_CORTEXM4
+
+/* see sha3.h */
+void
+shake_init(shake_context *sc, unsigned size)
+{
+	sc->rate = 200 - (size_t)(size >> 2);
+	sc->dptr = 0;
+	memset(sc->A, 0, sizeof sc->A);
+}
+
+/* see sha3.h */
+void
+shake_inject(shake_context *sc, const void *in, size_t len)
+{
+	size_t dptr, rate;
+	const uint8_t *buf;
+
+	dptr = sc->dptr;
+	rate = sc->rate;
+	buf = in;
+	while (len > 0) {
+		size_t clen, u;
+
+		clen = rate - dptr;
+		if (clen > len) {
+			clen = len;
+		}
+		for (u = 0; u < clen; u ++) {
+			size_t v;
+
+			v = u + dptr;
+			sc->A[v >> 3] ^= (uint64_t)buf[u] << ((v & 7) << 3);
+		}
+		dptr += clen;
+		buf += clen;
+		len -= clen;
+		if (dptr == rate) {
+			process_block(sc->A);
+			dptr = 0;
+		}
+	}
+	sc->dptr = (unsigned)dptr;
+}
+
+/* see sha3.h */
+void
+shake_flip(shake_context *sc)
+{
+	/*
+	 * We apply padding and pre-XOR the value into the state. We
+	 * set dptr to the end of the buffer, so that first call to
+	 * shake_extract() will process the block.
+	 */
+	unsigned v;
+
+	v = (unsigned)sc->dptr;
+	sc->A[v >> 3] ^= (uint64_t)0x1F << ((v & 7) << 3);
+	v = (unsigned)sc->rate - 1;
+	sc->A[v >> 3] ^= (uint64_t)0x80 << ((v & 7) << 3);
+	sc->dptr = sc->rate;
+}
+
+/* see sha3.h */
+void
+shake_extract(shake_context *sc, void *out, size_t len)
+{
+	size_t dptr, rate;
+	uint8_t *buf;
+
+	dptr = sc->dptr;
+	rate = sc->rate;
+	buf = out;
+	while (len > 0) {
+		size_t clen;
+
+		if (dptr == rate) {
+			process_block(sc->A);
+			dptr = 0;
+		}
+		clen = rate - dptr;
+		if (clen > len) {
+			clen = len;
+		}
+		len -= clen;
+		while (clen -- > 0) {
+			*buf ++ = (uint8_t)(sc->A[dptr >> 3]
+				>> ((dptr & 7) << 3));
+			dptr ++;
+		}
+	}
+	sc->dptr = (unsigned)dptr;
+}
+
+#if NTRUGEN_AVX2
+
+TARGET_AVX2
+static void
+process_block_x4(uint64_t *A)
+{
+	__m256i ya[25];
+
+	for (int i = 0; i < 25; i ++) {
+		ya[i] = _mm256_loadu_si256((const __m256i *)A + i);
+	}
+
+	/*
+	 * Invert some words (alternate internal representation, which
+	 * saves some operations).
+	 */
+	__m256i yones = _mm256_set1_epi32(-1);
+	ya[ 1] = _mm256_xor_si256(ya[ 1], yones);
+	ya[ 2] = _mm256_xor_si256(ya[ 2], yones);
+	ya[ 8] = _mm256_xor_si256(ya[ 8], yones);
+	ya[12] = _mm256_xor_si256(ya[12], yones);
+	ya[17] = _mm256_xor_si256(ya[17], yones);
+	ya[20] = _mm256_xor_si256(ya[20], yones);
+
+	/*
+	 * Compute the 24 rounds. This loop is partially unrolled (each
+	 * iteration computes two rounds).
+	 */
+	for (int j = 0; j < 24; j += 2) {
+		__m256i yt0, yt1, yt2, yt3, yt4;
+
+#define yy_rotl(yv, nn)   _mm256_or_si256( \
+	_mm256_slli_epi64(yv, nn), _mm256_srli_epi64(yv, 64 - (nn)))
+#define yy_or(a, b)        _mm256_or_si256(a, b)
+#define yy_ornotL(a, b)    _mm256_or_si256(_mm256_xor_si256(a, yones), b)
+#define yy_ornotR(a, b)    _mm256_or_si256(a, _mm256_xor_si256(b, yones))
+#define yy_and(a, b)       _mm256_and_si256(a, b)
+#define yy_andnotL(a, b)   _mm256_andnot_si256(a, b)
+#define yy_andnotR(a, b)   _mm256_andnot_si256(b, a)
+#define yy_xor(a, b)       _mm256_xor_si256(a, b)
+
+#define yCOMB1(yd, i0, i1, i2, i3, i4, i5, i6, i7, i8, i9)   do { \
+		__m256i ytt0, ytt1, ytt2, ytt3; \
+		ytt0 = yy_xor(ya[i0], ya[i1]); \
+		ytt1 = yy_xor(ya[i2], ya[i3]); \
+		ytt0 = yy_xor(ytt0, yy_xor(ya[i4], ytt1)); \
+		ytt0 = yy_rotl(ytt0, 1); \
+		ytt2 = yy_xor(ya[i5], ya[i6]); \
+		ytt3 = yy_xor(ya[i7], ya[i8]); \
+		ytt0 = yy_xor(ytt0, ya[i9]); \
+		ytt2 = yy_xor(ytt2, ytt3); \
+		yd = yy_xor(ytt0, ytt2); \
+	} while (0)
+
+#define yCOMB2(i0, i1, i2, i3, i4, op0, op1, op2, op3, op4)   do { \
+		__m256i yc0, yc1, yc2, yc3, yc4, ykt; \
+		ykt = yy_ ## op0(ya[i1], ya[i2]); \
+		yc0 = yy_xor(ykt, ya[i0]); \
+		ykt = yy_ ## op1(ya[i2], ya[i3]); \
+		yc1 = yy_xor(ykt, ya[i1]); \
+		ykt = yy_ ## op2(ya[i3], ya[i4]); \
+		yc2 = yy_xor(ykt, ya[i2]); \
+		ykt = yy_ ## op3(ya[i4], ya[i0]); \
+		yc3 = yy_xor(ykt, ya[i3]); \
+		ykt = yy_ ## op4(ya[i0], ya[i1]); \
+		yc4 = yy_xor(ykt, ya[i4]); \
+		ya[i0] = yc0; \
+		ya[i1] = yc1; \
+		ya[i2] = yc2; \
+		ya[i3] = yc3; \
+		ya[i4] = yc4; \
+	} while (0)
+
+		/* Round j */
+
+		yCOMB1(yt0, 1, 6, 11, 16, 21, 4, 9, 14, 19, 24);
+		yCOMB1(yt1, 2, 7, 12, 17, 22, 0, 5, 10, 15, 20);
+		yCOMB1(yt2, 3, 8, 13, 18, 23, 1, 6, 11, 16, 21);
+		yCOMB1(yt3, 4, 9, 14, 19, 24, 2, 7, 12, 17, 22);
+		yCOMB1(yt4, 0, 5, 10, 15, 20, 3, 8, 13, 18, 23);
+
+		ya[ 0] = yy_xor(ya[ 0], yt0);
+		ya[ 5] = yy_xor(ya[ 5], yt0);
+		ya[10] = yy_xor(ya[10], yt0);
+		ya[15] = yy_xor(ya[15], yt0);
+		ya[20] = yy_xor(ya[20], yt0);
+		ya[ 1] = yy_xor(ya[ 1], yt1);
+		ya[ 6] = yy_xor(ya[ 6], yt1);
+		ya[11] = yy_xor(ya[11], yt1);
+		ya[16] = yy_xor(ya[16], yt1);
+		ya[21] = yy_xor(ya[21], yt1);
+		ya[ 2] = yy_xor(ya[ 2], yt2);
+		ya[ 7] = yy_xor(ya[ 7], yt2);
+		ya[12] = yy_xor(ya[12], yt2);
+		ya[17] = yy_xor(ya[17], yt2);
+		ya[22] = yy_xor(ya[22], yt2);
+		ya[ 3] = yy_xor(ya[ 3], yt3);
+		ya[ 8] = yy_xor(ya[ 8], yt3);
+		ya[13] = yy_xor(ya[13], yt3);
+		ya[18] = yy_xor(ya[18], yt3);
+		ya[23] = yy_xor(ya[23], yt3);
+		ya[ 4] = yy_xor(ya[ 4], yt4);
+		ya[ 9] = yy_xor(ya[ 9], yt4);
+		ya[14] = yy_xor(ya[14], yt4);
+		ya[19] = yy_xor(ya[19], yt4);
+		ya[24] = yy_xor(ya[24], yt4);
+		ya[ 5] = yy_rotl(ya[ 5], 36);
+		ya[10] = yy_rotl(ya[10],  3);
+		ya[15] = yy_rotl(ya[15], 41);
+		ya[20] = yy_rotl(ya[20], 18);
+		ya[ 1] = yy_rotl(ya[ 1],  1);
+		ya[ 6] = yy_rotl(ya[ 6], 44);
+		ya[11] = yy_rotl(ya[11], 10);
+		ya[16] = yy_rotl(ya[16], 45);
+		ya[21] = yy_rotl(ya[21],  2);
+		ya[ 2] = yy_rotl(ya[ 2], 62);
+		ya[ 7] = yy_rotl(ya[ 7],  6);
+		ya[12] = yy_rotl(ya[12], 43);
+		ya[17] = yy_rotl(ya[17], 15);
+		ya[22] = yy_rotl(ya[22], 61);
+		ya[ 3] = yy_rotl(ya[ 3], 28);
+		ya[ 8] = yy_rotl(ya[ 8], 55);
+		ya[13] = yy_rotl(ya[13], 25);
+		ya[18] = yy_rotl(ya[18], 21);
+		ya[23] = yy_rotl(ya[23], 56);
+		ya[ 4] = yy_rotl(ya[ 4], 27);
+		ya[ 9] = yy_rotl(ya[ 9], 20);
+		ya[14] = yy_rotl(ya[14], 39);
+		ya[19] = yy_rotl(ya[19],  8);
+		ya[24] = yy_rotl(ya[24], 14);
+
+		yCOMB2(0, 6, 12, 18, 24, or, ornotL, and, or, and);
+		yCOMB2(3, 9, 10, 16, 22, or, and, ornotR, or, and);
+		ya[19] = yy_xor(ya[19], yones);
+		yCOMB2(1, 7, 13, 19, 20, or, andnotR, and, or, and);
+		ya[17] = yy_xor(ya[17], yones);
+		yCOMB2(4, 5, 11, 17, 23, and, ornotR, or, and, or);
+		ya[8] = yy_xor(ya[8], yones);
+		yCOMB2(2, 8, 14, 15, 21, and, or, and, or, andnotR);
+
+		ya[0] = yy_xor(ya[0], _mm256_set1_epi64x(RC[j + 0]));
+
+		/* Round j + 1 */
+
+		yCOMB1(yt0, 6, 9, 7, 5, 8, 24, 22, 20, 23, 21);
+		yCOMB1(yt1, 12, 10, 13, 11, 14, 0, 3, 1, 4, 2);
+		yCOMB1(yt2, 18, 16, 19, 17, 15, 6, 9, 7, 5, 8);
+		yCOMB1(yt3, 24, 22, 20, 23, 21, 12, 10, 13, 11, 14);
+		yCOMB1(yt4, 0, 3, 1, 4, 2, 18, 16, 19, 17, 15);
+
+		ya[ 0] = yy_xor(ya[ 0], yt0);
+		ya[ 3] = yy_xor(ya[ 3], yt0);
+		ya[ 1] = yy_xor(ya[ 1], yt0);
+		ya[ 4] = yy_xor(ya[ 4], yt0);
+		ya[ 2] = yy_xor(ya[ 2], yt0);
+		ya[ 6] = yy_xor(ya[ 6], yt1);
+		ya[ 9] = yy_xor(ya[ 9], yt1);
+		ya[ 7] = yy_xor(ya[ 7], yt1);
+		ya[ 5] = yy_xor(ya[ 5], yt1);
+		ya[ 8] = yy_xor(ya[ 8], yt1);
+		ya[12] = yy_xor(ya[12], yt2);
+		ya[10] = yy_xor(ya[10], yt2);
+		ya[13] = yy_xor(ya[13], yt2);
+		ya[11] = yy_xor(ya[11], yt2);
+		ya[14] = yy_xor(ya[14], yt2);
+		ya[18] = yy_xor(ya[18], yt3);
+		ya[16] = yy_xor(ya[16], yt3);
+		ya[19] = yy_xor(ya[19], yt3);
+		ya[17] = yy_xor(ya[17], yt3);
+		ya[15] = yy_xor(ya[15], yt3);
+		ya[24] = yy_xor(ya[24], yt4);
+		ya[22] = yy_xor(ya[22], yt4);
+		ya[20] = yy_xor(ya[20], yt4);
+		ya[23] = yy_xor(ya[23], yt4);
+		ya[21] = yy_xor(ya[21], yt4);
+		ya[ 3] = yy_rotl(ya[ 3], 36);
+		ya[ 1] = yy_rotl(ya[ 1],  3);
+		ya[ 4] = yy_rotl(ya[ 4], 41);
+		ya[ 2] = yy_rotl(ya[ 2], 18);
+		ya[ 6] = yy_rotl(ya[ 6],  1);
+		ya[ 9] = yy_rotl(ya[ 9], 44);
+		ya[ 7] = yy_rotl(ya[ 7], 10);
+		ya[ 5] = yy_rotl(ya[ 5], 45);
+		ya[ 8] = yy_rotl(ya[ 8],  2);
+		ya[12] = yy_rotl(ya[12], 62);
+		ya[10] = yy_rotl(ya[10],  6);
+		ya[13] = yy_rotl(ya[13], 43);
+		ya[11] = yy_rotl(ya[11], 15);
+		ya[14] = yy_rotl(ya[14], 61);
+		ya[18] = yy_rotl(ya[18], 28);
+		ya[16] = yy_rotl(ya[16], 55);
+		ya[19] = yy_rotl(ya[19], 25);
+		ya[17] = yy_rotl(ya[17], 21);
+		ya[15] = yy_rotl(ya[15], 56);
+		ya[24] = yy_rotl(ya[24], 27);
+		ya[22] = yy_rotl(ya[22], 20);
+		ya[20] = yy_rotl(ya[20], 39);
+		ya[23] = yy_rotl(ya[23],  8);
+		ya[21] = yy_rotl(ya[21], 14);
+
+		yCOMB2(0, 9, 13, 17, 21, or, ornotL, and, or, and);
+		yCOMB2(18, 22, 1, 5, 14, or, and, ornotR, or, and);
+		ya[23] = yy_xor(ya[23], yones);
+		yCOMB2(6, 10, 19, 23, 2, or, andnotR, and, or, and);
+		ya[11] = yy_xor(ya[11], yones);
+		yCOMB2(24, 3, 7, 11, 15, and, ornotR, or, and, or);
+		ya[16] = yy_xor(ya[16], yones);
+		yCOMB2(12, 16, 20, 4, 8, and, or, and, or, andnotR);
+
+		ya[0] = yy_xor(ya[0], _mm256_set1_epi64x(RC[j + 1]));
+
+		/* Apply combined permutation for next round */
+
+		__m256i yt = ya[ 5];
+		ya[ 5] = ya[18];
+		ya[18] = ya[11];
+		ya[11] = ya[10];
+		ya[10] = ya[ 6];
+		ya[ 6] = ya[22];
+		ya[22] = ya[20];
+		ya[20] = ya[12];
+		ya[12] = ya[19];
+		ya[19] = ya[15];
+		ya[15] = ya[24];
+		ya[24] = ya[ 8];
+		ya[ 8] = yt;
+		yt = ya[ 1];
+		ya[ 1] = ya[ 9];
+		ya[ 9] = ya[14];
+		ya[14] = ya[ 2];
+		ya[ 2] = ya[13];
+		ya[13] = ya[23];
+		ya[23] = ya[ 4];
+		ya[ 4] = ya[21];
+		ya[21] = ya[16];
+		ya[16] = ya[ 3];
+                ya[ 3] = ya[17];
+                ya[17] = ya[ 7];
+                ya[ 7] = yt;
+
+#undef yy_rotl
+#undef yy_or
+#undef yy_ornotL
+#undef yy_ornotR
+#undef yy_and
+#undef yy_andnotL
+#undef yy_andnotR
+#undef yy_xor
+#undef yCOMB1
+#undef yCOMB2
+	}
+
+	/*
+	 * Invert some words back to normal representation.
+	 */
+	ya[ 1] = _mm256_xor_si256(ya[ 1], yones);
+	ya[ 2] = _mm256_xor_si256(ya[ 2], yones);
+	ya[ 8] = _mm256_xor_si256(ya[ 8], yones);
+	ya[12] = _mm256_xor_si256(ya[12], yones);
+	ya[17] = _mm256_xor_si256(ya[17], yones);
+	ya[20] = _mm256_xor_si256(ya[20], yones);
+
+	/*
+	 * Write back state words.
+	 */
+	for (int i = 0; i < 25; i ++) {
+		_mm256_storeu_si256((__m256i *)A + i, ya[i]);
+	}
+}
+
+/* see sha3.h */
+void
+shake_x4_flip(shake_x4_context *scx4, const shake_context *sc)
+{
+	/*
+	 * We interleave the four contexts.
+	 */
+	for (int i = 0; i < 4; i ++) {
+		for (int j = 0; j < 25; j ++) {
+			scx4->A[i + (j << 2)] = sc[i].A[j];
+		}
+		unsigned v = (unsigned)sc[i].dptr;
+		scx4->A[i + ((v >> 3) << 2)] ^=
+			(uint64_t)0x1F << ((v & 7) << 3);
+		v = (unsigned)sc[i].rate - 1;
+		scx4->A[i + ((v >> 3) << 2)] ^=
+			(uint64_t)0x80 << ((v & 7) << 3);
+	}
+	scx4->dptr = scx4->rate = sc[0].rate;
+}
+
+/* see sha3.h */
+void
+shake_x4_extract_words(shake_x4_context *scx4, uint64_t *dst, size_t num_x4)
+{
+	size_t wptr = scx4->dptr >> 3;
+	size_t wrate = scx4->rate >> 3;
+	while (num_x4 > 0) {
+		if (wptr == wrate) {
+			process_block_x4(scx4->A);
+			wptr = 0;
+		}
+		size_t cnum = wrate - wptr;
+		if (cnum > num_x4) {
+			cnum = num_x4;
+		}
+		memcpy(dst, scx4->A + (wptr << 2), cnum << 5);
+		wptr += cnum;
+		dst += cnum << 2;
+		num_x4 -= cnum;
+	}
+	scx4->dptr = (unsigned)(wptr << 3);
+}
+
+#else // NTRUGEN_AVX2
+
+/* see sha3.h */
+void
+shake_x4_flip(shake_x4_context *scx4, const shake_context *sc)
+{
+	for (int i = 0; i < 4; i ++) {
+		shake_context sct = sc[i];
+		shake_flip(&sct);
+		memcpy(scx4->A + (i * 25), sct.A, 25 * sizeof(uint64_t));
+	}
+	scx4->dptr = scx4->rate = sc[0].rate;
+}
+
+/* see sha3.h */
+void
+shake_x4_extract_words(shake_x4_context *scx4, uint64_t *dst, size_t num_x4)
+{
+	size_t dptr = scx4->dptr;
+	size_t rate = scx4->rate;
+	while (num_x4 -- > 0) {
+		if (dptr == rate) {
+			for (int i = 0; i < 4; i ++) {
+				process_block(scx4->A + (i * 25));
+			}
+			dptr = 0;
+		}
+		for (int i = 0; i < 4; i ++) {
+			dst[i] = scx4->A[(i * 25) + (dptr >> 3)];
+		}
+		dptr += 8;
+		dst += 4;
+	}
+	scx4->dptr = (unsigned)dptr;
+}
+
+#endif // NTRUGEN_AVX2
+
+/* see sha3.h */
+void
+sha3_init(sha3_context *sc, unsigned size)
+{
+	shake_init(sc, size);
+}
+
+/* see sha3.h */
+void
+sha3_update(sha3_context *sc, const void *in, size_t len)
+{
+	shake_inject(sc, in, len);
+}
+
+/* see sha3.h */
+void
+sha3_close(sha3_context *sc, void *out)
+{
+	unsigned v;
+	uint8_t *buf;
+	size_t u, len;
+
+	/*
+	 * Apply padding. It differs from the SHAKE padding in that
+	 * we append '01', not '1111'.
+	 */
+	v = (unsigned)sc->dptr;
+	sc->A[v >> 3] ^= (uint64_t)0x06 << ((v & 7) << 3);
+	v = (unsigned)sc->rate - 1;
+	sc->A[v >> 3] ^= (uint64_t)0x80 << ((v & 7) << 3);
+
+	/*
+	 * Process the padded block.
+	 */
+	process_block(sc->A);
+
+	/*
+	 * Write output. Output length (in bytes) is obtained from the rate.
+	 */
+	buf = out;
+	len = (200 - sc->rate) >> 1;
+	for (u = 0; u < len; u ++) {
+		buf[u] = (uint8_t)(sc->A[u >> 3] >> ((u & 7) << 3));
+	}
+}
diff --git a/lib/dns/hawk/sha3.h b/lib/dns/hawk/sha3.h
new file mode 100644
index 0000000000..845eb83cca
--- /dev/null
+++ b/lib/dns/hawk/sha3.h
@@ -0,0 +1,126 @@
+#ifndef SHA3_H__
+#define SHA3_H__
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/*
+ * Context for a SHAKE computation. Contents are opaque.
+ * Contents are pure data with no pointer; they need not be released
+ * explicitly and don't reference any other allocated resource. The
+ * caller is responsible for allocating the context structure itself,
+ * typically on the stack.
+ * A running state can be cloned by copying the structure; this is
+ * useful if "partial hashes" (hash of data processed so far) are
+ * needed, without preventing injecting extra bytes later on.
+ */
+typedef struct {
+	uint64_t A[25];
+	unsigned dptr, rate;
+} shake_context;
+
+/*
+ * Initialize a SHAKE context to its initial state. The state is
+ * then ready to receive data (with shake_inject()).
+ *
+ * The "size" parameter should be 128 for SHAKE128, 256 for SHAKE256.
+ * This is half of the internal parameter known as "capacity" (SHAKE128
+ * works on an internal 256-bit capacity, SHAKE256 uses a 512-bit
+ * capacity).
+ */
+void shake_init(shake_context *sc, unsigned size);
+
+/*
+ * Inject some data bytes into the SHAKE context ("absorb" operation).
+ * This function can be called several times, to inject several chunks
+ * of data of arbitrary length.
+ */
+void shake_inject(shake_context *sc, const void *data, size_t len);
+
+/*
+ * Flip the SHAKE state to output mode. After this call, shake_inject()
+ * can no longer be called on the context, but shake_extract() can be
+ * called.
+ *
+ * Flipping is one-way; a given context can be converted back to input
+ * mode only by initializing it again, which forgets all previously
+ * injected data.
+ */
+void shake_flip(shake_context *sc);
+
+/*
+ * Extract bytes from the SHAKE context ("squeeze" operation). The
+ * context must have been flipped to output mode (with shake_flip()).
+ * Arbitrary amounts of data can be extracted, in one or several calls
+ * to this function.
+ */
+void shake_extract(shake_context *sc, void *out, size_t len);
+
+/*
+ * Structure holding four SHAKE context simultaneously. This is used for
+ * the "x4" mode, which outputs 64-bit words upon extraction, and may
+ * leverage architecture-specific vector instruction for better performance.
+ */
+typedef struct {
+	uint64_t A[100];
+	unsigned dptr, rate;
+} shake_x4_context;
+
+/*
+ * Flip four input SHAKE context to output mode. The four provided contexts
+ * are unmodified, but a new shake_x4_context structure is filled with
+ * their "flipped" counterparts. That structure may be used with
+ * shake_x4_extract_words() to obtain the four outputs as 64-bit words
+ * (the outputs are interleaved with a 64-bit granularity).
+ *
+ * All four input SHAKE context MUST use the same internal capacity
+ * (i.e. do not mix SHAKE128 and SHAKE256 contexts). The internal
+ * capacity MUST be a multiple of 64 bits.
+ */
+void shake_x4_flip(shake_x4_context *scx4, const shake_context *sc_in);
+
+/*
+ * Obtain num_x4 groups of four 64-bit words from the provided parallel
+ * SHAKE context. Each group contains one word from each of the four
+ * internal SHAKE instances, in the order they were provided when
+ * calling shake_x4_flip(). Each 64-bit word corresponds to the little
+ * endian interpretation of corresponding 8-byte output.
+ */
+void shake_x4_extract_words(shake_x4_context *scx4,
+	uint64_t *dst, size_t num_x4);
+
+/*
+ * Context for SHA3 computations. Contents are opaque.
+ * A running state can be cloned by copying the structure; this is
+ * useful if "partial hashes" (hash of data processed so far) are
+ * needed, without preventing injecting extra bytes later on.
+ */
+typedef shake_context sha3_context;
+
+/*
+ * Initialize a SHA3 context, for a given output size (in bits), e.g.
+ * set size to 256 for SHA3-256.
+ */
+void sha3_init(sha3_context *sc, unsigned size);
+
+/*
+ * Update a SHA3 context with some bytes.
+ */
+void sha3_update(sha3_context *sc, const void *in, size_t len);
+
+/*
+ * Finalize a SHA3 computation. The hash output is written in dst[],
+ * with a size that depends on the one provided when the context was
+ * last initialized.
+ *
+ * The context is modified. If a new hash must be computed, the context
+ * must first be reinitialized explicitly.
+ */
+void sha3_close(sha3_context *sc, void *out);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/lib/dns/hawk_link.c b/lib/dns/hawk_link.c
new file mode 100644
index 0000000000..9f0ddffdc2
--- /dev/null
+++ b/lib/dns/hawk_link.c
@@ -0,0 +1,419 @@
+/*
+ * Copyright (C) Internet Systems Consortium, Inc. ("ISC")
+ *
+ * SPDX-License-Identifier: MPL-2.0
+ *
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, you can obtain one at https://mozilla.org/MPL/2.0/.
+ *
+ * See the COPYRIGHT file distributed with this work for additional
+ * information regarding copyright ownership.
+ */
+
+#include <hawk/hawk.h>
+
+#include <isc/entropy.h>
+#include <isc/mem.h>
+#include <isc/result.h>
+#include <isc/safe.h>
+#include <isc/util.h>
+
+#include <dns/keyvalues.h>
+
+#include "dst_internal.h"
+#include "dst_parse.h"
+#include "hawk/sha3.h"
+#include "isc/attributes.h"
+#include "isc/buffer.h"
+#include "isc/hash.h"
+#include "isc/result.h"
+
+static void
+dst__hawk_rng(void *ctx ISC_ATTR_UNUSED, void *dst, size_t len) {
+	isc_entropy_get(dst, len);
+}
+
+static isc_result_t
+dst__hawk_createctx(dst_key_t *key ISC_ATTR_UNUSED, dst_context_t *dctx) {
+	REQUIRE(dctx != NULL && dctx->key != NULL);
+	REQUIRE(dctx->key->key_alg == DST_ALG_HAWK);
+
+	hawk_sign_start(&dctx->ctxdata.shake_context);
+
+	return ISC_R_SUCCESS;
+}
+
+static void
+dst__hawk_destroyctx(dst_context_t *dctx) {
+	REQUIRE(dctx != NULL && dctx->key != NULL);
+	REQUIRE(dctx->key->key_alg == DST_ALG_HAWK);
+}
+
+static isc_result_t
+dst__hawk_adddata(dst_context_t *dctx, const isc_region_t *data) {
+	REQUIRE(dctx != NULL && dctx->key != NULL);
+	REQUIRE(dctx->key->key_alg == DST_ALG_HAWK);
+
+	shake_inject(&dctx->ctxdata.shake_context, data->base, data->length);
+
+	return ISC_R_SUCCESS;
+}
+
+static isc_result_t
+dst__hawk_sign(dst_context_t *dctx, isc_buffer_t *sig) {
+	REQUIRE(dctx != NULL && dctx->key != NULL);
+	REQUIRE(dctx->key->key_alg == DST_ALG_HAWK);
+
+	isc_result_t result = ISC_R_SUCCESS;
+	dst_key_t *key = dctx->key;
+	isc_region_t sigreg;
+	uint8_t tmp[HAWK_TMPSIZE_SIGN(8)];
+
+	isc_buffer_availableregion(sig, &sigreg);
+	if (sigreg.length < DNS_SIG_HAWKSIZE) {
+		result = ISC_R_NOSPACE;
+		goto done;
+	}
+
+	int status = hawk_sign_finish(
+		8, dst__hawk_rng, NULL, sig->base, &dctx->ctxdata.shake_context,
+		key->keydata.keypair.priv, tmp, sizeof(tmp));
+	if (status == 0) {
+		result = DST_R_SIGNFAILURE;
+		isc_log_write(dctx->category, DNS_LOGMODULE_CRYPTO,
+			      ISC_LOG_WARNING, "hawk_sign (%s:%d) failed (%s)",
+			      __FILE__, __LINE__, isc_result_totext(result));
+		goto done;
+	}
+	isc_buffer_add(sig, DNS_SIG_HAWKSIZE);
+
+done:
+	dctx->ctxdata.generic = NULL;
+
+	return result;
+}
+
+static isc_result_t
+dst__hawk_verify(dst_context_t *dctx, const isc_region_t *sig) {
+	REQUIRE(dctx != NULL && dctx->key != NULL);
+	REQUIRE(dctx->key->key_alg == DST_ALG_HAWK);
+
+	isc_result_t result = ISC_R_SUCCESS;
+	dst_key_t *key = dctx->key;
+	uint8_t tmp[HAWK_TMPSIZE_VERIFY_FAST(8)];
+
+	if (sig->length != DNS_SIG_HAWKSIZE) {
+		result = DST_R_VERIFYFAILURE;
+		goto done;
+	}
+
+	int status = hawk_verify_finish(
+		8, sig->base, sig->length, &dctx->ctxdata.shake_context,
+		key->keydata.keypair.pub, DNS_KEY_HAWKSIZE, tmp, sizeof(tmp));
+	if (status == 0) {
+		result = DST_R_VERIFYFAILURE;
+		/* FIXME: Is it really a warning if the verification fails */
+		isc_log_write(dctx->category, DNS_LOGMODULE_CRYPTO,
+			      ISC_LOG_WARNING,
+			      "hawk_verify (%s:%d) failed (%s)", __FILE__,
+			      __LINE__, isc_result_totext(result));
+		goto done;
+	}
+done:
+	return result;
+}
+
+static bool
+dst__hawk_compare(const dst_key_t *key1, const dst_key_t *key2) {
+	uint8_t *pk1 = key1->keydata.keypair.pub;
+	uint8_t *pk2 = key2->keydata.keypair.pub;
+
+	uint8_t *sk1 = key1->keydata.keypair.priv;
+	uint8_t *sk2 = key2->keydata.keypair.priv;
+
+	if ((pk1 == pk2) && (sk1 == sk2)) {
+		/* The keys are identical or all NULL */
+		return true;
+	} else if (pk1 == NULL || pk2 == NULL) {
+		return false;
+	}
+
+	if (memcmp(pk1, pk2, DNS_KEY_HAWKSIZE) != 0) {
+		return false;
+	}
+
+	if (sk1 == sk2) {
+		/* The keys are identical or both NULL */
+		return true;
+	} else if (sk1 == NULL || sk1 == NULL) {
+		return false;
+	}
+
+	if (memcmp(sk1, sk2, DNS_SEC_HAWKSIZE) != 0) {
+		return false;
+	}
+
+	return true;
+}
+
+static isc_result_t
+dst__hawk_generate(dst_key_t *key, int unused ISC_ATTR_UNUSED,
+		   void (*callback ISC_ATTR_UNUSED)(int)) {
+	REQUIRE(key != NULL);
+	REQUIRE(key->key_alg == DST_ALG_HAWK);
+	REQUIRE(key->keydata.keypair.pub == NULL &&
+		key->keydata.keypair.priv == NULL);
+
+	isc_result_t result = ISC_R_UNSET;
+	uint8_t *pk = isc_mem_get(key->mctx, DNS_KEY_HAWKSIZE);
+	uint8_t *sk = isc_mem_get(key->mctx, DNS_SEC_HAWKSIZE);
+	uint8_t tmp[HAWK_TMPSIZE_KEYGEN(8)];
+
+	int status = hawk_keygen(8, sk, pk, dst__hawk_rng, NULL, tmp,
+				 sizeof(tmp));
+	if (status == 0) {
+		result = DST_R_CRYPTOFAILURE;
+		isc_log_write(DNS_LOGCATEGORY_GENERAL, DNS_LOGMODULE_CRYPTO,
+			      ISC_LOG_WARNING,
+			      "hawk_keypair (%s:%d) failed (%s)", __FILE__,
+			      __LINE__, isc_result_totext(result));
+		goto done;
+	}
+
+	key->keydata.keypair.pub = pk;
+	key->keydata.keypair.priv = sk;
+	key->key_size = DNS_KEY_HAWKSIZE * 8;
+
+	result = ISC_R_SUCCESS;
+
+done:
+	if (result != ISC_R_SUCCESS) {
+		isc_mem_put(key->mctx, pk, DNS_KEY_HAWKSIZE);
+	}
+
+	return ISC_R_SUCCESS;
+}
+
+static isc_result_t
+dst__hawk_todns(const dst_key_t *key, isc_buffer_t *data) {
+	REQUIRE(key != NULL);
+	REQUIRE(key->key_alg == DST_ALG_HAWK);
+
+	uint8_t *pk = key->keydata.keypair.pub;
+	isc_region_t r;
+
+	isc_buffer_availableregion(data, &r);
+	if (r.length < DNS_KEY_HAWKSIZE) {
+		return ISC_R_NOSPACE;
+	}
+
+	isc_buffer_putmem(data, pk, DNS_KEY_HAWKSIZE);
+
+	return ISC_R_SUCCESS;
+}
+
+static isc_result_t
+dst__hawk_fromdns(dst_key_t *key, isc_buffer_t *data) {
+	REQUIRE(key != NULL);
+	REQUIRE(key->key_alg == DST_ALG_HAWK);
+	REQUIRE(key->keydata.keypair.pub == NULL);
+
+	isc_region_t r;
+
+	isc_buffer_remainingregion(data, &r);
+	if (r.length == 0) {
+		return ISC_R_SUCCESS;
+	}
+	INSIST(r.length == DNS_KEY_HAWKSIZE);
+
+	key->keydata.keypair.pub = isc_mem_get(key->mctx, DNS_KEY_HAWKSIZE);
+	memmove(key->keydata.keypair.pub, r.base, r.length);
+	key->key_size = DNS_KEY_HAWKSIZE * 8;
+
+	return ISC_R_SUCCESS;
+}
+
+static bool
+dst__hawk_isprivate(const dst_key_t *key) {
+	return key->keydata.keypair.priv != NULL;
+}
+
+static void
+dst__hawk_destroy(dst_key_t *key) {
+	REQUIRE(key != NULL);
+	REQUIRE(key->key_alg == DST_ALG_HAWK);
+	REQUIRE(key->keydata.keypair.pub != NULL);
+
+	if (key->keydata.keypair.priv != NULL) {
+		isc_mem_put(key->mctx, key->keydata.keypair.priv,
+			    DNS_SEC_HAWKSIZE);
+	}
+
+	isc_mem_put(key->mctx, key->keydata.keypair.pub, DNS_KEY_HAWKSIZE);
+}
+
+static isc_result_t
+dst__hawk_tofile(const dst_key_t *key, const char *directory) {
+	REQUIRE(key != NULL);
+	REQUIRE(key->key_alg == DST_ALG_HAWK);
+
+	dst_private_t priv;
+	int i = 0;
+
+	if (key->keydata.pkeypair.pub == NULL) {
+		return DST_R_NULLKEY;
+	}
+
+	INSIST(!key->external);
+
+	priv.elements[i].tag = TAG_HAWK_PUBLICKEY;
+	priv.elements[i].length = DNS_KEY_HAWKSIZE;
+	priv.elements[i].data = key->keydata.keypair.pub;
+	i++;
+
+	if (dst_key_isprivate(key)) {
+		priv.elements[i].tag = TAG_HAWK_SECRETKEY;
+		priv.elements[i].length = DNS_SEC_HAWKSIZE;
+		priv.elements[i].data = key->keydata.keypair.priv;
+		i++;
+	}
+
+	priv.nelements = i;
+
+	return dst__privstruct_writefile(key, &priv, directory);
+}
+
+static isc_result_t
+dst__hawk_parse(dst_key_t *key, isc_lex_t *lexer, dst_key_t *pub) {
+	REQUIRE(key != NULL);
+	REQUIRE(key->key_alg == DST_ALG_HAWK);
+	REQUIRE(key->keydata.keypair.pub == NULL &&
+		key->keydata.keypair.priv == NULL);
+
+	isc_result_t result = ISC_R_UNSET;
+	dst_private_t priv;
+	uint8_t *pk = isc_mem_get(key->mctx, DNS_KEY_HAWKSIZE);
+	uint8_t *sk = isc_mem_get(key->mctx, DNS_SEC_HAWKSIZE);
+
+	result = dst__privstruct_parse(key, DST_ALG_HAWK, lexer, key->mctx,
+				       &priv);
+	if (result != ISC_R_SUCCESS) {
+		goto done;
+	}
+
+	if (key->external) {
+		if (priv.nelements != 0 || pub == NULL) {
+			result = DST_R_INVALIDPRIVATEKEY;
+			goto done;
+		}
+
+		key->keydata.pkeypair.priv = pub->keydata.pkeypair.priv;
+		key->keydata.pkeypair.pub = pub->keydata.pkeypair.pub;
+		pub->keydata.pkeypair.priv = NULL;
+		pub->keydata.pkeypair.pub = NULL;
+
+		result = ISC_R_SUCCESS;
+		goto done;
+	}
+
+	for (size_t i = 0; i < priv.nelements; i++) {
+		switch (priv.elements[i].tag) {
+		case TAG_HAWK_PUBLICKEY:
+			if (priv.elements[i].length != DNS_KEY_HAWKSIZE) {
+				result = DST_R_INVALIDPUBLICKEY;
+				goto done;
+			}
+			memmove(pk, priv.elements[i].data, DNS_KEY_HAWKSIZE);
+			break;
+		case TAG_HAWK_SECRETKEY:
+			if (priv.elements[i].length != DNS_SEC_HAWKSIZE) {
+				result = DST_R_INVALIDPRIVATEKEY;
+				goto done;
+			}
+			memmove(sk, priv.elements[i].data, DNS_SEC_HAWKSIZE);
+			break;
+		default:
+			break;
+		}
+	}
+
+	if (pk == NULL) {
+		result = DST_R_INVALIDPUBLICKEY;
+		goto done;
+	}
+
+	if (sk == NULL) {
+		result = DST_R_INVALIDPRIVATEKEY;
+		goto done;
+	}
+
+	key->keydata.keypair.priv = sk;
+	key->keydata.keypair.pub = pk;
+	key->key_size = DNS_KEY_HAWKSIZE * 8;
+
+	result = ISC_R_SUCCESS;
+
+done:
+	if (result != ISC_R_SUCCESS) {
+		isc_safe_memwipe(pk, DNS_KEY_HAWKSIZE);
+		isc_mem_put(key->mctx, pk, DNS_KEY_HAWKSIZE);
+
+		isc_safe_memwipe(sk, DNS_SEC_HAWKSIZE);
+		isc_mem_put(key->mctx, sk, DNS_SEC_HAWKSIZE);
+
+		key->keydata.generic = NULL;
+	}
+
+	dst__privstruct_free(&priv, key->mctx);
+	isc_safe_memwipe(&priv, sizeof(priv));
+
+	return result;
+}
+
+static dst_func_t dst__hawk_functions = {
+	dst__hawk_createctx,
+	dst__hawk_destroyctx,
+	dst__hawk_adddata,
+	dst__hawk_sign,
+	dst__hawk_verify,
+	dst__hawk_compare,
+	dst__hawk_generate,
+	dst__hawk_isprivate,
+	dst__hawk_destroy,
+	dst__hawk_todns,
+	dst__hawk_fromdns,
+	dst__hawk_tofile,
+	dst__hawk_parse,
+	NULL, /*%< fromlabel */
+	NULL, /*%< dump */
+	NULL, /*%< restore */
+};
+
+static isc_result_t
+check_algorithm(unsigned char algorithm) {
+	switch (algorithm) {
+	case DST_ALG_HAWK:
+		break;
+	default:
+		return ISC_R_NOTIMPLEMENTED;
+	}
+
+	/*
+	 * TODO: check that we can verify HawkHD signature
+	 * like we do with the other algorithms.
+	 */
+
+	return ISC_R_SUCCESS;
+}
+
+void
+dst__hawk_init(dst_func_t **funcp, unsigned char algorithm) {
+	REQUIRE(funcp != NULL);
+
+	if (*funcp == NULL) {
+		if (check_algorithm(algorithm) == ISC_R_SUCCESS) {
+			*funcp = &dst__hawk_functions;
+		}
+	}
+}
diff --git a/lib/dns/include/dns/keyvalues.h b/lib/dns/include/dns/keyvalues.h
index 08cd0c05da..c20717f88f 100644
--- a/lib/dns/include/dns/keyvalues.h
+++ b/lib/dns/include/dns/keyvalues.h
@@ -72,6 +72,7 @@ enum {
 	DNS_KEYALG_ECDSA384 = 14,
 	DNS_KEYALG_ED25519 = 15,
 	DNS_KEYALG_ED448 = 16,
+	DNS_KEYALG_HAWK = 121,
 	DNS_KEYALG_INDIRECT = 252,
 	DNS_KEYALG_PRIVATEDNS = 253,
 	DNS_KEYALG_PRIVATEOID = 254, /*%< Key begins with OID giving alg */
@@ -97,3 +98,26 @@ enum {
 
 #define DNS_KEY_ED448SIZE 57
 #define DNS_SIG_ED448SIZE 114
+
+/* FIXME: Copied verbatim from hawk.h */
+
+/*
+ * Private key length (in bytes).
+ */
+#define HAWK_PRIVKEY_SIZE(logn) (8u + (11u << ((logn) - 5)))
+
+/*
+ * Public key length (in bytes).
+ */
+#define HAWK_PUBKEY_SIZE(logn) \
+	(450u + 574u * (2u >> (10 - (logn))) + 842u * ((1u >> (10 - (logn)))))
+
+/*
+ * Signature length (in bytes).
+ */
+#define HAWK_SIG_SIZE(logn) \
+	(249u + 306u * (2u >> (10 - (logn))) + 360u * ((1u >> (10 - (logn)))))
+
+#define DNS_SIG_HAWKSIZE HAWK_SIG_SIZE(8)
+#define DNS_KEY_HAWKSIZE HAWK_PUBKEY_SIZE(8)
+#define DNS_SEC_HAWKSIZE HAWK_PRIVKEY_SIZE(8)
diff --git a/lib/dns/include/dst/dst.h b/lib/dns/include/dst/dst.h
index 0a9472b284..a80fd2e22c 100644
--- a/lib/dns/include/dst/dst.h
+++ b/lib/dns/include/dst/dst.h
@@ -14,7 +14,6 @@
 #pragma once
 
 /*! \file dst/dst.h */
-
 #include <inttypes.h>
 #include <stdbool.h>
 
@@ -93,6 +92,7 @@ typedef enum dst_algorithm {
 	DST_ALG_ECDSA384 = 14,
 	DST_ALG_ED25519 = 15,
 	DST_ALG_ED448 = 16,
+	DST_ALG_HAWK = 123, /* FIXME */
 
 	/*
 	 * Do not renumber HMAC algorithms as they are used externally to named
diff --git a/lib/dns/kasp.c b/lib/dns/kasp.c
index d1e342b130..d8f1ca7070 100644
--- a/lib/dns/kasp.c
+++ b/lib/dns/kasp.c
@@ -13,6 +13,7 @@
 
 /*! \file */
 
+#include <hawk/hawk.h>
 #include <string.h>
 
 #include <isc/assertions.h>
@@ -455,6 +456,9 @@ dns_kasp_key_size(dns_kasp_key_t *key) {
 	case DNS_KEYALG_ED448:
 		size = 456;
 		break;
+	case DNS_KEYALG_HAWK:
+		size = DNS_KEY_HAWKSIZE;
+		break;
 	default:
 		/* unsupported */
 		break;
diff --git a/lib/dns/opensslecdsa_link.c b/lib/dns/opensslecdsa_link.c
index c95050020b..a62b90700c 100644
--- a/lib/dns/opensslecdsa_link.c
+++ b/lib/dns/opensslecdsa_link.c
@@ -675,13 +675,12 @@ opensslecdsa_extract_private_key(const dst_key_t *key, unsigned char *buf,
 #endif /* OPENSSL_VERSION_NUMBER >= 0x30000000L */
 
 static isc_result_t
-opensslecdsa_createctx(dst_key_t *key, dst_context_t *dctx) {
+opensslecdsa_createctx(dst_key_t *key ISC_ATTR_UNUSED, dst_context_t *dctx) {
 	isc_result_t ret = ISC_R_SUCCESS;
 	EVP_MD_CTX *evp_md_ctx;
 	EVP_PKEY_CTX *pctx = NULL;
 	const EVP_MD *type = NULL;
 
-	UNUSED(key);
 	REQUIRE(opensslecdsa_valid_key_alg(dctx->key->key_alg));
 	REQUIRE(dctx->use == DO_SIGN || dctx->use == DO_VERIFY);
 
diff --git a/lib/dns/openssleddsa_link.c b/lib/dns/openssleddsa_link.c
index fd383f688b..5ff8083434 100644
--- a/lib/dns/openssleddsa_link.c
+++ b/lib/dns/openssleddsa_link.c
@@ -108,14 +108,14 @@ static isc_result_t
 openssleddsa_fromlabel(dst_key_t *key, const char *label, const char *pin);
 
 static isc_result_t
-openssleddsa_createctx(dst_key_t *key, dst_context_t *dctx) {
+openssleddsa_createctx(dst_key_t *key ISC_ATTR_UNUSED, dst_context_t *dctx) {
 	isc_buffer_t *buf = NULL;
 	const eddsa_alginfo_t *alginfo =
 		openssleddsa_alg_info(dctx->key->key_alg);
 
-	UNUSED(key);
 	REQUIRE(alginfo != NULL);
 
+	/* The 64 constant here is suspicious */
 	isc_buffer_allocate(dctx->mctx, &buf, 64);
 	dctx->ctxdata.generic = buf;
 
@@ -138,27 +138,13 @@ openssleddsa_destroyctx(dst_context_t *dctx) {
 static isc_result_t
 openssleddsa_adddata(dst_context_t *dctx, const isc_region_t *data) {
 	isc_buffer_t *buf = (isc_buffer_t *)dctx->ctxdata.generic;
-	isc_buffer_t *nbuf = NULL;
-	isc_region_t r;
-	unsigned int length;
-	isc_result_t result;
 	const eddsa_alginfo_t *alginfo =
 		openssleddsa_alg_info(dctx->key->key_alg);
 
 	REQUIRE(alginfo != NULL);
 
-	result = isc_buffer_copyregion(buf, data);
-	if (result == ISC_R_SUCCESS) {
-		return ISC_R_SUCCESS;
-	}
-
-	length = isc_buffer_length(buf) + data->length + 64;
-	isc_buffer_allocate(dctx->mctx, &nbuf, length);
-	isc_buffer_usedregion(buf, &r);
-	(void)isc_buffer_copyregion(nbuf, &r);
-	(void)isc_buffer_copyregion(nbuf, data);
-	isc_buffer_free(&buf);
-	dctx->ctxdata.generic = nbuf;
+	isc_result_t result = isc_buffer_copyregion(buf, data);
+	INSIST(result == ISC_R_SUCCESS);
 
 	return ISC_R_SUCCESS;
 }
diff --git a/lib/dns/opensslrsa_link.c b/lib/dns/opensslrsa_link.c
index cc945a20ea..fff5c9ebb0 100644
--- a/lib/dns/opensslrsa_link.c
+++ b/lib/dns/opensslrsa_link.c
@@ -161,11 +161,10 @@ opensslrsa_valid_key_alg(unsigned int key_alg) {
 }
 
 static isc_result_t
-opensslrsa_createctx(dst_key_t *key, dst_context_t *dctx) {
+opensslrsa_createctx(dst_key_t *key ISC_ATTR_UNUSED, dst_context_t *dctx) {
 	EVP_MD_CTX *evp_md_ctx;
 	const EVP_MD *type = NULL;
 
-	UNUSED(key);
 	REQUIRE(dctx != NULL && dctx->key != NULL);
 	REQUIRE(opensslrsa_valid_key_alg(dctx->key->key_alg));
 
diff --git a/lib/dns/rcode.c b/lib/dns/rcode.c
index 6c957e870c..ec5cb7fa97 100644
--- a/lib/dns/rcode.c
+++ b/lib/dns/rcode.c
@@ -107,6 +107,7 @@
 		{ DNS_KEYALG_ECDSA384, "ECDSA384", 0 },         \
 		{ DNS_KEYALG_ED25519, "ED25519", 0 },           \
 		{ DNS_KEYALG_ED448, "ED448", 0 },               \
+		{ DNS_KEYALG_HAWK, "HAWK", 0 },                 \
 		{ DNS_KEYALG_INDIRECT, "INDIRECT", 0 },         \
 		{ DNS_KEYALG_PRIVATEDNS, "PRIVATEDNS", 0 },     \
 		{ DNS_KEYALG_PRIVATEOID, "PRIVATEOID", 0 }, { 0, NULL, 0 }
diff --git a/lib/isccfg/kaspconf.c b/lib/isccfg/kaspconf.c
index 4645b50368..06f78b9b67 100644
--- a/lib/isccfg/kaspconf.c
+++ b/lib/isccfg/kaspconf.c
@@ -278,6 +278,7 @@ cfg_kaspkey_fromconfig(const cfg_obj_t *config, dns_kasp_t *kasp,
 			case DNS_KEYALG_ECDSA384:
 			case DNS_KEYALG_ED25519:
 			case DNS_KEYALG_ED448:
+			case DNS_KEYALG_HAWK:
 				cfg_obj_log(obj, ISC_LOG_WARNING,
 					    "dnssec-policy: key algorithm %s "
 					    "has predefined length; ignoring "
-- 
2.43.0

